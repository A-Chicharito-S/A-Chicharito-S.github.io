<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script> </head> <body> <p>This is the blog about the implementation of Transformer from <a href="https://arxiv.org/abs/1706.03762" rel="external nofollow noopener" target="_blank">Attention Is All You Need</a>. The full code (model + training loop) is <a href="https://github.com/A-Chicharito-S/PyTorch-Projects-for-Learning/tree/main/Transformer" rel="external nofollow noopener" target="_blank">[here]</a>, which takes inspirations from <a href="#0">[Reference]</a>.</p> <h2 id="introduction">Introduction</h2> <p>“<strong>Attention Is All You Need</strong>” (see <a href="https://arxiv.org/abs/1706.03762" rel="external nofollow noopener" target="_blank">[here]</a>) is the original paper of the Transformer model, which is a powerful structure in the field of NLP and has been the foundation of many following works of different sub-fields of NLP. In this post, I will introduce the implementation of the Transformer in the following three steps: <strong>1</strong>. the overall architecture of the Transformer; <strong>2</strong>. the reusable components of the Transformer; <strong>3</strong>. The final model constructed by the components from <strong>2</strong>.</p> <h2 id="overall-architecture">Overall Architecture</h2> <p>The following picture is from <strong>Attention Is All You Need</strong> and illustrates the overall architecture of the Transformer.</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/pytorch_learning_1/pic1.png" style="zoom:80%;"></div> <p><a name="4"></a> We can see that the Transformer is mainly composed of two parts: the Encoder and the Decoder. Both of which are then staked by <strong>N</strong> encoder/decoder blocks. And for each block, it can be decoupled into the following components: <strong>a</strong>. Multi-Head Attention; <strong>b</strong>. Add&amp;Norm; <strong>c</strong>. Feed Forward. And there is an embedding module that embeds the discrete digits into neural-network-friendly dense vectors.</p> <p>For an NLP task, given the source sequence $X=(BOS,\,x_1,\,…,x_n,\,EOS)$ and its target $Y=(BOS,\,y_1,\,…,y_m,\,EOS)$, the Transformer feeds $x$ as ‘<strong>inputs</strong>’ and embeds it into high-dimension dense vectors, which later are further encoded by the Encoder. When <strong>training</strong>, Target $y$ is shifted right (the last token $EOS$ is removed) and embedded to be fed into the Decoder as <strong>Outputs</strong>, where an interaction with the final outputs from the Encoder is performed in the Multi-Head Attention module. The final outputs from the Decoder are later projected to the target vocabulary size to produce the prediction $\hat{Y}=(\hat{y}_1,\,…,\hat{y}_m,\,EOS)$ When <strong>testing</strong>, at first there will only be $BOS$ fed into the Decoder to produce the first predicted token $\hat{y}_1$, and later $(BOS,\,\hat{y}_1)$ is fed and so on. (This process is called <strong>auto-regressive</strong> since the output from the last state is used as input for the current state.)</p> <h2 id="components">Components</h2> <p>As identified above, we have the following components that make up our Transformer model: <strong>a</strong>. Embedding; <strong>b</strong>. Multi-Head Attention; <strong>c</strong>. Add&amp;Norm; <strong>d</strong>. Feed Forward.</p> <h3 id="a-embedding">a. Embedding</h3> <p>The Embedding has two parts: first is the embedding layer that maps discrete digits into dense vectors, and second is the positional encoding layer that gives the tokens $EOS,\,x_1,\,x_2,\,…$ in $X=(BOS,\,x_1,\,…,x_n,\,EOS)$ a sequential feature. (since unlike RNN taking one input token as a time, the Transformer takes inputs at one time)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>


<span class="k">class</span> <span class="nc">TransformerEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_vocab</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">PAD</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">is_frozen</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerEmbedding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="o">=</span><span class="n">num_vocab</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">PAD</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">embedding_matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">embedding_matrix</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="n">num_vocab</span> <span class="ow">or</span> <span class="n">embedding_matrix</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">!=</span> <span class="n">dim</span>
            <span class="n">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">embeddings</span><span class="o">=</span><span class="n">embedding_matrix</span><span class="p">,</span> <span class="n">freeze</span><span class="o">=</span><span class="n">is_frozen</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">PE</span><span class="sh">'</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">max_seq_len</span><span class="o">=</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
</code></pre></div></div> <p>The above code is the “<strong>__init__</strong>” method of our Embedding. To be specific, it uses the <strong>nn.Embedding</strong> module to map discrete digits (0, 1, 2, …) into dense vectors of size “<strong>dim</strong>”. By passing “<strong>embedding_matrix</strong>” and “<strong>is_frozen</strong>” parameters, we can initialize the embedding table and decide whether it is trainable. We also can use the “<strong>self.register_buffer</strong>” method (inherited from “<strong>nn.Module</strong>”, and can be quickly accessed as a constant) to store the <strong>positional encoding</strong>, which is computed by:</p> <div> $$ \begin{split} PE_{(pos,\,2i)} &amp;= sin(pos\,/\,10000^{2i\,/\,d_{model}}) \\ PE_{(pos,\,2i+1)} &amp;= cos(pos\,/\,10000^{2i\,/\,d_{model}}) \end{split} $$ </div> <p>The above formula tells us for a token at position $pos$ in a sequence, the <strong>positional encoding</strong> of its even dimension is a $sin$ function and that of odd dimension is a $cos$ function. The wavelength for dimension $2i$ and $2i+1$ is $2\pi·10000^{2i\,/d_{model}}\in[2\pi,\,10000·2\pi]$ and the max length that <strong>positional encoding</strong> can present is roughly viewed as $10000$ (since the max period is $10000·2\pi$). One good thing about using trigonometric functions is: $sin(pos+k)=sin(pos)cos(k)+cos(pos)sin(k)$ and $cos(pos+k)=cos(pos)cos(k)-sin(pos)sin(k)$; thus, the relationship between the $pos$-th position and $pos+k$-th position in the sentence can be view as a linear transformation $PE_{pos}\longrightarrow PE_{pos+k}$ where the coefficients are constants about $k$, namely, $sin(w_i·k)$ and $cos(w_i·k)$ (see discussion <a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/#the-intuition" rel="external nofollow noopener" target="_blank">[here]</a>).</p> <p>The implementation is:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">PositionalEncoding</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># returns: value: [0, 1, ..., max_seq_len-1] ; shape: (max_seq_len, 1)
</span>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">dim</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">))</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="c1"># i:j:k means from position i to j, take indices every k steps.
</span>        <span class="k">return</span> <span class="n">pe</span>
</code></pre></div></div> <p>where noticing the <strong>div_term</strong> is actually:</p> \[\frac{1}{10000^{2i/d_{model}}}=e^{log(\frac{1}{10000^{2i/d_{model}}})}=e^{-\frac{2i}{d_{model}}log10000}\] <p>And we can defined the computation process as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># inputs shape: (batch_size, seq_len)
</span>        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">dim</span><span class="p">)</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">PE</span><span class="p">[:</span><span class="n">seq_len</span><span class="p">]))</span>
</code></pre></div></div> <p>Note that after “<strong>self.embedding(inputs)</strong>” it is multiplied by $\sqrt{d_{model}}$ (which is the case in <a href="#2">[2]</a> and <a href="#3">[3]</a>) and a <strong>layer normalization</strong> is performed before output. (which is as well equivalent to the implementations from <a href="#2">[2]</a> and <a href="#3">[3]</a>). In <a href="#1">[1]</a>, the returning is simply: <strong>self.dropout(self.embedding(inputs) + self.PE[:seq_len])</strong></p> <h3 id="b-multi-head-attention">b. Multi-Head Attention</h3> <p>The Attention layer follows the following computation operation: $Attention(Q,\,K,\,V)=softmax(\frac{Q·K^{T}}{\sqrt{d_k}})·V$, and the term “<strong>Multi-Head</strong>” can be roughly treated as performing this operation multiple times for the same set of $Q,\,K,\,V$. To be specific, the <strong>Multi-Head Attention</strong> is computed by:</p> <div> $$ \begin{split} MultiHead(Q,\,K,\,V) &amp;= Concat(head_1,\,...,\,head_h)W^O \\ where\,\,head_i &amp;= Attention(QW^Q_i,\,KW^K_i,\,VW^V_i) \end{split} $$ </div> <p>The implementation is:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>


<span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Attention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>

        <span class="n">self</span><span class="p">.</span><span class="n">sub_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_K</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_Q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_V</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">dropout_rate</span>
        <span class="k">if</span> <span class="n">dropout_rate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>

</code></pre></div></div> <p>The “<strong>__init__</strong>” method of our Multi-Head Attention layer is quite simple. By passing the number of attention heads, we are actually operating on the hidden dimension (split it into <strong>num_heads</strong> equal parts) to do the attention. Note for <strong>Multi-Head Attention</strong> the module “<strong>nn.Dropout</strong>” is not in <a href="#1">[1]</a> and can be turned off in <a href="#2">[2]</a> and <a href="#3">[3]</a>.</p> <p>The forward pass of out attention layer will be:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># inputs is of shape (batch_size, seq_len, dim)
</span>        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_Q</span><span class="p">(</span><span class="n">Q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">Q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">sub_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># shape: (batch_size, seq_len, num_heads, sub_dim) ---&gt; (batch_size, num_heads, seq_len_q, sub_dim)
</span>        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_K</span><span class="p">(</span><span class="n">K</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">sub_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># shape: (batch_size, num_heads, seq_len_kv, sub_dim)
</span>        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_V</span><span class="p">(</span><span class="n">V</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">V</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">sub_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># shape: (batch_size, num_heads, seq_len_kv, sub_dim)
</span>        <span class="n">att_score</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sub_dim</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">att_score</span> <span class="o">=</span> <span class="n">att_score</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
        <span class="n">att_score</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">att_score</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># (batch_size, num_heads, seq_len_q, seq_len_kv)
</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">att_score</span> <span class="o">=</span> <span class="n">att_score</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">dropout_rate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">att_score</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">att_score</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">att_score</span><span class="p">,</span> <span class="n">V</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">sub_dim</span><span class="p">)</span>
        <span class="c1"># shape: (batch_size, num_heads, seq_len_q, sub_dim) ---&gt; (batch_size, seq_len_q, num_heads, sub_dim)
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_out</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="c1"># shape: (batch_size, seq_len_q, dim)
</span></code></pre></div></div> <p>We can see that, for a given input, say <strong>Q</strong>, a following pre-processing is performed:</p> \[(batch\_size,\,seq\_len\_q,\,dim\stackrel{W^{Q}}{\longrightarrow}dim)\stackrel{reshape}{\longrightarrow}(batch\_size,\,num\_heads,\,seq\_len\_q,\,sub\_dim)\] <p>And the “<strong>Multi-Head</strong>” operation is actually performed by one matrix (in our example: $W^Q$). The following codes are quite easy to understand. Note that <a href="#1">[1]</a>, <a href="#2">[2]</a>, <a href="#3">[3]</a> all implement the $Attention(Q,\,K,\,V)=softmax(\frac{Q·K^{T}}{\sqrt{d_k}})·V$ as another callable function (such as “<strong>def attention(query, key, value, mask=None, dropout=None):</strong>”) and masking the padded positions of attention score “<strong>att_score = att_score.masked_fill(mask, 0)</strong>” is not in <a href="#2">[2]</a> and <a href="#3">[3]</a>. And the usage of “<strong>.reshape()</strong>”, “<strong>.view()</strong>”, and “<strong>.contiguous()</strong>” is specified <a href="https://blog.csdn.net/Flag_ing/article/details/109129752" rel="external nofollow noopener" target="_blank">[here]</a></p> <p>A very important thing is the shape of the mask. Based on the function of the masks, we can simply divide them into two kinds: padding mask (used in <strong>Multi-Head Attention</strong>, both in the Encoder and the Decoder, see <a href="#4">[overall architecture]</a>) and subsequent mask (used in <strong>Masked Multi-Head Attention</strong>, only in the Decoder, see <a href="#4">[overall architecture]</a>). A <strong>padded mask</strong> <a name="5"></a> is to mask out the paddings in the sentence, for example: for an input: $[3,\,5,\,8,\,9,\,4,\,0,\,0,\,0]$, the three zeros at the end are just place holders thus should not be considered real words. A <strong>subsequent mask</strong> <a name="6"></a>, however is only used in the decoder, and is of shape: $seq\_len\_outputs\times seq\_len\_outputs$ where $seq\_len\_outputs$ is the sentence length of the “<strong>Outputs</strong>” fed into the Decoder (see <a href="#4">[overall architecture]</a>). A general sense of the <strong>subsequent mask</strong> is this ($seq\_len\_outputs=3$):</p> <div> $$ \begin{bmatrix} 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 0 \end{bmatrix} $$ </div> <p>where the diagonal and below are all set to 0 (or 1) and above the diagonal are all set to 1 (or 0).<a name="7"></a></p> <h3 id="c-addnorm">c. Add&amp;Norm</h3> <p>This layer simply leverages the residual connection from <a href="https://arxiv.org/abs/1512.03385" rel="external nofollow noopener" target="_blank">[paper]</a> to add a direct path between two sub-layers to avoid gradient vanishing, which can be implemented by: $LayerNorm(x+Sublayer(x))$ where $x$ is the input of the sub-layer and $Sublayer(x)$ is its output.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="c1"># This is the example from the encoder block
</span>    	<span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">att</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">pad_mask</span><span class="p">))</span>
        <span class="c1"># att: Q=inputs, K=inputs, V=inputs
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">feedforward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
    
    
        <span class="c1"># This is the example from the decoder block
</span>        <span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">masked_att</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">upper_triangle_mask</span><span class="p">))</span>
        <span class="c1"># masked_att: Q=inputs, K=inputs, V=inputs, mask=upper_triangle_mask
</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">att</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">pad_mask</span><span class="p">))</span>
        <span class="c1"># att: Q=inputs, K=encoder_outputs, V=encoder_outputs, mask=pad_mask
</span>        
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">feedforward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
</code></pre></div></div> <h3 id="d-feed-forward">d. Feed Forward</h3> <p>The full name of this layer is called <strong>Position-wise Feed-Forward Networks</strong> in the origin paper, since it’s applied to each position separately. And it is computed by:</p> \[FFN(x)=max(0,\,xW_1+b_1)W_2+b_2\] <p>The implementation is very simple:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>


<span class="c1"># implements: max(0, x·W_1 + b_1)·W_2 + b_2
</span><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">inner_layer_dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">):</span>  <span class="c1"># in paper: dim=512, inner_layer_dim=2048
</span>        <span class="nf">super</span><span class="p">(</span><span class="n">FeedForward</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">inner_layer_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">inner_layer_dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_2</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">W_1</span><span class="p">(</span><span class="n">inputs</span><span class="p">))))</span>
</code></pre></div></div> <h2 id="final-model">Final Model</h2> <p>With the aforementioned four basic components, we can now build our Transformer model! First, we need to construct the encoder/decoder block for the Encoder/Decoder, and then aggregate them together to get our final model. At last, we will also look a little deeper into the decoding strategies when <strong>testing</strong>.</p> <p>A encoder block is:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>


<span class="k">class</span> <span class="nc">TransformerBaseEncoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1"># the input is of size: (batch_size, seq_len, dim)
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">Attention</span><span class="p">,</span> <span class="n">FeedForward</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerBaseEncoderBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">att</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">Attention</span><span class="p">)</span>
        <span class="c1"># for reusable components, we pass it in as parameter and use deepcopy()
</span>        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="c1"># normalized_shape:
</span>        <span class="c1"># If a single integer is used, it is treated as a singleton list, and this module will normalize over the last
</span>        <span class="c1"># dimension which is expected to be of that specific size.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">feedforward</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">FeedForward</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">pad_mask</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">att</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">pad_mask</span><span class="p">))</span>
        <span class="c1"># att: Q=inputs, K=inputs, V=inputs
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">feedforward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
</code></pre></div></div> <p>where the modules (such as <strong>Attention</strong>, <strong>FeedForward</strong>) are the components you see from the previous section. And by stacking these blocks together, we can get <strong>the Encoder</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TransformerBaseEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_encoder_block</span><span class="p">,</span> <span class="n">encoder_block</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerBaseEncoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoder_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">encoder_block</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_encoder_block</span><span class="p">)])</span>
        <span class="c1"># nn.ModuleList receives a list containing modules and stores them
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="c1"># inputs are the embedded source sentences
</span>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">encoder_blocks</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="nf">block</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span>
</code></pre></div></div> <p>where the <strong>encoder_block</strong> is the above-defined <strong>TransformerBaseEncoderBlock</strong> module, and by calling its “<strong>forward()</strong>” method, we can get the final output of the entire Transformer encoder.</p> <p>Similarly, a decoder block is defined as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>


<span class="k">class</span> <span class="nc">TransformerBaseDecoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">Attention</span><span class="p">,</span> <span class="n">FeedForward</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerBaseDecoderBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">masked_att</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">Attention</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">att</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">Attention</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feedforward</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">FeedForward</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">pad_mask</span><span class="p">,</span> <span class="n">upper_triangle_mask</span><span class="p">):</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">masked_att</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">upper_triangle_mask</span><span class="p">))</span>
        <span class="c1"># masked_att: Q=inputs, K=inputs, V=inputs, mask=upper_triangle_mask
</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">att</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">pad_mask</span><span class="p">))</span>
        <span class="c1"># att: Q=inputs, K=encoder_outputs, V=encoder_outputs, mask=pad_mask
</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">feedforward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
</code></pre></div></div> <p>And <strong>the Decoder</strong> is then implemented as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TransformerBaseDecoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_decoder_block</span><span class="p">,</span> <span class="n">decoder_block</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerBaseDecoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">decoder_block</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_decoder_block</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">pad_mask</span><span class="p">,</span> <span class="n">upper_triangle_mask</span><span class="p">):</span>
        <span class="c1"># inputs are the embedded target sentences (training) / the newest embedded decoded word (test)
</span>        <span class="k">for</span> <span class="n">decoder_block</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">decoder_blocks</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="nf">decoder_block</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">pad_mask</span><span class="p">,</span> <span class="n">upper_triangle_mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span>
</code></pre></div></div> <p>where the <strong>decoder_block</strong> is just the above-defined <strong>TransformerBaseDecoderBlock</strong></p> <p>Finally, we can define our Transformer as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">model.Embedding</span> <span class="kn">import</span> <span class="n">TransformerEmbedding</span>
<span class="kn">from</span> <span class="n">model.Attention</span> <span class="kn">import</span> <span class="n">Attention</span>
<span class="kn">from</span> <span class="n">model.FeedForward</span> <span class="kn">import</span> <span class="n">FeedForward</span>
<span class="kn">from</span> <span class="n">model.TransformerEncoder</span> <span class="kn">import</span> <span class="n">TransformerBaseEncoderBlock</span><span class="p">,</span> <span class="n">TransformerBaseEncoder</span>
<span class="kn">from</span> <span class="n">model.TransformerDecoder</span> <span class="kn">import</span> <span class="n">TransformerBaseDecoderBlock</span><span class="p">,</span> <span class="n">TransformerBaseDecoder</span>


<span class="k">class</span> <span class="nc">TransformerBase</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_encoder_block</span><span class="p">,</span> <span class="n">num_decoder_block</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">num_vocab_src</span><span class="p">,</span> <span class="n">num_vocab_tgt</span><span class="p">,</span>
                 <span class="n">dim</span><span class="p">,</span> <span class="n">inner_layer_dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">PAD</span><span class="p">,</span> <span class="n">BOS</span><span class="p">,</span> <span class="n">EOS</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerBase</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">PAD</span> <span class="o">=</span> <span class="n">PAD</span>
        <span class="n">self</span><span class="p">.</span><span class="n">BOS</span> <span class="o">=</span> <span class="n">BOS</span>
        <span class="n">self</span><span class="p">.</span><span class="n">EOS</span> <span class="o">=</span> <span class="n">EOS</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_vocab_tgt</span> <span class="o">=</span> <span class="n">num_vocab_tgt</span>

        <span class="n">attention</span> <span class="o">=</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">)</span>
        <span class="n">feedforward</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">inner_layer_dim</span><span class="o">=</span><span class="n">inner_layer_dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="n">encoder_block</span> <span class="o">=</span> <span class="nc">TransformerBaseEncoderBlock</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">Attention</span><span class="o">=</span><span class="n">attention</span><span class="p">,</span>
                                                    <span class="n">FeedForward</span><span class="o">=</span><span class="n">feedforward</span><span class="p">)</span>
        <span class="n">decoder_block</span> <span class="o">=</span> <span class="nc">TransformerBaseDecoderBlock</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">Attention</span><span class="o">=</span><span class="n">attention</span><span class="p">,</span>
                                                    <span class="n">FeedForward</span><span class="o">=</span><span class="n">feedforward</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">embedding_src</span> <span class="o">=</span> <span class="nc">TransformerEmbedding</span><span class="p">(</span><span class="n">num_vocab</span><span class="o">=</span><span class="n">num_vocab_src</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">PAD</span><span class="o">=</span><span class="n">PAD</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding_tgt</span> <span class="o">=</span> <span class="nc">TransformerEmbedding</span><span class="p">(</span><span class="n">num_vocab</span><span class="o">=</span><span class="n">num_vocab_tgt</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">PAD</span><span class="o">=</span><span class="n">PAD</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="nc">TransformerBaseEncoder</span><span class="p">(</span><span class="n">num_encoder_block</span><span class="o">=</span><span class="n">num_encoder_block</span><span class="p">,</span> <span class="n">encoder_block</span><span class="o">=</span><span class="n">encoder_block</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="nc">TransformerBaseDecoder</span><span class="p">(</span><span class="n">num_decoder_block</span><span class="o">=</span><span class="n">num_decoder_block</span><span class="p">,</span> <span class="n">decoder_block</span><span class="o">=</span><span class="n">decoder_block</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">num_vocab_tgt</span><span class="p">)</span>
</code></pre></div></div> <p>The main components in our Transformer are: <strong>embedding layers</strong> of the source (<strong>inputs</strong>) and target (<strong>outputs</strong>); <strong>the Encoder</strong> and <strong>the Decoder</strong>; <strong>a projection layer</strong> $\mathbf{R}^{dim}\longrightarrow \mathbf{R}^{target\,vocab} $ that maps the final output to the vocabulary of the target.</p> <p>Note that in <a href="#2">[2]</a> and <a href="#3">[3]</a> there are two embedding layers for source and target respectively. In <a href="#1">[1]</a> there is only one embedding layer shared for source and target.</p> <p>Before we define the forward pass of our model, let’s first met some masking functions that helps us:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_pad_mask</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">PAD</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># padded places are True, real words are False
</span>
<span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">create_upper_triangle_mask</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># x shape: (batch_size, seq_len)
</span>    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)).</span><span class="nf">triu</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">bool</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># unseen words are True, available words are False
</span></code></pre></div></div> <p>The above two functions produce the aforementioned two different kinds of masks, namely <a href="#5">[padded mask]</a> and <a href="#6">[subsequent mask]</a>. And now we define the forward pass of our model as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># inputs shape: (batch_size, seq_len)
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">inputs_mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">create_pad_mask</span><span class="p">(</span><span class="n">inputs</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># of shape: (batch_size, 1, 1, seq_len_inputs) &lt;---&gt; in Attention,
</span>    <span class="c1"># when masking, (Q·K.transpose) is of shape: (batch_size, num_heads, seq_len_q, seq_len_kv)
</span>    <span class="c1"># where the seq_len_kv should equal inputs' seq_len
</span>    <span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding_src</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">from_encoder</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs_mask</span><span class="p">)</span>
    <span class="c1"># shape: (batch_size, seq_len, dim)
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">teacher_forcing</span><span class="p">(</span><span class="n">inputs_mask</span><span class="o">=</span><span class="n">inputs_mask</span><span class="p">,</span> <span class="n">from_encoder</span><span class="o">=</span><span class="n">from_encoder</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span>
<span class="c1"># the outputs are the log probabilities of shape: (batch_size, seq_len, vocab)
</span>
<span class="k">def</span> <span class="nf">teacher_forcing</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs_mask</span><span class="p">,</span> <span class="n">from_encoder</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">pad_mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">create_pad_mask</span><span class="p">(</span><span class="n">targets</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">upper_triangle_mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">create_upper_triangle_mask</span><span class="p">(</span><span class="n">targets</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">upper_triangle_mask</span> <span class="o">=</span> <span class="n">upper_triangle_mask</span> <span class="o">+</span> <span class="n">pad_mask</span>  <span class="c1"># batch_size, seq_len, seq_len
</span>    <span class="c1"># now the dtype is int, should convert to bool later
</span>    <span class="c1"># the upper_triangle_mask should not only mask the unseen words but also the paddings
</span>    <span class="n">targets</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding_tgt</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">from_encoder</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">inputs_mask</span><span class="p">,</span> <span class="n">upper_triangle_mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">bool</span><span class="p">())</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">projection</span><span class="p">(</span><span class="n">outputs</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># shape: (batch_size, seq_len, dim ---&gt; vocab)
</span>
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div> <p>During training the Transformer adapts the <strong>teacher-forcing training strategy</strong>, which means the decoder at time-step $t$ will always be fed the ground truth output token $y_t$ instead of using its own prediction $\hat{y}_t$ from time-step $t-1$. Another interesting observation is that <strong>the final output is not the probabilities but the log of them</strong>. (since <strong>F.log_softmax()</strong> is used instead of <strong>F.softmax()</strong>) This is because for loss computing we use the “<strong>nn.KLDivLoss</strong>”, which expects the predictions to be log probabilities. Finally we need to talk about the masks used here: <strong>a</strong>. the <strong>padded mask</strong> for the Encoder is used to mask the paddings in the inputs; <strong>b</strong>. the <strong>subsequent mask</strong> used in the Decoder is a little bit different from what we discussed <a href="#7">[here]</a>, it is actually the combination of a standard <strong>subsequent mask</strong> and a <strong>padded mask</strong>, which function is <strong>not only</strong> to prevent from attending to future tokens ($y_{t+1},\,y_{t+2},\,…$) at time-step $t$ <strong>but also</strong> to mask out the paddings; <strong>c</strong>. in <a href="#2">[2]</a> and <a href="#3">[3]</a> the <strong>padded mask</strong> in the Decoder is used for masking the paddings in source (input, which serves as <strong>K</strong> in <strong>Multi-Head Attention</strong>), however that of <a href="#1">[1]</a> is used for masking the paddings in targets (outputs, which serves as <strong>Q</strong> in <strong>Multi-Head Attention</strong>), and here I follow the implementation of <a href="#2">[2]</a> and <a href="#3">[3]</a>.</p> <p>At last, we now introduce implementation of the inference:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">max_generating_len</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">decoding_method</span><span class="o">=</span><span class="sh">'</span><span class="s">greedy</span><span class="sh">'</span><span class="p">,</span> <span class="n">beam_size</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">inputs_mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">create_pad_mask</span><span class="p">(</span><span class="n">inputs</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding_src</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">from_encoder</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs_mask</span><span class="p">)</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">greedy</span><span class="p">(</span><span class="n">inputs_mask</span><span class="o">=</span><span class="n">inputs_mask</span><span class="p">,</span> <span class="n">from_encoder</span><span class="o">=</span><span class="n">from_encoder</span><span class="p">,</span> <span class="n">max_generating_len</span><span class="o">=</span><span class="n">max_generating_len</span><span class="p">)</span> \
        <span class="k">if</span> <span class="n">decoding_method</span> <span class="o">==</span> <span class="sh">'</span><span class="s">greedy</span><span class="sh">'</span> <span class="k">else</span> \
        <span class="n">self</span><span class="p">.</span><span class="nf">beam_search</span><span class="p">(</span><span class="n">from_encoder</span><span class="o">=</span><span class="n">from_encoder</span><span class="p">,</span> <span class="n">max_generating_len</span><span class="o">=</span><span class="n">max_generating_len</span><span class="p">,</span> <span class="n">beam_size</span><span class="o">=</span><span class="n">beam_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sentences</span>
</code></pre></div></div> <p>for the decoding methods (“<strong>greedy</strong>” and “<strong>beam_search</strong>”), I did a minor adjustment to the decoding methods provided in <a href="#1">[1]</a> (basically the same, and it is already very excellent in <a href="#1">[1]</a>). And here I just introduce the idea of implementations for these two methods.</p> <p>For “<strong>greedy</strong>”:</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/pytorch_learning_1/pic2.png" style="zoom:80%;"></div> <p>For “<strong>beam search</strong>”:</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/pytorch_learning_1/pic3.png" style="zoom:76%;"></div> <h2 id="reference">Reference</h2> <p><a name="0"></a></p> <p>The following projects/websites are used as code references:</p> <p>[1] <a href="https://github.com/L-Zhe/SDISS" rel="external nofollow noopener" target="_blank">SDISS</a> is a Transformer-based sentence simplification model <a name="1"></a></p> <p>[2] <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="external nofollow noopener" target="_blank">The Annotated Transformer</a> is a post about the Transformer and its implementation <a name="2"></a></p> <p>[3] <a href="https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec" rel="external nofollow noopener" target="_blank">How to code The Transformer in Pytorch</a> is a post about the Transformer and its implementation <a name="3"></a></p> </body> </html>