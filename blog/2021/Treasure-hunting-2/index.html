<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Treasure Hunting 2 | Yuchen Shen </title> <meta name="author" content="Yuchen Shen"> <meta name="description" content="treasure hunting"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://a-chicharito-s.github.io/blog/2021/Treasure-hunting-2/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yuchen </span> Shen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Treasure Hunting 2</h1> <p class="post-meta"> December 07, 2021 </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a>   ·   <a href="/blog/tag/formatting"> <i class="fa-solid fa-hashtag fa-sm"></i> formatting</a>   <a href="/blog/tag/links"> <i class="fa-solid fa-hashtag fa-sm"></i> links</a>     ·   <a href="/blog/category/sample-posts"> <i class="fa-solid fa-tag fa-sm"></i> sample-posts</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script> <h2 id="conditional-poisson-stochastic-beam-search">Conditional Poisson Stochastic Beam Search</h2> <p>Today I’m going to introduce the paper ‘<strong>Conditional Poisson Stochastic Beam Search</strong>’ by <em>Clara Meister</em>, <em>Afra Amini</em>, <em>Tim Viera</em>, and <em>Ryan Cotterell</em>, you can find the paper <a href="https://arxiv.org/abs/2109.11034" rel="external nofollow noopener" target="_blank">[here]</a></p> <p>And different from the previous ‘<strong>Treasure Hunting</strong>’ episodes, in this very episode, instead of introducing the paper following its structure, I re-arrange the paper and introduce it in a more logical way that I consider myself better to understand. (<strong>And to be honest the paper is really tough for me !</strong>) Though still some parts of the paper remain unclear for me, I try my best to illustrate its main ideas.</p> <p>Note that: ‘<strong>xxx</strong>’ means quoted from the paper; <strong>xxx</strong> is to underline; <strong><em>sub_title</em></strong> is the sub title I named to help understand the structure of the paper; ‘<em>xxx</em>’ is the question / comments I raised to help understand the idea/logic of the paper; And the formulas are all from the paper (some minor modification for consistence with my own signs are done), and for writing fluency, I sometimes introduce concepts as ‘We define …’ instead of ‘The authors define …’</p> <h3 id="deterministic-vs-stochastic">Deterministic vs. Stochastic</h3> <ul> <li><strong><em>What is beam search ?</em></strong></li> </ul> <p><strong>Beam search</strong> is a very important decoding strategy for NLP tasks involving generation (e.g., NMT, text generation), they’re usually done in the following way:</p> <p>For example, we’re going to generate a sentence with a vocabulary $\mathcal{V}=\{I, like, spring, EOS\}$, where $BOS$ only indicates the end of the sentence. At the beginning of generation, we start with a $BOS$ token which indicates the beginning of the sentence. And beam search aims to find the top-$K$ possible decoding options at each time step, thus, at step 1, we may have a probability of ${I=0.6, like=0.1, spring=0.3}$, indicating the likelihood of a word from $\mathcal{V}$ being placed at the step ( at step 1, which means after $BOS$), and let the $K=2$ , then the decoding process can be roughly illustrated as: <a name="1"></a></p> <div> $$\begin{split} \{BOS\} &amp; \stackrel{step 1}{\longrightarrow}\{BOS+I={\color{red}0.6},BOS+like=0.1,BOS+spring={\color{red}0.3},BOS+EOS=0.0\} \\ &amp; \stackrel{step 2}{\longrightarrow}\{I+like={\color{red}0.6\times 0.9},I+I=0.6\times 0.05,I+spring=0.6\times 0.05,I\,+EOS=0.6\times 0.0; \\ &amp; \,\,\,\,\,\,\,\,\,\,\,\,\,\,spring+like={\color{red}0.3\times 0.8},spring+I=0.8\times 0.05,spring+spring=0.8\times 0.1, \\ &amp; \,\,\,\,\,\,\,\,\,\,\,\,\,\,spring\,+EOS=0.8\times 0.05;\} \\ &amp; \stackrel{step 3}{\longrightarrow}\,\,\,\,... \end{split}$$ </div> <p>Where ‘$+$’ means appending the word after the sequence from previous step, and <strong>for simplicity $BOS$ is removed after step 1</strong>. And we can see that each step the preserved top-$K$ beams (with highest top-$K$ probability of being generated, marked <font color="red">red</font>) will have a possible extension over $\mathcal{V}$, generating $K\times |\mathcal{V}|$ candidates, and then top-$K$ high probability beams are selected then (in above case, at step 2, they are: ‘<strong><em>I like</em></strong>’ and ‘<strong><em>spring like</em></strong>’).</p> <ul> <li><strong><em>What’s the drawbacks of beam search ? And can we solve them ?</em></strong></li> </ul> <p>With the above brief introduction about beam search, now we face a very tough question: <strong><em>What if the $K+1$-th beam at step $t$ is actually a better beam at step $t+1$ ?</em></strong> That means, by <strong>deterministically</strong> select top-$K$ best options out of $K\times |\mathcal{V}|$ at each time step, <strong>we may lose some better candidates</strong> ! And we of course want a generalized version of beam search, which can include beam search as a special case while has some <strong>stochasticity</strong>. $\longrightarrow$ and this leads to the answer: <strong>by sampling !</strong> Instead of deterministically choose top-$K$ beams at each time step, we <strong>stochastically</strong> sample a set with $K$ beams out of a base set with $K\times |\mathcal{V}|$ beams, and of cause a set with top-$K$ beams (in this sense, it means <strong>beam search</strong>) will still have a probability being sampled, thus we can say the <strong>stochastic sampling strategy</strong> is the generation of the <strong>deterministic</strong> beam search.</p> <ul> <li><strong><em>How the problem is formed ?</em></strong></li> </ul> <p>And the authors formulate the <strong>beam search</strong> problem as follows:</p> <p>the generation process can be written as:</p> <div> $$p(\textbf{y})=\prod \limits_{t=1}^{\|\textbf{y}\|}p(y_t|\textbf{y}_{&lt;t})$$ </div> <p>‘<strong>where $\textbf{y}$ is a member of a set of well-formed outputs $\mathcal{Y}$</strong> ‘. And $\textbf{y}=\{y_1,\,y_2,\,… \}$ where $y_k \in \mathcal{Y}$, $\textbf{y}_{&lt; t}=\{y_1,\,y_2,\,…,y_{t-1},\,y_t\}$. And in the following discussion, a max generation length $T$ for the sentence is considered.</p> <p>To solve the problem of $\textbf{y}^{*}=\underset{y\in \mathcal{Y}}{argmax}\,\,log\,p(\textbf{y})$, the beam search is then formulated as:</p> \[\begin{eqnarray*} Y_0 &amp;=&amp; {BOS} \tag{1} \\ Y_t &amp;=&amp; \underset{Y_t^{'}\subseteq B_t}{argmax}\,\,Q_t({Y_t^{'}}\,\|\,Y_{t-1}) \tag{2} \\ re&amp;turn\,Y_T \end{eqnarray*}\] <p>Where:</p> <div> $$Q_t(Y_t\,\|\,Y_{t-1})\overset{def}{\propto} \begin{cases} \prod \limits_{n=1}^{N}w_n &amp; \text{if |Y|=K}\\ 0&amp; \textbf{otherwise} \end{cases}\qquad \qquad \qquad \qquad \qquad (3)$$ </div> <p>Note that $Q_t(Y_t\,|\,Y_{t-1})$ is only assigned value when $|Y_t|=K$, and the though the assigned value is written as $\prod \limits_{n=1}^{N}w_n$ , it actually means for those $w_n$’s belonging to the set $Y_t$ . For example, if $K=3,\,N=9$ and ${w_1,\,w_4,\,w_5}$ belongs to $Y_t$ , then $\prod \limits_{n=1}^{N}w_n$ indicates $w_1\times w_4\times w_5$ . <a name="2"></a></p> <p>And now let’s continue to sort out some undefined concepts:</p> <p>if we define steps as $t=1,\,2,\,…T$, and $Y_{t-1}\,\circ\,V\overset{def}{=}{\textbf{y}\,\circ\,y\,|\,\textbf{y}\in Y_{t-1}\,\,\textbf{and}\,\,y\in V}$ , where $\circ$ means concatenation (which is the ‘$+$’ in the <a href="#1">above-mentioned case</a> ), and also: $B_t\overset{def}{=}Y_{t-1}\,\circ\,V$, thus $B_t$ is actually: ${\textbf{y}_{\leq t}^{(1)},\,…\textbf{y}_{\leq t}^{(N)}}$ where $N=K\times |\mathcal{V}|$ (except when $t=0$ since there is only a choice of $|\mathcal{V}|$ words for $BOS$), again <strong>for simplicity, ${\textbf{y}_{\leq t}^{(1)},\,…\textbf{y}_{\leq t}^{(N)}}$ is represented as ${1,\,2,\,…N}$ .</strong> And $w_n\,(=p(\textbf{y}_{\leq t}^{(n)}))$ indicates the probability of generation under the model (e.g., $spring+like={\color{red}0.3\times 0.8}$)</p> <h3 id="conditional-poisson-stochastic-beams">Conditional Poisson Stochastic Beams</h3> <ul> <li><strong><em>why called ‘conditional Poisson stochastic beam search’ ?</em></strong></li> </ul> <p>With the above mentioned definitions, a normalization to ‘<strong>the time-step dependent set function</strong>’ $Q_t(Y_t\,|\,Y_{t-1})$ (which makes it a distribution) can derive the ‘<strong>sample-without-replacement</strong>’ (‘<strong>without-replacement</strong>’ means one element after being chosen, can’t be chosen again) version of beam search:</p> <div> $$\begin{eqnarray*} Y_0 &amp;=&amp; {BOS} \tag{4} \\ Y_t &amp;\sim&amp; Q_t({Y_t^{'}}\,|\,Y_{t-1}) \tag{5} \\ re&amp;turn\,Y_T \end{eqnarray*}$$ </div> <p>And ‘<strong>This recursion corresponds to performing conditional Poisson sampling (CPS; Hájek 1964;see App. A for overview), a common sampling-without-replacement design (Tillé, 2006)<sup>3</sup>, at every time step.</strong>’ where in <strong>3</strong> the authors explain: ‘<strong>A sampling design is a probability distribution over sets of samples.</strong>’ And that’s why the proposed work is referred to as ‘<strong>conditional Poisson stochastic beam search</strong>’ (CPSBS)</p> <ul> <li><strong><em>How is it performed in detail ?</em></strong></li> </ul> <p>First of all, to understand the probability of a size $K$ set $Y_T$ is sampled, its marginal probability can be written as follows:</p> <div> $$P(Y_T)\,=\sum_{Y_1}...\sum_{Y_{T-1}}\prod \limits_{t=1}^{T}Q_t\,(\,Y_t\,|\,Y_{t-1})\qquad \qquad \qquad \qquad \qquad (6)$$ </div> <p><a name="5"></a></p> <p>And the summation is actually computing the marginal distribution out of a joint distribution. The above marginal distribution tells us that: for the final beam set $Y_T$ of size $K$, there’re roughly (<strong>less than</strong>): $|\text{#}Y_1|\times |\text{#}Y_2|\times ··· \times |\text{#}Y_{T-1}| \times |\text{#}Y_T|$ available values to be assigned with, where $|\text{#}Y_t|$ denotes the number of possible values for set $Y_t$ at time-step $t$ . And the authors state that: ‘<strong>Note the structural zeros of $Q_t$ prevent any incompatible sequence of beams</strong>’ , which can be answered by the following example:</p> <p>For a $K=2$ CPSBS with a vocabulary $\mathcal{V}={1,\,2,\,…\,,7}$ . If at $t=1$ , $Y_1$ can be: ${BOS+1,\,BOS+3}$ , then at $t=2$, $Y_2$ can be: ${BOS+12,\,BOS+15,\,BOS+32,\,BOS+34}$ .</p> <p><strong>However</strong>, note that $Q_2(Y_2=BOS+12\,|\,Y_1=BOS+3)$ and $Q_2(Y_2=BOS+15\,|\,Y_1=BOS+3)$ are both incompatible, vice versa. And this <strong>explains</strong> <strong>why</strong> $Q_t=0$ can prevent ‘<strong>incompatible sequence of beams</strong>’ and <strong>why</strong> the assignable values are <strong>less than</strong> the multiplication of available values at each time step.</p> <p>And also, for a given $Y_T^{(m)}=\{\textbf{y}_{\leq T}^{(m_1)}\,,\,\textbf{y}_{\leq T}^{(m_2)}\,,\,…,\,\textbf{y}_{\leq T}^{(m_K)}\}$, it’s generation probability can be simply computed as (<strong>no need to compute the summation</strong>, since the stochastic sample at each time-step should be deterministic in order to generate $Y_T^{(m)}$, to be specific, it means only $BOS+1$ can generate $BOS+12$ and $BOS+15$) : $P(Y_T=Y_T^{(m)})\,=\prod \limits_{t=1}^{T}Q_t\,(\,Y_t=\{\textbf{y}_{\leq t}^{(m_1)}\,,\,\textbf{y}_{\leq t}^{(m_2)}\,,\,…,\,\textbf{y}_{\leq t}^{(m_K)}\}\,|\,Y_{t-1}=\{\textbf{y}_{\leq t-1}^{(m_1)}\,,\,\textbf{y}_{\leq t-1}^{(m_2)}\,,\,…,\,\textbf{y}_{\leq t-1}^{(m_K)}\})$</p> <p>Thus, for a given final beam set $Y_T^{(m)}$ , we can actually compute its generation probability.</p> <ul> <li><strong><em>How is CPSBS performed ?</em></strong></li> </ul> <p>Now we formally look deeper into how CPSBS is performed at time-step $t$ . Before we start, bear in mind that we have a few things to do to perform CPSBS: <strong>1</strong>. the previous set function $Q_t(Y_t\,|\,Y_{t-1})$ is a scoring function (see <a href="#2">here</a>), we need to convert it into a distribution to perform sampling; <strong>2</strong>. an efficient and general algorithm should be there for us to perform sampling at each time-step; With these two preliminaries acknowledged, we now see how CPSBS is performed by the authors:</p> <p>Step 1: Normalize $Q_t(·\,|\,Y_{t-1})$ .</p> <p>We know that now $Q_t(·\,|\,Y_{t-1})$ should be able to sample a $Y_t^{any}$ containing $K$ beams based on the previous size-$K$ set $Y_{t-1}$ , and since we are actually selecting $K$ beams out of $N=K \times |\mathcal{V}|$ , there are actually $\binom{N}{K}$ options for us to sample such a size-$K$ set $Y_t^{any}$ , thus, $p(Y_t^{any})$ should be modified by the summation of the probabilities of size-$K$ sets that are possible to be sampled, and the normalization constant is defined as:</p> <div> $$Z_t\overset{def}{=}\sum_{Y_t\subseteq B_t,\,|Y_t|=K} \prod \limits_{n=1}^{N}w_n\qquad \qquad \qquad \qquad \qquad (7)$$ </div> <p><a name="4"></a></p> <p>Where the notation $\prod \limits_{n=1}^{N}w_n$ still follows the meaning of <a href="#2">this</a> . And following Kulesza and Taskar (2012, see <a href="https://www.nowpublishers.com/article/Details/MAL-044" rel="external nofollow noopener" target="_blank">here</a>), an iterative algorithm can be proposed: ( For detailed pseudocode please refer to the App. C of the paper)</p> <div> $$W\binom{n}{k}=\begin{cases} 1&amp; \text{if k=0 or n=k}\\ W\binom{n-1}{k}+w_nW\binom{n-1}{k-1}&amp; \text{if k} \in (0,n)\\ 0&amp; \text{otherwise} \end{cases}\qquad \qquad \qquad \qquad \qquad (8)$$ </div> <p><a name="3"></a></p> <p>And $Z_t=W\binom{N}{K}$.</p> <p>Step 2: Sample from $Q_t(·\,|\,Y_{t-1})$ (normalized) .</p> <p>After the distribution $Q_t(·\,|\,Y_{t-1})$ is normalized, the following algorithm is proposed by the authors:</p> <p>1: $Y_t \longleftarrow \emptyset$ (<em>Initialization</em>)<br> <br> 2: <strong>for</strong>$\,\,n=N,\,…\,,1$ :<br></p> <p>   $\qquad k\longleftarrow K-|Y_t|$ (<em>Number of remaining elements</em>)<br></p> <p>      Add the $n^{th}$ element of $B_t$ to $Y_t$ with probability:<br></p> <p>                               $\frac{w_n\,W\binom{n-1}{k-1}}{W\binom{n}{k}}$<br></p> <p>3: <strong>return</strong> $Y_t$ (Guaranteed to have size $K$)<br></p> <p>And I explain the <strong>why</strong> the probability is $\frac{w_n\,W\binom{n-1}{k-1}}{W\binom{n}{k}}$ and <strong>why</strong> it is guaranteed to have size $K$ as follows:</p> <p><em>why the probability is $\frac{w_n\,W\binom{n-1}{k-1}}{W\binom{n}{k}}$ ?</em></p> <p>We can consider $\frac{w_n\,W\binom{n-1}{k-1}}{W\binom{n}{k}}$ as the probability of the $n^{th}$ element of $B_t$ being <strong>included</strong> in the final $Y_t$ . With <a href="#3">(8)</a> , we can derive:</p> <div> $$\frac{w_n\,W\binom{n-1}{k-1}}{W\binom{n}{k}}=\frac{W\binom{n}{k}-W\binom{n-1}{k}}{W\binom{n}{k}}=1-\frac{W\binom{n-1}{k}}{W\binom{n}{k}}$$ </div> <p>Where we consider $\frac{W\binom{n-1}{k}}{W\binom{n}{k}}$ as the probability of the $n^{th}$ element of $B_t$ being <strong>excluded</strong> in the final $Y_t$ . And according <a href="#4">(7)</a> and <a href="#3">(8)</a>, we can interpret $W\binom{n}{k}$ as the total probability of choosing the remaining $k$ of the total $K$-to-be-chosen elements <strong>out of</strong> the available element set : ${n,\,n-1,\,…\,1}$, and since the elements are chosen in a reverse order (from $N$ to $1$), thus $W\binom{n-1}{k}$ is then the total probability of choosing the remaining $k$ of the total $K$-to-be-chosen elements <strong>out of</strong> the available element set : ${n-1,\,…\,1}$ where element $n$ is excluded. Thus, the probability of the $n^{th}$ element of $B_t$ being <strong>excluded</strong> in the final $Y_t$ is: probability of choosing $k$ elements without element $n$ / probability of choosing $k$ elements considering element $n$ (though it may not be necessarily chosen), which is: $\frac{W\binom{n-1}{k}}{W\binom{n}{k}}$ .</p> <p><em>why it is guaranteed to have size $K$ ?</em></p> <p>Let’s consider an extreme situation: for the first $N-K$ elements, namely ${N,\,…\,,K+1}$, no elements are added to $Y_t$ , and now $n=k\,(=K)$, in this sense, according to <a href="#3">(8)</a>, the next element $K$, is added with a probability $\frac{w_n\,W\binom{n-1}{k-1}}{W\binom{n}{k}}=1-\frac{W\binom{n-1}{k}}{W\binom{n}{k}}$ ( <strong>though this only holds when $k\in (0,\,n)$</strong> ) where $W\binom{n}{k}=1$ and $W\binom{n-1}{k}=0$, thus, with a deterministic probability ‘1’ , the element $n$ will then be added, and $n,\,k$ again equals $K-1$, which means the above selecting process goes on again until all the remaining elements are added.</p> <h3 id="statistical-estimation-with-cpsbs">Statistical Estimation with CPSBS</h3> <ul> <li><strong><em>A supplementary: what is inclusion probability ?</em></strong></li> </ul> <p>For a certain beam $\textbf{y}_{\leq t}^{(n)}$ at time-step $t$, what’s its probability of being included in the $Y_t$ of this time-step? (i.e., $Pr(\textbf{y}_{\leq t}^{(n)}\in Y_t)$, which is called the <strong>inclusion probability</strong>) To understand this, we denote the inclusion probability of $\textbf{y}_{\leq t}^{(n)}$ ( <em>w.r.t.</em> $Q_t(·\,|\,Y_{t-1})$ ) as :</p> <div> $$\pi_{Q_t}(\textbf{y}_{\leq t}^{(n)}\,\|\,Y_{t-1})\overset{def}{=}\sum_{Y_t}Q_t(Y_t\,\|\,Y_{t-1})\,\mathbb{1}(\textbf{y}_{\leq t}^{(n)}\in Y_t)\qquad \qquad \qquad \qquad \qquad (9)$$ </div> <p>Where $Y_t$ ranges over all the possible size-$K$ set sampled from the base set $B_t$ , and $\mathbb{1}(\textbf{y}_{\leq t}^{(n)}\in Y_t)$ is an indicator, equals <strong>one</strong> if the desired beam $\textbf{y}_{\leq t}^{(n)}$ is in $Y_t$ , <strong>zero otherwise</strong>. And if at time-step $t$ we choose $w_n$ to make $\pi_{Q_t}(\textbf{y}_{\leq t}^{(n)}\,|\,Y_{t-1})\approx p(\textbf{y}_{\leq t}^{(n)})$ . It can recover beam search as we anneal the chosen weights: $w_n\rightarrow w_n^{1/\tau}$ as $\tau \rightarrow 0$, and the conditional Poisson distribution will assign probability 1 to the set containing the top-$K$ beams at time-step $t$ . And finding such $w_n$ s resulting a desired inclusion probability is possible, though requires solving a numerical optimization problem (Aires, 1999 {see <a href="https://link.springer.com/article/10.1023/A:1010091628740" rel="external nofollow noopener" target="_blank">[paper]</a>}; Grafström, 2009 {see <a href="https://www.sciencedirect.com/science/article/pii/S037837580800387X?via%3Dihub" rel="external nofollow noopener" target="_blank">[paper]</a>}) , thus the authors use an approximation of $w_n=p(\textbf{y}_{\leq t}^{(n)})/(1-p(\textbf{y}_{\leq t}^{(n)}))$ which yields good approximation in both theory and practice as reported in (Hájek, 1981 {see [[paper]]}; Bondesson et al. {see <a href="https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9469.2006.00497.x" rel="external nofollow noopener" target="_blank">[paper]</a>}, 2006; Aires, 1999 {see <a href="https://link.springer.com/article/10.1023/A:1010091628740" rel="external nofollow noopener" target="_blank">[paper]</a>}) .</p> <ul> <li><strong><em>How do we estimate statistical features of CPSBS ?</em></strong></li> </ul> <p>With above-mentioned sampling process of CPSBS at each time-step known, now we of course can’t help thinking about some questions like: ‘How to calculate one specific beam, say: $\textbf{y}^{(m)}$’s entropy ?’ or ‘How do we compute the BLEU score (see <a href="https://aclanthology.org/P02-1040.pdf" rel="external nofollow noopener" target="_blank">BLEU</a>) of $\textbf{y}^{(m)}$ ?’ . And unlike beam search where $\textbf{y}^{(m)}$ appears deterministically, in CPSBS, $\textbf{y}^{(m)}$ can be in many different $Y_T$ s, which leads us to the statistical estimation of the CPSBS. In a more mathematical way, as the authors state: ‘<strong>Let be $f:\,\mathcal{Y}\rightarrow \mathbb{R}^{d}$ , we seek to approximate its expected value under $p$:</strong>’</p> <div> $$\mathbb{E}_{\textbf{y}\sim p}[f(\textbf{y})]=\sum_{\textbf{y}\in \mathcal{Y}}p(\textbf{y})f(\textbf{y})\qquad \qquad \qquad \qquad \qquad (10)$$ </div> <p>And a traditional way is the Monte Carlo estimator: $G_{MC}\overset{def}{=}\frac{1}{M}\sum_{m=1}^{M}f(\textbf{y}^{(m)})$ where $\textbf{y}^{(m)}\overset{i.i.d}{\sim}p$ , and the authors argue that: ‘<strong>However, in the special case of sampling from a finite population—which is extremely common in NLP—it can be very wasteful. For example, if a distribution is very peaked, it will sample the same item repeatedly; this could lead to inaccurate approximations for some $f$. As a consequence, the mean square error (MSE) of the estimator with respect to $\mathbb{E}_{\textbf{y}\sim p}[f(\textbf{y})]$ can be quite high for small M.</strong>’ And since the sampling process of CPSBS is not independent (which means $\textbf{y}\nsim p$) , a <strong>Horvitz–Thompson estimator</strong> (see <a href="https://www.jstor.org/stable/2280784" rel="external nofollow noopener" target="_blank">[paper]</a> here) is used to estimate the expectation of a certain $f$ over $Y_T\sim P$ , where $P$ is <a href="#5">this</a> :</p> <div> $$G_{HT}\overset{def}{=}\sum_{\textbf{y}\in Y_T}\frac{p(\textbf{y})}{\pi_P(\textbf{y})}f(\textbf{y})\qquad \qquad \qquad \qquad \qquad (11)$$ </div> <p><a name="10"></a></p> <ul> <li><strong><em>Estimate the inclusion probability $\pi_P(y)$</em></strong></li> </ul> <p>As equation (11) mentions, to use the Horvitz–Thompson estimator we need to know the probability of generation: $p(y)$ (which is rather easy since it simply equals that of $Y_T$) and the inclusion probability: $\pi_P(y)$ . However, though computing $\pi_P(y)$ can give us an ‘<strong>unbiased</strong>’ HT estimator (for detailed information please refer to App. B. of the paper), <strong>to actually compute such a value is almost impossible</strong> (see the summations in the following <a href="#6">equation</a>). And of course, we have two ways: <strong>Naive Monte Carlo</strong> and <strong>Importance sampling</strong> to estimate the following inclusion probability:</p> <div> $$\pi_P(y)=\sum_{Y_T}P(Y_T)\,\mathbb{1}(\textbf{y}\in Y_T)=\sum_{Y_1}···\sum_{Y_T}\prod \limits_{t=1}^{T}Q_t\,(\,Y_t\,|\,Y_{t-1})\,\mathbb{1}(\textbf{y}_{\leq t}\in Y_t)\qquad \qquad \qquad \qquad \qquad (12)$$ </div> <p><a name="6"></a></p> <p>The Naive Monte Carlo estimator:</p> <p>We can defined the Naive Monte Carlo estimator as follows:</p> <div> $$\hat{\pi}_P^{MC}(\textbf{y})\overset{def}{=}\frac{1}{M}\sum_{m=1}^{M}\mathbb{1}(\textbf{y}\in Y_T^{(m)})\qquad \qquad \qquad \qquad \qquad (13)$$ </div> <p><a name="7"></a></p> <p>Where $Y^{(m)}\sim P$ . And ‘<strong>$\hat{\pi}_p^{MC}$ is an unbiased estimator of $\pi_P$ with variance : $\mathbb{V}[\hat{\pi}_P^{MC}]=\frac{1}{M}(\pi_P(\textbf{y})-\pi_P(\textbf{y})^{2})$ . Meanwhile $1/\hat{\pi}_P^{MC}$ is a <em>consistent estimator</em> of $1/\pi_P$ with <em>asymptotic variance</em> : $\mathbb{V}_a[\frac{1}{\hat{\pi}_P^{MC}}]=\frac{1}{M}(\frac{1}{\pi_P(\textbf{y})^{3}}-\frac{1}{\pi_P(\textbf{y})^{2}})$</strong>’ . (for proof please refer to the App. B.2. of the paper)</p> <p>And we can easily see the problems of the Naive Monte Carlo estimator. For a desired beam $\textbf{y}$ with a very low $\pi_P(\textbf{y})$ , the asymptotic variance of its estimated reverse is very high, thus resulting a less accurate estimate. Also, consider the summation in <a href="#7">(13)</a> , most of the sampled $Y_T^{(m)}$ s may not contain the desired beam $\textbf{y}$ , thus the sampling process will be time-consuming in order to have one $Y_T^{(m)}$ contain the desired beam $\textbf{y}$ , and most cases are the estimate of rare desired beam $\textbf{y}$ s tend to be zero.</p> <p>Importance sampling:</p> <p>To solve the problem of sampling lots of $Y_T^{(m)}$ s to contain the desired beam $\textbf{y}$ . We can actually make the sampled set include the desired beam $\textbf{y}$ , which the authors call: <strong>hindsight samples</strong> : $\tilde{Y}_1,\,\tilde{Y}_2,\,…,\tilde{Y}_T$ where they all contain the desired beam $\textbf{y}$ . And the hindsight samples can be generated through a <strong>proposal distribution</strong> conditioned on $\textbf{y}$ :</p> <div> $$\tilde{Q}_t(\tilde{Y}_t\,|\,\tilde{Y}_{t-1}\,,\textbf{y})\overset{def}{=}\frac{Q_t(\tilde{Y}_t\,|\,\tilde{Y}_{t-1})}{\pi_{Q_t}(\textbf{y}_{\leq t}\,|\,\tilde{Y}_{t-1})}\qquad \qquad \qquad \qquad \qquad (14)$$ </div> <p><a name="9"></a></p> <p>And according to the authors, the proposal distribution can be done by a simply modification to the CPSBS algorithm: ‘<strong>where $w(\textbf{y})$ corresponding to $\textbf{y}_{\leq t}^{(n)}$ is plcaed at the beginning and added to $Y_t$ deterministically</strong>’. For simplicity, the fact $\tilde{Y}$ and $\tilde{Q}$ are conditioned on $\textbf{y}$ are omitted. And the following lemma is proposed by the authors (see the proof in the App. B.2. of the paper):</p> <div> $$\tilde{P}(\tilde{Y}_1,\,...,\tilde{Y}_T)=\frac{P(\tilde{Y}_1,\,...,\tilde{Y}_T)}{\prod \limits_{t=1}^{T}\pi_{Q_t}(\textbf{y}_{\leq t}\,|\,\tilde{Y}_{t-1})}\qquad \qquad \qquad \qquad \qquad (15)$$ </div> <p><a name="8"></a></p> <p>Where $\tilde{P}(\tilde{Y}_1,\,…,\tilde{Y}_T)\overset{def}{=}\prod \limits_{t=1}^{T}\tilde{Q}_t(\tilde{Y}_t\,|\,\tilde{Y}_{t-1})$ is the <em>joint proposal distribution</em> . And $P(\tilde{Y}_1,\,…,\tilde{Y}_T)\overset{def}{=}\prod \limits_{t=1}^{T}Q_t(\tilde{Y}_t\,|\,\tilde{Y}_{t-1})$ is defined as the <em>joint probability of the beams under the original distribution $Q_t$</em> . And both $P$ and $\tilde{P}$ conditioning on $Y_0$ are omitted.</p> <p>And the computation for <a href="#8">(15)</a> makes use of the fact that the inclusion probability $\pi_{Q_t}(\textbf{y}_{\leq t})$ for a given $Q_t$ at each time-step can be computed with dynamic programming: (see the pseudocode in App. C. of the paper)</p> <div> $$\begin{eqnarray*}\pi_{Q_t}(\textbf{y}_{\leq t}^{(n)}\,|\,Y_{t-1}) &amp;\overset{def}{=}&amp; \sum_Y Q_t(Y_t)\,\mathbb{1}(\textbf{y}_{\leq t}^{(n)}\in Y_t) \\ &amp;=&amp; \frac{w_n}{Z}\frac{\partial Z}{\partial w_n} \tag{16} \end{eqnarray*}$$ </div> <p>Where $\textbf{y}_{\leq t}^{(n)}$ indicates the $n$-th candidate beam out of the $N$ beams. And for $\tilde{Y}_T^{(m)}\sim \tilde{P}$ where $\tilde{P}$ is defined by the proposal distribution in <a href="#9">(14)</a> . The inclusion probability in the HT estimator mentioned in <a href="#10">(11)</a> can be estimated as:</p> <div> $$\hat{\pi}_P^{IS}(\textbf{y})\overset{def}{=}\frac{1}{M}\sum_{m=1}^{M}\prod \limits_{t=1}^{T}\pi_{Q_t}(\textbf{y}_{\leq t}\,|\,\tilde{Y}_{t-1}^{(m)})\qquad \qquad \qquad \qquad \qquad (17)$$ </div> <p><a name="11"></a></p> <p>And the above estimation can be derived from:</p> <div> $$\begin{eqnarray*} \sum_{Y_T}P(Y_T)\,\mathbb{1}(\textbf{y}\in Y_T)&amp;=&amp;\sum_{Y_T}...\sum_{Y_1}P(Y_1,\,...\,,Y_T)\,\mathbb{1}(\textbf{y}\in Y_T)\\ &amp;=&amp;\sum_{\tilde{Y}_T}...\sum_{\tilde{Y}_1}P(\tilde{Y}_1,\,...\,,\tilde{Y}_T)\,\frac{\tilde{P}(\tilde{Y}_1,\,...\,,\tilde{Y}_T)}{\tilde{P}(\tilde{Y}_1,\,...\,,\tilde{Y}_T)} \\ &amp;=&amp;\sum_{\tilde{Y}_T}...\sum_{\tilde{Y}_1}\tilde{P}(\tilde{Y}_1,\,...\,,\tilde{Y}_T)\,\frac{P(\tilde{Y}_1,\,...\,,\tilde{Y}_T)}{\tilde{P}(\tilde{Y}_1,\,...\,,\tilde{Y}_T)} \\ &amp;\overset{lemma}{=}&amp;\sum_{\tilde{Y}_T}...\sum_{\tilde{Y}_1}\tilde{P}(\tilde{Y}_1,\,...\,,\tilde{Y}_T)\,\prod \limits_{t=1}^{T}\pi_{Q_t}(\textbf{y}_{\leq t}\,|\,\tilde{Y}_{t-1}) \\ \end{eqnarray*}$$ </div> <p>Which indicates that <a href="#11">(17)</a> inherits unbiasedness from the Naive Monte Carlo estimator in <a href="#7">(13)</a> . And the following properties can be observed from the Importance Sampling strategy in <a href="#11">(17)</a> :</p> <p>‘<strong><em>$\hat{\pi}_p^{IS}$ is an unbiased estimator of $\pi_P$ . Meanwhile $1/\hat{\pi}_P^{IS}$ is a consistent estimator of $1/\pi_P$ with an upper bound on asymptotic variance : $\mathbb{V}_a[\frac{1}{\hat{\pi}_P^{MC}}]\leq\frac{1}{M}\frac{r-1}{\pi_P(\textbf{y})^{2}}$ where an assumption that “for all $\tilde{Y}_1,\,…\,,\tilde{Y}_T$ the following bound: $\frac{\prod \limits_{t=1}^{T}\pi_{Q_t}(\textbf{y}_{\leq t}\,|\,\tilde{Y}_{t-1})}{\pi_P(\textbf{y})}\leq r$ holds” is made</em></strong>’ (for proof please refer to the App. B.2. of the paper). And the authors also mention that when $\prod \limits_{t=1}^{T}\pi_{Q_t}(\textbf{y}_{\leq t}\,|\,\tilde{Y}_{t-1})$ approximates the real $\pi_P(\textbf{y})$ , the variance of the Importance Sampling estimate is relatively smaller that that of the Naive Monte Carlo, ‘<strong>which is often the case for estimators when a proposal distribution is chosen judiciously (Rubinstein and Kroese, 2016).</strong>’ (see <a href="https://www.wiley.com/en-us/Simulation+and+the+Monte+Carlo+Method%2C+3rd+Edition-p-9781118632161" rel="external nofollow noopener" target="_blank">[paper]</a>)</p> <h3 id="experiments">Experiments</h3> <p>To test the performance of CPSBS and its HT estimator, they are tested on the sentence-level BLEU together with the Monte Carlo estimator, Sum and Sample estimator, an estimator for Stochastic Beam Search. To observe behaviors of the HT estimator under both high- and low-entropy setting, the model’s distribution is annealed as: $p_\tau(y_t\,|\,\textbf{y}_{&lt;t})\propto p\,(y_t\,|\,\textbf{y}_{&lt;t})^{\frac{1}{\tau}}$ .</p> <ul> <li><strong><em>Additional estimators to be compared with</em></strong></li> </ul> <p>For <strong>Monte Carlo</strong> sampling strategy with sample size $K$, the estimator: $G_{MC}\overset{def}{=}\frac{1}{M}\sum_{m=1}^{M}f(\textbf{y}^{(m)})$ is used to estimate the expectation of $f$ under the model where: $\textbf{y}^{(1)},\,…\,,\textbf{y}^{(K)}\overset{i.i.d.}{\sim}p$ . And this estimator is also <strong>used as a baseline by computing 50 times with a sample size of 200 each time</strong>.</p> <p>For <strong>Sum and Sample</strong>, the sum-and-sample estimator is an unbiased estimator which takes { a deterministically chosen set $Y$ of size $K-1$ (obtained using beam search in this experiment) and a sampled $\textbf{y}^{‘}$ from the remaining set $supp(p)\backslash Y$ } as input, the estimator can be written as : $G_{SAS}\overset{def}{=}\sum_{k=1}^{K-1}p(\textbf{y}^{(k)})f(\textbf{y}^{(k)})+\left( 1-\sum_{k=1}^{K-1}p(\textbf{y}^{(k)})\right)f(\textbf{y}^{‘})$ .</p> <p>For <strong>Stochastic Beam Search</strong>, which is a similar Sample-Without-Replacement algorithm built on the beam search making use of the truncated Gumbel random variables at each time-step. And a estimator following the Horvitz-Thompson scheme is built (similar to <a href="#10">(11)</a>).</p> <p>And for a more efficient estimation, a truncated distribution is used which preserves the 99% of the probability mass to accelerate the computation of <a href="#4">(7)</a>, which is ‘<strong>similar to the process in nucleus sampling (Holtzman et al., 2020)</strong>’ (see <a href="https://openreview.net/forum?id=rygGQyrFvH" rel="external nofollow noopener" target="_blank">[paper]</a> here). See the differences here:</p> <p><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_2/pic4.png" alt=""></p> <ul> <li><strong><em>the function $f$ s whose expectation is to be computed</em></strong></li> </ul> <p><strong>BLEU score estimation</strong>:</p> <p>the score is defined as : $f(\textbf{y})=BLEU(\textbf{x},\,\textbf{y})$ where if for NMT models, $\textbf{x}$ is the reference translation. The sampling process is repeated 20 times and the mean and variance are plotted for each sample size. And see in <a href="#12">Figure 2.a</a> about the RMSE of the BLUE estimators that when temperature is not relatively high, CPSBS has a quite low RMSE, when the temperature is high, CPSBS becomes biased, which is ‘<strong>similar to Kool et al. (2019)’s observations</strong>’ (see <a href="https://proceedings.mlr.press/v97/kool19a.html" rel="external nofollow noopener" target="_blank">[paper]</a> here).</p> <p><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_2/pic1.png" alt=""></p> <p><strong>Conditional Entropy Estimation</strong>:</p> <p>To estimate a model’s conditional entropy: $f(\textbf{y})=-log\,p(\textbf{y}\,|\,\textbf{x})$ where $\textbf{x}$ can be seen as the initial information necessary to generate the first set of beams, i.e., $Y_1$ . And see in <a href="#12">Figure 2.b</a> to see the RMSE for the conditional entropy estimation.</p> <p><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_2/pic3.png" alt=""></p> <p><strong>RMSE</strong>:</p> <p>The RMSE evaluations for above-mentioned BLUE and conditional entropy are:</p> <p><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_2/pic2.png" alt=""><a name="12"></a></p> <ul> <li><strong><em>Additional experiments</em></strong></li> </ul> <p><strong>Diverse Sampling</strong>:</p> <p>To test the diversity of the sampled translations $Y_T\sim P$, where $w_n=p(\textbf{y}_{\leq t}^{(n)})/(1-p(\textbf{y}_{\leq t}^{(n)}))$ at each time-step $t$ as suggested, an $n$-gram diversity metric is proposed: $D=\sum_{n=1}^{4} \frac{\text{#unique}\,\text{n-grams}\,in\,\text{K}\,\text{strings}}{\text{#}\,\text{n-grams}\,in\,\text{K}\,\text{strings}}$ and three decoding strategy: SBS, DiverseBS and ancestral sampling are compared with CPSBS, results are as follows:</p> <p><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_2/pic5.png" alt=""></p> <p><strong>Decoding time evaluation</strong>:</p> <p>By setting different temperature and sample size, the decoding time for CPSBS, SBS and MC are evaluated:</p> <p><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_2/pic6.png" alt=""></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/Treasure-hunting-1/">Treasure Hunting 1</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/paper-summary-2/">Paper Summary 2(special edition)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/paper-summary-1/">Paper Summary 1</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/paper-summary-3/">Paper Summary 3</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/paper-summary-4/">Paper Summary 4</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Yuchen Shen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>