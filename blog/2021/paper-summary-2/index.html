<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Paper Summary 2(special edition) | Yuchen Shen </title> <meta name="author" content="Yuchen Shen"> <meta name="description" content="paper summary"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://a-chicharito-s.github.io/blog/2021/paper-summary-2/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yuchen </span> Shen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Paper Summary 2(special edition)</h1> <p class="post-meta"> November 17, 2021 </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a>   ·   <a href="/blog/tag/formatting"> <i class="fa-solid fa-hashtag fa-sm"></i> formatting</a>   <a href="/blog/tag/links"> <i class="fa-solid fa-hashtag fa-sm"></i> links</a>     ·   <a href="/blog/category/sample-posts"> <i class="fa-solid fa-tag fa-sm"></i> sample-posts</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script> <h3 id="introduction">Introduction</h3> <p>This is the second paper summary focusing on the ‘best paper award’ winners of IJCAI (2015~2022), and if you want to see some other best papers of AI related major conferences, please refer to <a href="https://jeffhuang.com/best_paper_awards/#ijcai" rel="external nofollow noopener" target="_blank">[here]</a>. And as a <strong>special edition</strong> of the ‘paper summary’ series, here I only focus on the ‘<strong>abstract</strong>’ and ‘<strong>introduction</strong>’ part (which I may quote directly from the paper) instead of the architecture.</p> <h2 id="2020">2020</h2> <ul> <li> <strong>Paper</strong> <h4 id="a-multi-objective-approach-to-mitigate-negative-side-effects-paper">A Multi-Objective Approach to Mitigate Negative Side Effects <a href="https://www.ijcai.org/proceedings/2020/0050.pdf" rel="external nofollow noopener" target="_blank">[paper]</a> </h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“Agents operating in unstructured environments <strong>often</strong> <strong>create negative side effects (NSE)</strong>$^1$ that may not {$\longleftarrow$ <em>1. explain current problems, which is ‘NSE’</em>} be easy to identify at design time. <strong>We examine</strong>$^2$ how various forms of human feedback or autonomous exploration can be used to learn a penalty function associated with NSE during system deployment. {$\longleftarrow$ <em>2. what research we did towards the problem</em>} <strong>We formulate the problem</strong>$^3$ of mitigating the impact of NSE as a multi-objective Markov decision process with lexicographic reward preferences and slack. <strong>The slack denotes</strong>$^3$ the maximum deviation from an optimal policy with respect to the agent’s primary objective allowed in order to mitigate {$\longleftarrow$ <em>3. the main work we’ve done in this paper</em>} NSE as a secondary objective. <strong>Empirical evaluation</strong> <strong>of our approach</strong>$^4$ shows that the proposed framework can successfully mitigate NSE and that different feedback mechanisms introduce different biases, which influence the identification of NSE.” {$\longleftarrow$ <em>4. the results/improvement gains of our work</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>A detailed explanation of why ‘NSE’ will happen:</p> <blockquote> <p>“but inevitably details in the environment that are unrelated to the agent’s primary objective are ignored” “As a result of the limited fidelity of $\tilde{M}$ , the agent’s actions may have unmodeled, undesirable negative side effects (NSE) in some states”</p> </blockquote> </li> <li> <p>What we focus on:</p> <blockquote> <p>“We focus on NSE that are undesirable but not prohibitive.”</p> </blockquote> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_2/pic1.png"></div> </li> <li> <p>The importance of our work (to eliminate ‘NSE’) and the efforts from previous work:</p> <blockquote> <p>“Learning to detect and minimize NSE is critical for safe deployment of autonomous systems” “Existing works (outlined in Table 1) mitigate NSE by recomputing the reward function for the agent’s primary objective”</p> </blockquote> </li> <li> <p>Our work (following quotes are with sequential-time order) and a summary on the contribution:</p> <blockquote> <p>“We propose a multi-objective approach that exploits …”$^1$ “The agent’s primary objective is to achieve its assigned task, while the secondary objective is to minimize NSE.”$^2$ “We investigate the efficiency of different feedback approaches …”$^3$ “Our primary contributions are: (1) …”</p> </blockquote> <p>in ‘1’—the main methods and why are using them are discussed</p> <p>in ‘2’—how the proposed methods work</p> <p>in ‘3’—how are the experiments and ideas behind them</p> </li> </ol> </li> <li> <strong>Paper</strong> <h4 id="synthesizing-aspect-driven-recommendation-explanations-from-reviews-paper"> <strong>Synthesizing Aspect-Driven Recommendation Explanations from Reviews</strong> <a href="https://www.ijcai.org/proceedings/2020/0336.pdf" rel="external nofollow noopener" target="_blank">[paper]</a> </h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“Explanations help to make sense of recommendations, increasing the likelihood of adoption. <strong>However,</strong> <strong>existing approaches</strong> to explainable recommendations tend to rely on rigid, standardized templates, customized only via fill-in-the-blank aspect {$\longleftarrow$ <em>1. point out the existing limits of the field</em>} sentiments. <strong>For more</strong> flexible, literate, and varied explanations covering various aspects of interest, <strong>we synthesize</strong> an explanation by selecting snippets from reviews, while optimizing for representativeness and coherence. <strong>To fit target users’ aspect preferences</strong>, <strong>we contextualize</strong> the opinions based on a compatible explainable recommendation model. {$\longleftarrow$ <em>2. introduce the motivations and our solution</em>} <strong>Experiments</strong> on datasets of several product categories showcase the efficacies of our method as compared to baselines based on templates, review summarization, selection, and text generation.” {$\longleftarrow$ <em>3. explain experiment results (on which how it showed the model is good)</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <p>This is a quite different introduction since it actually has had itself organized by using the bold, descriptive titles in ‘Introduction’</p> <ol> <li> <p>Introducing the task:</p> <blockquote> <p>“Explainable recommendations are motivated by the need for …”</p> </blockquote> </li> <li> <p>Explain the existing problems and previous attempts towards the problem</p> <blockquote> <p>“<strong>Problem</strong>. An explanation is typically generated post hoc to the recommendation model.” “For instance, EFM [Zhang et al., 2014] has standardized templates for positive and negative opinions, each time substituting only the [aspect], e.g.,:”</p> </blockquote> </li> <li> <p>Introducing our contributions: (which includes the brief introduction of the contents of each following subsection)</p> <blockquote> <p>“<strong>Contributions.</strong> We make several contributions in this work.”</p> </blockquote> </li> </ol> </li> </ul> <h2 id="2019">2019</h2> <ul> <li> <strong>Paper</strong> <h4 id="boosting-for-comparison-based-learning--paper"> <strong>Boosting for Comparison-Based Learning</strong> <a href="https://www.ijcai.org/proceedings/2019/0255.pdf" rel="external nofollow noopener" target="_blank">[paper]</a> </h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“We <strong>consider the problem of classification in a</strong> <strong>comparison-based setting</strong>: given a set of objects, we only have access to triplet comparisons of the form object $x_i$ is closer to object $x_j$ than to object {$\longleftarrow$ <em>1. specify the setting of the task/ introducing background</em>} $x_k$. <strong>In this paper we introduce</strong> TripletBoost, a new method that can learn a classifier just from such triplet comparisons. <strong>The main idea</strong> is to aggregate the triplets information into weak classifiers, which can subsequently be boosted to a strong classifier. {$\longleftarrow$ <em>2. the motivation/idea of this work</em>} Our method has <strong>two main advantages</strong>: (i) it is applicable to data from any metric space, and (ii) it can deal with large scale problems using only passively obtained and noisy triplets. We derive theoretical generalization guarantees and a lower bound on the number of necessary triplets, and <strong>we empirically show that</strong> our method is both competitive with state of the art approaches and resistant to noise.” {$\longleftarrow$ <em>3. the gains/improvements of our methods</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Introduce what is ‘comparison-based learning’:</p> <blockquote> <p>“In the past few years the problem of comparison-based learning has attracted growing interest …”</p> </blockquote> </li> <li> <p>Specifically explain the proposed setting/ assumption/ question in this work:</p> <blockquote> <p>“We address the problem of classification with noisy triplets that have been obtained in a passive manner: the examples lie in an unknown metric space, …”</p> <p>“Another interesting question in this context is that of the minimal number of triplets required to successfully learn a classifier. …”</p> </blockquote> </li> <li> <p>Further introduction to our proposed method, and why they are efficient:</p> <blockquote> <p>“In this paper we propose TripletBoost …”</p> <p>“Our method is based on the idea that the triplets can be aggregated into simple triplet classifiers, …”</p> <p>“From a theoretical point of view we prove that …”</p> <p>“From an empirical point of view we demonstrate that”</p> </blockquote> </li> </ol> </li> </ul> <p>and for ‘triplet classifiers’, there is a demonstration:</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_2/pic2.png" style="zoom:75%;"></div> <h2 id="2018">2018</h2> <ul> <li> <strong>Paper</strong> <h4 id="a-degeneracy-framework-for-graph-similarity--paper">A Degeneracy Framework for Graph Similarity <a href="https://www.ijcai.org/proceedings/2018/0360.pdf" rel="external nofollow noopener" target="_blank">[paper]</a> </h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>The problem of</strong> accurately measuring the similarity between graphs is at the core of many applications in a variety of disciplines. <strong>Most existing</strong> <strong>methods</strong> for graph similarity focus either on local or on global properties of graphs. <strong>However, even if</strong> graphs seem very similar from a local or a global perspective, they may exhibit different structure at {$\longleftarrow$ <em>1. point out the problems for measuring graph similarity</em>} different scales. <strong>In this paper, we present</strong> a general framework for graph similarity which takes into account structure at multiple different scales. <strong>The proposed framework capitalizes</strong> on the well-known k-core decomposition of graphs in order to build a hierarchy of nested subgraphs. <strong>We apply</strong> <strong>the framework to derive</strong> variants of four graph kernels, namely graphlet kernel, shortest-path kernel, Weisfeiler-Lehman subtree kernel, and pyramid match graph kernel. <strong>The framework is not</strong> <strong>limited to</strong> graph kernels, but can be applied to any {$\longleftarrow$ <em>2. present the work and its main appealing properites</em>} graph comparison algorithm. The proposed framework <strong>is evaluated on</strong> several benchmark datasets for graph classification. In most cases, the corebased kernels achieve significant improvements in terms of classification accuracy over the base kernels, while their time complexity remains very attractive.” {$\longleftarrow$ <em>3. summary of the gains of the proposed method based on</em> <em>experiment</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Introducing graph and the importance of the kernel (background):</p> <blockquote> <p>“Graphs are well-studied structures which are …”</p> <p>“So far, kernel methods have emerged as one of the most effective tools for graph classification, and …”</p> </blockquote> </li> <li> <p>A general introduction of the kernel and some lacks:</p> <blockquote> <p>“Most graph kernels in the literature are instances of …”</p> <p>“Most existing graph kernels can thus be divided into two classes. …”</p> <p>“Therefore, existing graph kernels focus mainly on either local or global properties of graphs. In practice, it would be desirable to have a kernel that can take structure into account at multiple different scales”</p> </blockquote> </li> <li> <p>Summary on our work:</p> <blockquote> <p>“In this paper, we propose a framework for comparing structure in graphs at a range of different scales.”</p> <p>“More specifically, the contributions of this paper are threefold:”</p> </blockquote> </li> </ol> </li> <li> <strong>Paper</strong> <h4 id="commonsense-knowledge-aware-conversation-generation-with-graph-attention--paper"> <strong>Commonsense Knowledge Aware Conversation Generation with Graph Attention</strong> <a href="https://www.ijcai.org/proceedings/2018/0643.pdf" rel="external nofollow noopener" target="_blank">[paper]</a> </h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>Commonsense knowledge is vital to</strong> many natural language processing tasks. <strong>In this paper, we</strong> <strong>present</strong> a novel open-domain conversation generation model to demonstrate how large-scale commonsense knowledge can facilitate language understanding {$\longleftarrow$ <em>1. explain what this work did in general</em>} and generation. <strong>Given a user post</strong>, the model retrieves relevant knowledge graphs from a knowledge base and <strong>then</strong> encodes the graphs with a static graph attention mechanism, which augments the semantic information of the post and thus supports better understanding of the post. <strong>Then, during</strong> <strong>word generation</strong>, the model attentively reads the retrieved knowledge graphs and the knowledge triples within each graph to facilitate better generation through a dynamic graph attention mechanism. {$\longleftarrow$ <em>2. the more detailed working flow process of the proposed method</em>} <strong>This is the first attempt</strong> that uses large-scale commonsense knowledge in conversation generation. <strong>Furthermore, unlike existing models</strong> that use knowledge triples (entities) separately and independently, our model treats each knowledge graph as a whole, which encodes more structured, connected {$\longleftarrow$ <em>3. show the originality and advantages of this work</em>} semantic information in the graphs. <strong>Experiments</strong> <strong>show that</strong> the proposed model can generate more appropriate and informative responses than state of- the-art baselines.” {$\longleftarrow$ <em>4. summary the performance gain of the model shown by</em><br> <em>experiments</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Introducing the background:</p> <blockquote> <p>“Semantic understanding, particularly when facilitated by commonsense knowledge or world facts, is essential to many natural language processing tasks”</p> <p>“Recently, a variety of neural models has been proposed for conversation generation”</p> </blockquote> </li> <li> <p>Explain motivation (an example is given):</p> <blockquote> <p>“A model can understand conversations better and thus respond more properly if it can access and make full use of large-scale commonsense knowledge”</p> <p>“For instance, to understand a post-response pair “Don’t order drinks at the restaurant , ask for free water” and “Not in Germany. Water cost more than beer. Bring you own water bottle”, we need commonsense knowledge such as (water, At Location, restaurant), (free, Related To, cost), etc.”</p> </blockquote> </li> <li> <p>Drawbacks of previous work:</p> <blockquote> <p>“First, they are highly dependent on …”</p> <p>“Second, they usually …”</p> </blockquote> </li> <li> <p>Introduction to our methods (and a graph is provided to help better understand the concept of introducing commonsense knowledge) :</p> <blockquote> <p>“To address the two issues, we propose …”</p> <p>“We use a large-scale commonsense knowledge …”, “To fully leverage the retrieved graphs …”</p> <p>“In summary, this paper makes the following contributions: …”</p> </blockquote> </li> </ol> </li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_2/pic3.png" style="zoom:75%;"></div> <ul> <li> <strong>Paper</strong> <h4 id="from-conjunctive-queries-to-instance-queries-in-ontology-mediated-querying--paper"> <strong>From Conjunctive Queries to Instance Queries in Ontology-Mediated Querying</strong> <a href="https://arxiv.org/pdf/2010.11848.pdf" rel="external nofollow noopener" target="_blank">[paper]</a> </h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>We consider</strong> ontology-mediated queries (OMQs) based on expressive description logics of the $\mathcal{ALC}$ family and (unions) of conjunctive queries, studying the rewritability into OMQs based on instance {$\longleftarrow$ <em>1. explain generally what this work has done</em>} queries (IQs). <strong>Our results include</strong> exact characterizations of when such a rewriting is possible and tight complexity bounds for deciding rewritability. We also give a tight complexity bound for the related problemof deciding whether a given MMSNP sentence is equivalent to a CSP.” {$\longleftarrow$ <em>2. the results observed from this work</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Explains a few (which means many terms aren’t specified) concepts and previous work:</p> <blockquote> <p>“An ontology-mediated query (OMQ) is …” “An important step into this direction has been made by …”</p> </blockquote> </li> <li> <p>Motivations of this work:</p> <blockquote> <p>“There are two additional motivations to study the stated question. The first one comes from …” “The second motivation stems from …”</p> </blockquote> </li> <li> <p>A detailed introduction about what this work has done:</p> <blockquote> <p>“The main aim of this paper is to study the rewritability …” “Regarding IQ-rewritability as a decision problem, we show NP-completeness for the case of …” “We also consider …”</p> </blockquote> </li> </ol> <p><em>I really need to comment here:</em></p> <p><strong>Now I understand the suffering of reading something you truly unable to understand … (even if you searched online ** **and had a grasp about what it is, and that’s basically all you know about it).</strong> However, if given time, I still will be happy to dig deeper into this area and at least understand what this field is doing.</p> </li> <li> <strong>Paper</strong> <h4 id="reasoning-about-consensus-when-opinions-diffuse-through-majority-dynamics--paper"> <strong>Reasoning about Consensus when Opinions Diffuse through Majority Dynamics</strong> <a href="https://www.ijcai.org/proceedings/2018/0007.pdf" rel="external nofollow noopener" target="_blank">[paper]</a> </h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>Opinion diffusion is</strong> studied on social graphs where agents hold binary opinions and where social pressure leads them to conform to the opinion manifested by the majority of their neighbors. <strong>Within</strong> {$\longleftarrow$ <em>1. Explain main concepts</em>} <strong>this setting</strong>, questions related to whether a minority/ majority can spread the opinion it supports to all the other agents are considered. <strong>It is shown that</strong>, no matter of the underlying graph, there is always a group formed by a half of the agents that can annihilate the opposite opinion. <strong>Instead, the influence</strong> <strong>power of minorities depends on</strong> certain features of the given graph, which are NP-hard to be identified. <strong>Deciding whether</strong> the two opinions can coexist in some stable configuration is NP-hard, too.” {$\longleftarrow$ <em>2. introduce the background</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Explain the background by a detailed example:</p> <blockquote> <p>“Consider the following prototypical scenario. …”</p> </blockquote> </li> <li> <p>Introduce the motivation of this work and previous work:</p> <blockquote> <p>“Our goal is to analyze these questions under the lens of algorithm design and computational complexity.” “Indeed, while the study of opinion diffusion, originated in [Granovetter, 1978], has …”</p> </blockquote> </li> <li> <p>Give a detailed overview of the following sections:</p> <blockquote> <p>“In this paper we fill this gap. In more details, we first …” “Moreover, we evidence in Section 4 that …” “Finally, we address the question …”</p> </blockquote> </li> </ol> </li> <li> <strong>Paper</strong> <h4 id="r-svm-robust-learning-with-privileged-information--paper"> <strong>R-SVM+: Robust Learning with Privileged Information</strong> <a href="https://www.ijcai.org/proceedings/2018/0334.pdf" rel="external nofollow noopener" target="_blank">[paper]</a> </h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>In practice, the circumstance that training and test data are</strong> <strong>clean is not always satisfied.</strong> The performance of existing methods in the learning using privileged information (LUPI) paradigm may be seriously challenged, due to the lack of clear strategies {$\longleftarrow$ <em>1. Introducing the background</em>} to address potential noises in the data. <strong>This paper proposes</strong> a novel Robust SVM+ (RSVM+) algorithm based on a rigorous theoretical analysis. Under the SVM+ framework in the LUPI paradigm, we study the lower bound of perturbations of both example feature data and privileged feature data, which will mislead the model to make wrong decisions. By maximizing the lower bound, tolerance of the learned model over perturbations will be increased. Accordingly, a novel regularization function is introduced to upgrade a variant form of SVM+. The objective function of RSVM+ is transformed into a quadratic programming problem, which can be efficiently {$\longleftarrow$ <em>2. the detailed process of the proposed methods</em>} optimized using off-the-shelf solvers. <strong>Experiments</strong> on real world datasets demonstrate the necessity of studying robust SVM+ and the effectiveness of the proposed algorithm.” {$\longleftarrow$ <em>3. the gains proved by the experiments</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Introducing the background and concepts:</p> <blockquote> <p>“This auxiliary information can be widely found in human teaching and learning process. For example …” “Inspired by this fact, Vapnik and Vashist [Vapnik and Vashist, 2009] introduced the paradigm of learning using privileged information (LUPI) …” “Since this auxiliary information will not be available at the test stage, it is referred to as privileged information.” “As one of the most popular classifiers, support vector machine (SVM) was first upgraded …”</p> </blockquote> </li> <li> <p>Drawbacks of the current work:</p> <blockquote> <p>“These methods have largely advanced the developments on LUPI. However, their successes are usually achieved …”</p> </blockquote> </li> <li> <p>Contribution of this work:</p> <blockquote> <p>“In this paper, we derive a novel Robust SVM+ (R-SVM+) …” “In this way, the capability of the learned model to tolerate …” “Experimental results demonstrate …”</p> </blockquote> </li> </ol> </li> <li> <strong>Paper</strong> <h4 id="sentigan-generating-sentimental-texts-via-mixture-adversarial-networks---paper"> <strong>SentiGAN: Generating Sentimental Texts via Mixture Adversarial Networks</strong> <a href="https://www.ijcai.org/proceedings/2018/0618.pdf" rel="external nofollow noopener" target="_blank">[paper]</a> </h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“Generating texts of different sentiment labels <strong>is getting</strong> <strong>more and more attention</strong> in the area of natural language generation. <strong>Recently, Generative Adversarial</strong> <strong>Net (GAN) has shown</strong> promising results in text generation. <strong>However,</strong> the texts generated by GAN usually <strong>suffer from</strong> the problems of poor quality, <strong>lack of</strong> diversity and mode collapse. In this {$\longleftarrow$ <em>1. introduce background and drawbacks</em>} paper, we propose a novel framework - SentiGAN, <strong>which has</strong> multiple generators and one multi-class discriminator, <strong>to address the above problems</strong>. <strong>In</strong> <strong>our framework</strong>, multiple generators are trained simultaneously, aiming at generating texts of different sentiment labels without supervision. <strong>We propose</strong> a penalty based objective in the generators to force each of them to generate diversified examples of a specific sentiment label. <strong>Moreover, the use of</strong> multiple generators and one multi-class discriminator can make each generator focus on generating its own examples of a specific sentiment label {$\longleftarrow$ <em>2. a detailed introduction of the method and corresponding reasons</em>} accurately. <strong>Experimental results</strong> on four datasets demonstrate that our model consistently outperforms several state-of-the-art text generation methods in the sentiment accuracy and quality of generated texts.” {$\longleftarrow$ <em>3. performance gain shown by the experiments</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Introducing backgrounds:</p> <blockquote> <p>“Unsupervised text generation is an important …” “Generative Adversarial Nets (GANs) …”</p> </blockquote> </li> <li> <p>challenges may encounter:</p> <blockquote> <p>“However, there are a few challenges to be addressed …”</p> </blockquote> </li> <li> <p>detailed work-flow of the methods and a summary:</p> <blockquote> <p>“We propose a new text generation framework …” “We use a well-performed sentiment classifier as evaluator to …” “The major contributions …”</p> </blockquote> </li> </ol> </li> <li> <strong>Paper</strong> <h4 id="what-game-are-we-playing-end-to-end-learning-in-normal-and-extensive-form-games---paper"> <strong>What Game Are We Playing? End-to-end Learning in Normal and Extensive Form Games</strong> <a href="https://www.ijcai.org/proceedings/2018/0055.pdf" rel="external nofollow noopener" target="_blank">[paper]</a> </h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>Although recent work</strong> in AI has made great progress in solving large, zero-sum, extensive-form games, <strong>the underlying assumption in most past</strong> <strong>work</strong> is that the parameters of the game itself are known to the agents. <strong>This paper deals with the relatively</strong> <strong>under-explored but</strong> equally important “inverse” setting, where the parameters of the underlying game are not known to all agents, but must {$\longleftarrow$ <em>1. introduce background and drawbacks of recent work</em>} be learned through observations. <strong>We propose</strong> a differentiable, end-to-end learning framework for addressing this task. In particular, <strong>we consider</strong> a regularized version of the game, equivalent to a particular form of quantal response equilibrium, <strong>and develop</strong></p> <p>1) a primal-dual Newton method for finding such equilibrium points in both normal and extensive form games; and 2) a backpropagation method that lets us analytically compute gradients of all relevant game parameters through the solution itself. This ultimately lets us learn the game by training in an end-to-end fashion, effectively by integrating a “differentiable game solver” into the loop of larger {$\longleftarrow$ <em>2. a detailed explanation of proposed methods</em>} deep network architectures. <strong>We demonstrate</strong> the effectiveness of the learning method in several settings including poker and security game tasks.” {$\longleftarrow$ <em>3. demonstrate gains and effectiveness of the methods</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Introduce the background of the task/motivation:</p> <blockquote> <p>“Recent work in …” “However, virtually all this progress in game …” “In contrast, in many real world scenarios, …” “For example, in security games, we may want to …”</p> </blockquote> </li> <li> <p>A detailed work-flow of what we did in this paper:</p> <blockquote> <p>“In this paper, we propose …” “The crux of our approach is …” “We demonstrate the effectiveness of our approach …”</p> </blockquote> </li> </ol> </li> </ul> <h2 id="2017">2017</h2> <ul> <li> <strong>Paper</strong> <h4 id="foundations-of-declarative-data-analysis-using-limit-datalog-programs---paper"> <strong>Foundations of Declarative Data Analysis Using Limit Datalog Programs</strong> <a href="https://www.ijcai.org/proceedings/2017/0156.pdf" rel="external nofollow noopener" target="_blank">[paper]</a> </h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>Motivated</strong> by applications in declarative data analysis, <strong>we study</strong> $Datalog_\mathbb{Z}$—an extension of positive Datalog with arithmetic functions over integers. This language is known to be undecidable, so we propose two fragments. In <em>limit</em> $Datalog_\mathbb{Z}$ predicates are axiomatised to keep minimal/maximal numeric values, allowing us to show that fact entailment is coNExpTime-complete in combined, and coNP-complete in data complexity. <strong>Moreover,</strong> an additional <em>stability</em> requirement causes the complexity to drop to ExpTime and PTime, respectively. <strong>Finally,</strong> we show that stable $Datalog_\mathbb{Z}$ can express many useful data analysis tasks, and so our results provide a sound foundation for the development of advanced information systems.” {$\longleftarrow$ <em>1. the motivation and main work as well as gains are shown in sequential order</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Background introduction:</p> <blockquote> <p>“Analysing complex datasets is currently a hot topic …” “It has recently been argued that data analysis should be <em>declarative</em> …” “An essential ingredient of declarative data analysis is …” “This extensive body of work, however, focuses primarily …”</p> </blockquote> </li> <li> <p>General introduction to the work and a summary over the contributions:</p> <blockquote> <p>“To develop a sound foundation for …, we study …” “In limit $Datalog_\mathbb{Z}$, all intensional predicates with …” “We provide a direct semantics for …” “Our contributions are as follows.”</p> </blockquote> </li> </ol> </li> </ul> <h2 id="2016">2016</h2> <ul> <li> <strong>Paper</strong> <h4 id="hierarchical-finite-state-controllers-for-generalized-planning-corrected-version---paper"> <strong>Hierarchical Finite State Controllers for Generalized Planning (Corrected Version)</strong> <a href="https://arxiv.org/pdf/1911.02887.pdf" rel="external nofollow noopener" target="_blank">[paper]</a> </h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>Finite State Controllers (FSCs) are</strong> an effective way to represent sequential plans compactly. By imposing appropriate conditions on transitions, <strong>FSCs</strong> <strong>can also represent</strong> generalized plans that solve a range of planning problems from a given domain. {$\longleftarrow$ <em>1. Introducing main concepts</em>} <strong>In this paper</strong> we introduce the concept of hierarchical FSCs for planning by allowing controllers to call other controllers. <strong>We show that</strong> hierarchical FSCs can represent generalized plans more compactly than individual FSCs. <strong>Moreover,</strong> our call mechanism makes it possible to generate hierarchical FSCs in a modular fashion, or even to apply recursion. <strong>We also introduce</strong> a compilation that enables a classical planner to generate hierarchical FSCs that solve challenging generalized planning problems. The compilation takes as input a set of planning problems from a given domain and outputs a single classical planning problem, whose solution corresponds to a hierarchical FSC.” {$\longleftarrow$ <em>2. a detailed illustration of what this work has done</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Introducing backgrounds:</p> <blockquote> <p>“Finite state controllers (FSCs) are …” “Even FSCs have limitations, however. Consider …”<br> <strong>(and a graphic example was given here to help understand the drawbacks)</strong></p> </blockquote> </li> <li> <p>Introducing our work (and reasonable ideas/motivations behind it) and summarize the efforts</p> <blockquote> <p>“In this paper we introduce a novel formalism for …” “To illustrate this idea, Figure 2 shows an example hierarchical FSC …”<br> <strong>(another graphic example was given here to help understand the idea)</strong> “Intuitively, by repeatedly …” “Compared to previous work on the automatic generation of FSCs for planning the contributions of this paper are:”</p> </blockquote> </li> </ol> </li> </ul> <h2 id="2015">2015</h2> <ul> <li> <strong>Paper</strong> <h4 id="bayesian-active-learning-for-posterior-estimation---paper"> <strong>Bayesian Active Learning for Posterior Estimation</strong> <a href="https://www.cs.cmu.edu/~schneide/kandasamyIJCAI15activePostEst.pdf" rel="external nofollow noopener" target="_blank">[paper]</a> </h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>This paper studies</strong> active posterior estimation in a Bayesian setting when the likelihood is expensive to evaluate. <strong>Existing techniques</strong> for posterior estimation are based on generating samples representative of the posterior. <strong>Such methods do not</strong> consider efficiency in terms of likelihood evaluations. <strong>In order to</strong> be query efficient we treat posterior estimation {$\longleftarrow$ <em>1. background introduction and drawbacks of existing methods / motivations</em>} in an active regression framework. <strong>We propose</strong> <br> two myopic query strategies to choose where to evaluate the likelihood and implement them using {$\longleftarrow$ <em>2. main work of this paper</em>} Gaussian processes. <strong>Via experiments</strong> on a series of synthetic and real examples <strong>we demonstrate</strong> <strong>that our approach is significantly more</strong> query efficient than existing techniques and other heuristics for posterior estimation.” {$\longleftarrow$ <em>3. gains shown by experiments</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Background introduction:</p> <blockquote> <p>“Computing the posterior distribution of parameters given observations is a central problem in statistics.” “In some cases, we only have access to …”</p> </blockquote> </li> <li> <p>Goal/motivation of the paper:</p> <blockquote> <p>“Our goal is an efficient way to estimate …” “Given observations, we wish to make inferences about …”</p> </blockquote> </li> <li> <p>Contribution (and it’s actually bold-lined in the paper as a subtitle):</p> <blockquote> <p>“<strong>Our contribution</strong> is to propose …” “We propose two myopic query strategies on …” “and we demonstrate the efficacy …”</p> </blockquote> </li> </ol> </li> <li> <strong>Paper</strong> <h4 id="recursive-decomposition-for-nonconvex-optimization---paper"> <strong>Recursive Decomposition for Nonconvex Optimization</strong> <a href="https://arxiv.org/pdf/1611.02755.pdf" rel="external nofollow noopener" target="_blank">[paper]</a> </h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>Continuous optimization is</strong> an important problem in many areas of AI, including vision, robotics, probabilistic inference, and machine learning. <strong>Unfortunately,</strong> <strong>most</strong> real-world optimization problems are nonconvex, causing standard convex techniques to find only local optima, <strong>even with</strong> extensions like random restarts and simulated annealing. We observe that, <strong>in many cases</strong>, the local modes of the objective function have combinatorial structure, and thus ideas from combinatorial optimization can {$\longleftarrow$ <em>1. the background (drawbacks thus motivations)</em>} be brought to bear. <strong>Based on this</strong>, we propose a problem-decomposition approach to nonconvex optimization. Similarly to DPLL-style SAT solvers and recursive conditioning in probabilistic inference, <strong>our algorithm, RDIS, recursively</strong> sets variables so as to simplify and decompose the objective function into approximately independent subfunctions, until the remaining functions are simple enough to be optimized by standard techniques like gradient descent. <strong>The variables to set are chosen by</strong> graph partitioning, ensuring decomposition whenever {$\longleftarrow$ <em>2. A detailed introduction of the method</em>} possible. <strong>We show analytically</strong> that RDIS can solve a broad class of nonconvex optimization problems exponentially faster than gradient descent with random restarts. <strong>Experimentally,</strong> RDIS outperforms standard techniques on problems like structure from motion and protein folding.” {$\longleftarrow$ <em>3. gains shown by the experiments</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Background:</p> <blockquote> <p>“AI systems that interact with the real world often have to solve …” “However, most continuous optimization problems in …”</p> </blockquote> </li> <li> <p>Methods and challenges to do this work:</p> <blockquote> <p>“In this paper we propose that …” “We thus propose a novel nonconvex optimization algorithm, which …” “The main challenges in applying …” “For example, consider …”</p> </blockquote> </li> <li> <p>Contributions:</p> <blockquote> <p>“We first define local structure and then present our algorithm, …” “In our analysis, we show …”</p> </blockquote> </li> </ol> </li> </ul> <h2 id="summary">Summary:</h2> <p>After analyzing <strong>Abstract / Introduction</strong> of the best paper award winner from 2015 ~ 2020, considering the:</p> <p><strong>structure</strong> (how / in what order the story is told, e.g., background, motivation, etc.) <strong>relations</strong> (how the corresponding part in Abstract and Introduction interacts with each other, e.g., callbacks.) <strong>writing skills</strong> (when and where and in what form an example should be provided)</p> <p>We epitomize the common patterns shown in these papers and form a writing guidance:</p> <p>For <strong>Abstract</strong>:</p> <p>​ functionality of the <strong>Abstract</strong>:</p> <p>​ Give a brief view of your work, including:</p> \[\begin{cases} \text{$\textbf{Introduction of your task / motivation}$ (some concepts, drawbacks)}\\ \text{$\textbf{A general view of how your work is done}$ (your key methods and why you do it in that way)}\\ \text{$\textbf{What your experiments told you}$ (usually stand as a proof for the efficiency of your work)} \end{cases}\] <p>​ <strong>note that it’s better you don’t insert any pictures here, the job is to explain your work in simplest words ​ without introducing any new concepts, thus some terms can be used here, and the confusions will be ​ solved in ‘Introduction’</strong></p> <ol> <li> <p>Explain the background of your work (<strong>basic concepts</strong>, <strong>drawbacks</strong> which later implicitly serve as the motivation of your work)</p> <blockquote> <p>e.g.: xxx is … (give a brief view of what you’re doing here), however, xxx is … (explain drawbacks of the existing methods)</p> <p><strong>note that the twist (‘however’) is very important</strong></p> </blockquote> </li> <li> <p>A general introduction of the work (you can go with the sequential style or logical style etc.)</p> <blockquote> <p>For sequential style: (in this style you basically are following the architecture of your work)</p> <p>​ e.g.: we … (what you did first), then … (what you did based on your first step)</p> <p>For logical style: (in this style you basically are following the logic of how you solve problem)</p> <p>​ e.g.: we … to … (the first advancement you’re making), for …. (another advancement you’re making)</p> <p>​ e.g.: we … to … (the first advancement you’re making), however …, (new problems raised by the first advancement)</p> <p>​ thus …. (another advancement you’re making to solve the problem)</p> <p>And of course you always can <strong>write in a way you feel the story is told straight</strong></p> <p><strong>note that the reason / goal (to …) of what you’re doing is very important</strong></p> </blockquote> </li> <li> <p>Show and summarize the experimental results</p> <blockquote> <p>e.g.: we demonstrate that …(on what your experimented), experiment results show our … can … (superiority of your work)</p> </blockquote> </li> </ol> <p>For <strong>Introduction</strong>:</p> <p>​ functionality of the <strong>Introduction</strong>:</p> <p>​ A detailed ‘<strong>Abstract</strong>’, which explains in details of what you did, in some sense, the abstract is like the guidance of the ​ <strong>Introduction</strong>, where general aspects are introduced in <strong>Abstract</strong>, and a more detailed explanation of your work is done ​ in <strong>Introduction</strong></p> <ol> <li> <p>Background introduction: (the <strong>history</strong> of the subfield of this work, <strong>drawbacks</strong>, and an <strong>illustration</strong> (graphic / textual) to better understand the task / problem)</p> <blockquote> <p>e.g.: … has been an important … (general introduction of your field), … in … proposed …(origin of your field here), recently … (some advancement here)</p> </blockquote> </li> <li> <p><strong>Drawbacks</strong> of the previous work and thus our <strong>proposal</strong> (how it’s down more specifically) as well as <strong>its challenges</strong>:</p> <blockquote> <p>e.g.: …, however …, to solve the problem …, we … (here you can (not necessity) follow the introductory style used in your <strong>Abstract</strong>)</p> <p><strong>note that explaining challenges and how the proposals solve them respectively is very important</strong></p> </blockquote> </li> <li> <p>How your experiments are carried and a summary over your main contribution:</p> <blockquote> <p>e.g.: we show … that … .</p> <p>​ Our main contributions are as follows: (it’s better to write this in a new line)</p> <p><strong>note that the summarization of your work should be short and overall</strong></p> </blockquote> </li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/Treasure-hunting-1/">Treasure Hunting 1</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/paper-summary-3/">Paper Summary 3</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/paper-summary-1/">Paper Summary 1</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/paper-summary-4/">Paper Summary 4</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/Treasure-hunting-2/">Treasure Hunting 2</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Yuchen Shen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>