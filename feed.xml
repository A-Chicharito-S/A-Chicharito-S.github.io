<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://a-chicharito-s.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://a-chicharito-s.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-08T04:27:46+00:00</updated><id>https://a-chicharito-s.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://a-chicharito-s.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://a-chicharito-s.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://a-chicharito-s.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Paper Summary 4</title><link href="https://a-chicharito-s.github.io/blog/2022/paper-summary-4/" rel="alternate" type="text/html" title="Paper Summary 4"/><published>2022-02-17T16:40:16+00:00</published><updated>2022-02-17T16:40:16+00:00</updated><id>https://a-chicharito-s.github.io/blog/2022/paper-summary-4</id><content type="html" xml:base="https://a-chicharito-s.github.io/blog/2022/paper-summary-4/"><![CDATA[<head> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script> </head> <h2 id="introduction">Introduction</h2> <p>This summary specifically focuses on the low-resource scenarios for text summarization where a very limited number of data is annotated (and we may or may not have access to some other unannotated data).</p> <ul> <li><strong>Paper</strong>:</li> </ul> <h3 id="abstract-text-summarization-a-low-resource-challenge---paper">Abstract Text Summarization: A Low Resource Challenge <a href="https://aclanthology.org/D19-1616/">[paper]</a></h3> <p><a name="2"></a></p> <ul> <li><strong>Background and motivation</strong>:</li> </ul> <p>​ Datasets for multilingual text summarization are very hard to construct; thus, in this work, a data augmentation technique is proposed to conduct abstract text summarization in German.</p> <ul> <li> <p><strong>Solution</strong>:</p> <p>​ The idea of augmenting the low-resource annotated data is originated from the “back-translation” idea in NMT. In detail, when doing NMT, if we have a group of labeled data from the source language (e.g., <strong>En</strong>) to the target language (e.g., <strong>Fr</strong>): $Annotated=\{(En\longrightarrow Fr)_i\}_{i=1}^N$ and a group of unannotated data in the target language: $Unannotated=\{Fr_j\}_{j=1}^M$. And usually, the model will be quite insufficiently trained if we only apply the $Annotated$ dataset. Thus, we need to convert the unannotated dataset (in target language) into annotated version. A good method is to apply “<strong>back-translation</strong>”, where we first train a model from target (<strong>Fr</strong>) to source (<strong>En</strong>): $Fr\stackrel{bt}{\longrightarrow}En$ using the $Annotated$ dataset. And then we feed the $Unannotated$ dataset into our trained “$bt$” model to generate some synthetic data $En_{syn}$ to finally get our augmented dataset:</p> \[Augmented=\{(En\longrightarrow Fr)_k\}_{k=1}^{N+M}=\{(En\longrightarrow Fr)_i\}_{i=1}^N+\{(En_{syn}\longrightarrow Fr)_j\}_{j=1}^M\] <p>Then the $Augmented$ dataset is used to train an NMT model from English to French.</p> <p>Like NMT, in abstract summarization, with a small annotated dataset from <strong>text</strong> to <strong>summary</strong> and a large group of unannotated sentences (viewed as <strong>summaries</strong>), this paper proposes to first map the unannotated summaries to synthetic texts, and later use the augmented dataset (origin+synthetic) to train an abstract summarization model.</p> </li> </ul> <p>​ <strong>Highlights</strong> ​ I think the highlight is the proposal of using back-translation to generate synthetic data for a low-resource setting in abstractive summarization. However, using back-translation has as well been under doubt and criticized (see <a href="#1">[here]</a>).</p> <ul> <li><strong>Architecture</strong>:</li> </ul> <p>The back-translation architecture:</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_4/pic1.png" style="zoom:75%;"/></div> <p>The “double” back-translation architecture (performs worse than above):</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_4/pic2.png"/></div> <ul> <li><strong>Paper</strong>:</li> </ul> <h3 id="long-document-summarization-in-a-low-resource-setting-using-pretrained-language-models---paper">Long Document Summarization in a Low Resource Setting using Pretrained Language Models <a href="https://arxiv.org/abs/2103.00751">[paper]</a></h3> <p><a name="8"></a></p> <ul> <li><strong>Background and motivation</strong>:</li> </ul> <p>​ Abstractive summarization task with long documents in a low-resource scenario is under-explored. This work uses BART ( which is a powerful abstractive summarization model pre-trained on relatively short documents) and aggregates it with long document abstractive summarization.</p> <ul> <li> <p><strong>Solution</strong>:</p> <p>​ This paper proposes to <strong>compress</strong> the long documents using a transformer-based classifier (“<strong>Extractor</strong>”) where sentences labeled as salient will be the compressed version of original long documents. To prepare the training data for the classifier, GPT-2 model is used to ground the summaries $T$ (target) to the long documents $S$ (source), and a score $\frac{1}{n}\sum_{j=0}^nf(s_i,\,t_j)$ measuring how much one summary is grounded in each sentence of its source document. After scoring, <strong>an aggregated sampling method</strong> is proposed: some sentences with top-3n highest (<strong>salient</strong>) and top-3n lowest (<strong>non-salient</strong>) scores (n is related to the number of sentences in the summary $T$ and is a tuned hyperparameter) are selected as the positive and negative samples of the original long documents; then, these sentences will serve as our training data for the “<strong>Extractor</strong>”.</p> <p>After using the “<strong>Extractor</strong>” to <strong>compress</strong> the original long documents, the compressed documents are fed to BART and then fine-tined. (Note that results for combinations of different sampling methods and $f$ are in the appendix, generally speaking, aggregated sampling + GPT-2 scoring is the best option)</p> </li> </ul> <p>​ <strong>Highlights</strong> ​ One very surprising idea in this paper is that <strong>it does not use extra data</strong>, which really is my ideal “low-resource” setting. The following strategy is very straightforward: <strong>a</strong>. train a long document <strong>compressor</strong> using pre-trained models to prepare labeled data; <strong>b</strong>. then leverage BART to summarize the compressed documents. The whole process can be viewed as “<strong>purification</strong>”.</p> <ul> <li><strong>Architecture</strong>:</li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_4/pic3.png"/></div> <ul> <li><strong>Paper</strong>:</li> </ul> <h3 id="extraphrase-efficient-data-augmentation-for-abstractive-summarization---paper">ExtraPhrase: Efficient Data Augmentation for Abstractive Summarization <a href="https://arxiv.org/abs/2201.05313">[paper]</a></h3> <p><a name="7"></a></p> <ul> <li><strong>Background and motivation</strong>:</li> </ul> <p>​ Previous abstractive summarization task in the low-resource setting usually needs a neural network to obtain synthetic data, this work proposes a new method that does not need pre-preparation.</p> <ul> <li> <p><strong>Solution</strong>:</p> <p>​ Unlike back-translation models (such as <a href="#2">[this]</a>) which need to first train a summary-to-text model in order to generate synthetic data, this work proposes to directly prune the parsing tree of a summary to generate its extracted version and uses <strong>round trip translation</strong> to generate paraphrases for the extracted summary. For <strong>round trip translation</strong>, this paper leveraged SOTA pre-trained NMT models, and the extracted summary (e.g., <strong>En</strong>) will be first translated into another language (e.g., <strong>De</strong>) and then translated back (e.g., <strong>De</strong> $\rightarrow$ <strong>En</strong>) to get the paraphrased version of the summary (the final pseudo summary). Later the generated summary and original data are used for training an abstractive summarization model.</p> <p>Interesting experiments have been done to show the effect of the model. <strong>One</strong> is the normal setting, where genuine datasets of size 380M/287K are used to generate pseudo data. And the model achieved a very small gain on the ROUGE-1, 2, L compared with methods like back-translation, self-training, and oversampling by simply double the size of the dataset<a name="3"></a>; <strong>The other</strong> is the low-resource setting, where the genuine dataset is only of size 1K and the rest data (3.8M) is used to generate pseudo data. Again, the <strong>ExtraPhrase</strong> has beaten all the comparing methods and achieved a great margin (e.g., <strong>23.58 v.s. 12.19</strong>)</p> </li> </ul> <p>​ <strong>Highlights</strong> <a name="1"></a> ​ The value of this work to me personally is that it has revealed the drawbacks of <strong>back-translation</strong> as a data augmentation technique in abstractive summarization to me. The reason why back-translation is successful in NMT is that the meaning and amount of information stored in the source and target are approximately the same, however, for abstract summarization, it might be irrational to synthesize documents with richer, redundant information from a concrete, relatively smaller summary, since the back-translation has no guidance on how to recover information unseen in the documents. And as well, this work also shows us a good example of using light-/non- neural network methods to achieve comparable (under normal setting) even remarkable (under low-resource setting) results.</p> <ul> <li><strong>Architecture</strong>:</li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_4/pic4.png" style="zoom:85%;"/></div> <ul> <li><strong>Paper</strong>:</li> </ul> <h3 id="mitigating-data-scarceness-through-data-synthesis-augmentation-and-curriculum-for-abstractive-summarization---paper">Mitigating Data Scarceness through Data Synthesis, Augmentation and Curriculum for Abstractive Summarization <a href="https://arxiv.org/abs/2109.08569">[paper]</a></h3> <p><a name="6"></a></p> <ul> <li><strong>Background and motivation</strong>:</li> </ul> <p>​ This work proposes three data augmentation techniques for low-resource abstractive summarization tasks without using any additional data.</p> <ul> <li> <p><strong>Solution</strong>:</p> <p>​ To achieve the augmentation without using any extra data, this paper proposes three possible solutions: <strong>a</strong>. synthesis via paraphrasing with GPT-2; <strong>b</strong>. augmentation with sample mixing; <strong>c</strong>. curriculum learning. For <strong>paraphrasing</strong>, the original summaries are paraphrased using the GPT-2 model. And for <strong>sample mixing</strong>, the original idea is to feed two sentences $x, \,x^{‘}$ into the same architecture and mix their representations with a proportion of $\lambda:1-\lambda$ at some layer and then the mixed representation will produce a probability distribution which indicates the “mixed” distribution of words in $x, \, x^{‘}$, which can be illustrated as:</p> </li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_4/pic5.png" style="zoom:75%;"/></div> <p>where the KL divergence between the predicted distribution $\hat{y}$ and the “mixed” real distribution will serve as the loss function. In the paper, the authors propose MixGen, which has a decoder where for <strong>training</strong>, the ground truth distribution at each time-step $t\in[1,\,min(|x|,\,|x^{‘}|\,)]$ will be:</p> <div> ​ $$ WordDistribution(t)=\left\{ ​ \begin{array}{rcl} ​ P(word_i)=\lambda &amp;&amp; {if\,\,x[t]=word_i}\\ ​ P(word_i)=1-\lambda &amp;&amp; {if\,\,x^{'}[t]=word_i}\\ ​ P(word_i)=0 &amp;&amp; {else} ​ \end{array} \right.\qquad for\,\,i\in[1,\,v]$$ </div> <p>​ And for time-steps that exceed the value of the minimum length of $x,\,x^{‘}$, the word distribution will be that of the remaining of the longer sentence. When decoding auto-regressively in <strong>training</strong>, instead of using $argmax()$ to decide which token to predict (in this case it will always be the tokens with a higher weight of $\lambda,\,1-\lambda$), the ground truth token at time-step $t\in[1,\,min(|x|,\,|x^{‘}|\,)]$ is chosen based on a probability $P_t\sim U(0,\,1)$. The illustration for MixGen is:</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_4/pic6.png"/></div> <p>For <strong>curriculum learning</strong>, data are fed to the model with difficulty from low to high, where the difficulty is measured by a pre-defined criterion (in this paper, there are two criteria, namely <strong>specificity</strong> (measured by a classifier) and <strong>ROUGE</strong>)</p> <p>​ <strong>Highlights</strong> ​ Unfortunately, though the authors have proposed some useful techniques and discussed them under a non-extra data setting, the experiments are quite disappointing:</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_4/pic7.png" style="zoom:75%;"/></div> <p>As shown above, the “shuff.” according to the authors is a synthetic baseline constructed by generating 10 samples for each of the original training samples by randomly shuffling the texts, and “+mask” means randomly mask 50% of the texts 50% of the time. As you can see, the training data of “shuff.” and “shuff.+mask” though redundant (whose power is reported <a href="#3">[here]</a>), is much bigger than the original data, which leads to unfair comparison. Also, the pre-training synthetic data experiment did not include the <strong>Mix(n=3)</strong> method.</p> <ul> <li> <p><strong>Architecture</strong>:</p> <p>No specific overall architecture</p> </li> <li> <p><strong>Paper</strong>:</p> </li> </ul> <h3 id="few-shot-learning-of-an-interleaved-text-summarization-model-by-pretraining-with-synthetic-data---paper">Few-Shot Learning of an Interleaved Text Summarization Model by Pretraining with Synthetic Data <a href="https://arxiv.org/abs/2103.05131">[paper]</a></h3> <p><a name="4"></a></p> <ul> <li><strong>Background and motivation</strong>:</li> </ul> <p>​ Interleaved texts are very common in online chatting where posts belonging to different threads occur in a sequence. Existing methods first disentangle the interleave texts by threads and then perform abstractive summarization, which may propagate errors if the disentanglement is wrong in the first place. And this work proposes to train an end-to-end summarization system with synthetic data interleaved from a regular document-summary corpus.</p> <ul> <li> <p><strong>Solution</strong>:</p> <p>​ To deal with the disentanglement implicitly in an end-to-end system, this paper proposes to use hierarchical attention mechanisms to attend to <strong>word-level</strong>, <strong>post-level</strong> (or in other words, sentence-level), and <strong>thread-level</strong> features. The overall architecture is shown below.</p> </li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_4/pic8.png"/></div> <p>When <strong>encoding</strong>, for the $i$-th post $P_i$, a word-level BiLSTM encoder $E_{w2w}$ first encodes the word embeddings of $P_i$ to get a sequence of hidden representations $(\textbf{h}_{i,\,0}^{E_{w2w}},\,…,\,\textbf{h}_{i,\,p}^{E_{w2w}})$, and the average: $\frac{1}{p}\sum_{j=0}^p\textbf{h}_{i,\,j}^{E_{w2w}}$ is fed into the BiLSTM post encoder $E^{p2p}$ at time-step $i$.</p> <p>When <strong>decoding</strong>, at the $k$-th step of the thread decoder $D^{t2t}$, post-level attention is computed $\gamma_{i}^k=Attention^\gamma(\textbf{h}_{k-1}^{D_{t2t}},\,P_i)\quad i\in[1,\,n]$ where $P_i$ is simply the representation $\textbf{h}_i^{E_{p2p}}$ for $i$-th post from the post-level encoder. The authors also argue that the phrases (short sequences of words in a post) in the documents are very important for abstractive summarization; thus phrase-level attention $\beta^{k}=(\beta_{0,\,0}^{k},\,…,\,\beta_{n,\,p}^{k})$ is computed as well, where $\beta_{i,\,j}^{k}=Attention^\beta(\textbf{h}_{k-1}^{D_{t2t}},\,\textbf{a}_{i,\,j})$ and $\textbf{a}_{i,\,j}=element-wise\,\,add(\textbf{W}_{i,\,j},\,\textbf{P}_i)\quad i\in[1,\,n],\,\,j\in[1,\,p]$ and $\beta_{i,\,j}^{k}$ is rescaled by the equation: $\hat{\beta}_{i,\,j}^{k}=\beta_{i,\,j}^{k}*\gamma_i^k$. A weighted representation of the word representations $\textbf{W}_{i,\,j}$ is given by: $\sum_{i=1}^n\sum_{j=1}^p\hat{\beta}_{i,\,j}^{k}\textbf{W}_{i,\,j}$, which together with the last hidden state from the previous word-level decoder $\textbf{h}_{k-1,\,q}^{D_{w2w}}$ is fed into the thread-level decoder $D_{t2t}$ (shown in blue circle). The current state of the thread-level decoder $\textbf{h}_k^{D_{t2t}}$ is fed to a feed-forward network $g$ to obtain a distribution over $STOP=1$ token and $CONTINUE=0$ token, where $p_k^{STOP}=\sigma(g(\textbf{h}_k^{D_{t2t}}))$ and the decoder stops decoding if $p_k^{STOP}\gt 0.5$. Additionally, the extra two inputs $\sum_{i=1}^n\sum_{j=1}^p\hat{\beta}_{i,\,j}^{k}\textbf{W}_{i,\,j}$ and $\textbf{h}_{k-1,\,q}^{D_{w2w}}$ together with the new state $\textbf{h}_k^{D_{t2t}}$ are fed to a two-layer feed-forward network $r$ with a dropout layer to compute the initial state $\textbf{s}_k$ for the unidirectional word decoder $D_{w2w}$.</p> <p>At the $l$-th step of the word-level decoder inside the $k$-th step of the thread-level decoder, word-level attention $\alpha_{i,\,·}^{k,\,l}$ is computed and then rescaled as: $\hat{\alpha}_{i,\,j}^{k,\,l}=norm(\hat{\beta}_{i,\,j}^k\times \alpha_{i,\,j}^{k,\,l})$ where $norm(·)$ stands for softmax operation.</p> <p>The construction of synthetic interleaved data is quite simple, which is basically randomly sample abstract-title pairs to consecutively construct threads and posts and corresponding summaries.</p> <p>​ <strong>Highlights</strong> ​ This work gives a specific way to construct interleaved data. However, I think the shining point lies in the proposed architecture of hierarchical attention, although it is quite complex, the experimental results show that the hierarchical attention can really disentangle the threads implicitly and performs better than models that disentangle threads explicitly.</p> <ul> <li> <p><strong>Architecture</strong>:</p> <p>See above</p> </li> <li> <p><strong>Paper</strong>:</p> </li> </ul> <h3 id="adaptsum-towards-low-resource-domain-adaptation-for-abstractive-summarization---paper">AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive Summarization <a href="https://arxiv.org/abs/2103.05131">[paper]</a></h3> <p><a name="5"></a></p> <ul> <li><strong>Background and motivation</strong>:</li> </ul> <p>​ Domain adaption for low-resource abstractive summarization is under-explored and this work hopes to set some baselines and references for future work.</p> <ul> <li> <p><strong>Solution</strong>:</p> <p>​ This work proposes three different tasks to investigate the effect of domain adaption in low-resource abstractive summarization on BART using <strong>second pre-trainin</strong>g: <strong>a</strong>. Source Domain Pre-Training (SDPT), where the model is second pre-trained on a source domain (general, e.g., News); <strong>b</strong>. Domain-Adaptive Pre-Training (DAPT), where the BART is second pre-trained on an unlabeled target-domain-related dataset with the original BART re-construction objective; <strong>c</strong>. Task-Adaptive Pre-Training (TAPT), where the BART is second pre-trained on an unlabeled target-domain dataset with the summarization task.</p> <p>For <strong>second pre-training</strong>, the model may face the problem of catastrophic forgetting and the authors tackle it using the RecAdam optimizer, which has the following training objective:</p> <div> $$Loss=\lambda(t)·Loss_T+(1-\lambda(t))·Loss_S$$ $$where\,\,\lambda(t)=\frac{1}{1+exp(-k·(t-t_0))},\,\,Loss_S=\frac{1}{2}\gamma\sum_i(\theta_i-\theta_i^*)^2$$ </div> <p>where $Loss_T$ is the target task objective function and $\theta_i$ is the parameter of the model and $\theta_i^*$ is the fixed original parameter of the pre-trained model.</p> </li> </ul> <p>​ <strong>Highlights</strong> ​ I think the second pre-training and the proposal of three domain adaption tasks are very interesting, especially the tasks of SDPT and TAPT show their effectiveness in boosting the model’s performance in the low-resource setting (meanwhile, DAPT is not always very effective). And this work provides a new perspective about how we can boost the performance of abstractive summarization models through domain adaptation.</p> <ul> <li> <p><strong>Architecture</strong>:</p> <p>No specific overall architecture</p> </li> </ul> <h2 id="closing-words">Closing words</h2> <p>In this summary, we looked through six papers on the topic of performing abstractive summarization under a low-resource scenario, where different tasks and settings are discussed. We can see that these work can be roughly divided into two kinds: <strong>one</strong> is given a small number of labeled data, extra unlabeled data can be obtained and is used to generate synthetic data; <strong>two</strong>. only the given small number of labeled data can be used for training.</p> <p>For <strong>the first setting</strong>, the key is to <em>generate synthetic data</em> / <em>leverage extra data</em>, and methods like <strong>back-translation</strong> (<a href="#2">[here]</a>), <strong>self-designed algorithm</strong> (<a href="#4">[here]</a>), <strong>domain adaption</strong> (<a href="#5">[here]</a>) are used. For <strong>the second setting</strong>, the key is to <strong>ONLY</strong> use the original data to enlarge the training data; thus, methods like <strong>paraphrasing</strong> (<a href="#6">[here]</a> and <a href="#7">[here]</a>) and <strong>compressing / pruning</strong> (<a href="#7">[here]</a> and <a href="#8">[here]</a>) are applied directly on the original data.</p> <p>However, the above methods all generate new training data (from extra available data/original data) in a <strong>sequence-to-sequence manner within the same source</strong> (e.g., digits to digits), which means the augmented data is a transformation of the original data (e.g., $(1, 3,…,5,2)\in\mathbf{R}^n\longrightarrow (1,34,…,4,2)\in\mathbf{R}^n$) instead of a <strong>sequence-to-hidden-state manner</strong> (e.g.,$(1, 3,…,5,2)\in\mathbf{R}^n\longrightarrow (\vec{a_1},\vec{a_2},…,\vec{a_{n-1}},\vec{a_n})\in\mathbf{R}^{n\times m}\stackrel{aggregate}{\longrightarrow}\vec{A}\in\mathbf{R}^m$).</p> <p>I think the method of directly generating its higher representation (hidden-state) may solve some challenges such as <strong>1</strong>. the synthesized sequence may be grammatically wrong, <strong>2</strong>. it may be irrational to generate synthetic documents with larger information from summaries using back-translation, etc.</p> <p>However, this proposal may face the following problems: <strong>1</strong>. the hidden-state may be hard to obtain and it varies for different models, <strong>2</strong>. there may need a delicately designed architecture to map the digits to a higher representation (which may need to first prune the long, redundant documents first then aggregate their hidden representations to a concrete one, such as the initial state of the decoder).</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[paper summary]]></summary></entry><entry><title type="html">Transformer Implementation</title><link href="https://a-chicharito-s.github.io/blog/2022/transformer-implementation/" rel="alternate" type="text/html" title="Transformer Implementation"/><published>2022-02-10T16:40:16+00:00</published><updated>2022-02-10T16:40:16+00:00</updated><id>https://a-chicharito-s.github.io/blog/2022/transformer-implementation</id><content type="html" xml:base="https://a-chicharito-s.github.io/blog/2022/transformer-implementation/"><![CDATA[<head> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script> </head> <p>This is the blog about the implementation of Transformer from <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>. The full code (model + training loop) is <a href="https://github.com/A-Chicharito-S/PyTorch-Projects-for-Learning/tree/main/Transformer">[here]</a>, which takes inspirations from <a href="#0">[Reference]</a>.</p> <h2 id="introduction">Introduction</h2> <p>“<strong>Attention Is All You Need</strong>” (see <a href="https://arxiv.org/abs/1706.03762">[here]</a>) is the original paper of the Transformer model, which is a powerful structure in the field of NLP and has been the foundation of many following works of different sub-fields of NLP. In this post, I will introduce the implementation of the Transformer in the following three steps: <strong>1</strong>. the overall architecture of the Transformer; <strong>2</strong>. the reusable components of the Transformer; <strong>3</strong>. The final model constructed by the components from <strong>2</strong>.</p> <h2 id="overall-architecture">Overall Architecture</h2> <p>The following picture is from <strong>Attention Is All You Need</strong> and illustrates the overall architecture of the Transformer.</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/pytorch_learning_1/pic1.png" style="zoom:80%;"/></div> <p><a name="4"></a> We can see that the Transformer is mainly composed of two parts: the Encoder and the Decoder. Both of which are then staked by <strong>N</strong> encoder/decoder blocks. And for each block, it can be decoupled into the following components: <strong>a</strong>. Multi-Head Attention; <strong>b</strong>. Add&amp;Norm; <strong>c</strong>. Feed Forward. And there is an embedding module that embeds the discrete digits into neural-network-friendly dense vectors.</p> <p>For an NLP task, given the source sequence $X=(BOS,\,x_1,\,…,x_n,\,EOS)$ and its target $Y=(BOS,\,y_1,\,…,y_m,\,EOS)$, the Transformer feeds $x$ as ‘<strong>inputs</strong>’ and embeds it into high-dimension dense vectors, which later are further encoded by the Encoder. When <strong>training</strong>, Target $y$ is shifted right (the last token $EOS$ is removed) and embedded to be fed into the Decoder as <strong>Outputs</strong>, where an interaction with the final outputs from the Encoder is performed in the Multi-Head Attention module. The final outputs from the Decoder are later projected to the target vocabulary size to produce the prediction $\hat{Y}=(\hat{y}_1,\,…,\hat{y}_m,\,EOS)$ When <strong>testing</strong>, at first there will only be $BOS$ fed into the Decoder to produce the first predicted token $\hat{y}_1$, and later $(BOS,\,\hat{y}_1)$ is fed and so on. (This process is called <strong>auto-regressive</strong> since the output from the last state is used as input for the current state.)</p> <h2 id="components">Components</h2> <p>As identified above, we have the following components that make up our Transformer model: <strong>a</strong>. Embedding; <strong>b</strong>. Multi-Head Attention; <strong>c</strong>. Add&amp;Norm; <strong>d</strong>. Feed Forward.</p> <h3 id="a-embedding">a. Embedding</h3> <p>The Embedding has two parts: first is the embedding layer that maps discrete digits into dense vectors, and second is the positional encoding layer that gives the tokens $EOS,\,x_1,\,x_2,\,…$ in $X=(BOS,\,x_1,\,…,x_n,\,EOS)$ a sequential feature. (since unlike RNN taking one input token as a time, the Transformer takes inputs at one time)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>


<span class="k">class</span> <span class="nc">TransformerEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_vocab</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">PAD</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">is_frozen</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerEmbedding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="o">=</span><span class="n">num_vocab</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">PAD</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">embedding_matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">embedding_matrix</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="n">num_vocab</span> <span class="ow">or</span> <span class="n">embedding_matrix</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">!=</span> <span class="n">dim</span>
            <span class="n">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">embeddings</span><span class="o">=</span><span class="n">embedding_matrix</span><span class="p">,</span> <span class="n">freeze</span><span class="o">=</span><span class="n">is_frozen</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">PE</span><span class="sh">'</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">max_seq_len</span><span class="o">=</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
</code></pre></div></div> <p>The above code is the “<strong>__init__</strong>” method of our Embedding. To be specific, it uses the <strong>nn.Embedding</strong> module to map discrete digits (0, 1, 2, …) into dense vectors of size “<strong>dim</strong>”. By passing “<strong>embedding_matrix</strong>” and “<strong>is_frozen</strong>” parameters, we can initialize the embedding table and decide whether it is trainable. We also can use the “<strong>self.register_buffer</strong>” method (inherited from “<strong>nn.Module</strong>”, and can be quickly accessed as a constant) to store the <strong>positional encoding</strong>, which is computed by:</p> <div> $$ \begin{split} PE_{(pos,\,2i)} &amp;= sin(pos\,/\,10000^{2i\,/\,d_{model}}) \\ PE_{(pos,\,2i+1)} &amp;= cos(pos\,/\,10000^{2i\,/\,d_{model}}) \end{split} $$ </div> <p>The above formula tells us for a token at position $pos$ in a sequence, the <strong>positional encoding</strong> of its even dimension is a $sin$ function and that of odd dimension is a $cos$ function. The wavelength for dimension $2i$ and $2i+1$ is $2\pi·10000^{2i\,/d_{model}}\in[2\pi,\,10000·2\pi]$ and the max length that <strong>positional encoding</strong> can present is roughly viewed as $10000$ (since the max period is $10000·2\pi$). One good thing about using trigonometric functions is: $sin(pos+k)=sin(pos)cos(k)+cos(pos)sin(k)$ and $cos(pos+k)=cos(pos)cos(k)-sin(pos)sin(k)$; thus, the relationship between the $pos$-th position and $pos+k$-th position in the sentence can be view as a linear transformation $PE_{pos}\longrightarrow PE_{pos+k}$ where the coefficients are constants about $k$, namely, $sin(w_i·k)$ and $cos(w_i·k)$ (see discussion <a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/#the-intuition">[here]</a>).</p> <p>The implementation is:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">PositionalEncoding</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># returns: value: [0, 1, ..., max_seq_len-1] ; shape: (max_seq_len, 1)
</span>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">dim</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">))</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="c1"># i:j:k means from position i to j, take indices every k steps.
</span>        <span class="k">return</span> <span class="n">pe</span>
</code></pre></div></div> <p>where noticing the <strong>div_term</strong> is actually:</p> \[\frac{1}{10000^{2i/d_{model}}}=e^{log(\frac{1}{10000^{2i/d_{model}}})}=e^{-\frac{2i}{d_{model}}log10000}\] <p>And we can defined the computation process as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># inputs shape: (batch_size, seq_len)
</span>        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">dim</span><span class="p">)</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">PE</span><span class="p">[:</span><span class="n">seq_len</span><span class="p">]))</span>
</code></pre></div></div> <p>Note that after “<strong>self.embedding(inputs)</strong>” it is multiplied by $\sqrt{d_{model}}$ (which is the case in <a href="#2">[2]</a> and <a href="#3">[3]</a>) and a <strong>layer normalization</strong> is performed before output. (which is as well equivalent to the implementations from <a href="#2">[2]</a> and <a href="#3">[3]</a>). In <a href="#1">[1]</a>, the returning is simply: <strong>self.dropout(self.embedding(inputs) + self.PE[:seq_len])</strong></p> <h3 id="b-multi-head-attention">b. Multi-Head Attention</h3> <p>The Attention layer follows the following computation operation: $Attention(Q,\,K,\,V)=softmax(\frac{Q·K^{T}}{\sqrt{d_k}})·V$, and the term “<strong>Multi-Head</strong>” can be roughly treated as performing this operation multiple times for the same set of $Q,\,K,\,V$. To be specific, the <strong>Multi-Head Attention</strong> is computed by:</p> <div> $$ \begin{split} MultiHead(Q,\,K,\,V) &amp;= Concat(head_1,\,...,\,head_h)W^O \\ where\,\,head_i &amp;= Attention(QW^Q_i,\,KW^K_i,\,VW^V_i) \end{split} $$ </div> <p>The implementation is:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>


<span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Attention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>

        <span class="n">self</span><span class="p">.</span><span class="n">sub_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_K</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_Q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_V</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout_rate</span> <span class="o">=</span> <span class="n">dropout_rate</span>
        <span class="k">if</span> <span class="n">dropout_rate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>

</code></pre></div></div> <p>The “<strong>__init__</strong>” method of our Multi-Head Attention layer is quite simple. By passing the number of attention heads, we are actually operating on the hidden dimension (split it into <strong>num_heads</strong> equal parts) to do the attention. Note for <strong>Multi-Head Attention</strong> the module “<strong>nn.Dropout</strong>” is not in <a href="#1">[1]</a> and can be turned off in <a href="#2">[2]</a> and <a href="#3">[3]</a>.</p> <p>The forward pass of out attention layer will be:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># inputs is of shape (batch_size, seq_len, dim)
</span>        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_Q</span><span class="p">(</span><span class="n">Q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">Q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">sub_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># shape: (batch_size, seq_len, num_heads, sub_dim) ---&gt; (batch_size, num_heads, seq_len_q, sub_dim)
</span>        <span class="n">K</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_K</span><span class="p">(</span><span class="n">K</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">sub_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># shape: (batch_size, num_heads, seq_len_kv, sub_dim)
</span>        <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_V</span><span class="p">(</span><span class="n">V</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">V</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">sub_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># shape: (batch_size, num_heads, seq_len_kv, sub_dim)
</span>        <span class="n">att_score</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sub_dim</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">att_score</span> <span class="o">=</span> <span class="n">att_score</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
        <span class="n">att_score</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">att_score</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># (batch_size, num_heads, seq_len_q, seq_len_kv)
</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">att_score</span> <span class="o">=</span> <span class="n">att_score</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">dropout_rate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">att_score</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">att_score</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">att_score</span><span class="p">,</span> <span class="n">V</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">sub_dim</span><span class="p">)</span>
        <span class="c1"># shape: (batch_size, num_heads, seq_len_q, sub_dim) ---&gt; (batch_size, seq_len_q, num_heads, sub_dim)
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_out</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="c1"># shape: (batch_size, seq_len_q, dim)
</span></code></pre></div></div> <p>We can see that, for a given input, say <strong>Q</strong>, a following pre-processing is performed:</p> \[(batch\_size,\,seq\_len\_q,\,dim\stackrel{W^{Q}}{\longrightarrow}dim)\stackrel{reshape}{\longrightarrow}(batch\_size,\,num\_heads,\,seq\_len\_q,\,sub\_dim)\] <p>And the “<strong>Multi-Head</strong>” operation is actually performed by one matrix (in our example: $W^Q$). The following codes are quite easy to understand. Note that <a href="#1">[1]</a>, <a href="#2">[2]</a>, <a href="#3">[3]</a> all implement the $Attention(Q,\,K,\,V)=softmax(\frac{Q·K^{T}}{\sqrt{d_k}})·V$ as another callable function (such as “<strong>def attention(query, key, value, mask=None, dropout=None):</strong>”) and masking the padded positions of attention score “<strong>att_score = att_score.masked_fill(mask, 0)</strong>” is not in <a href="#2">[2]</a> and <a href="#3">[3]</a>. And the usage of “<strong>.reshape()</strong>”, “<strong>.view()</strong>”, and “<strong>.contiguous()</strong>” is specified <a href="https://blog.csdn.net/Flag_ing/article/details/109129752">[here]</a></p> <p>A very important thing is the shape of the mask. Based on the function of the masks, we can simply divide them into two kinds: padding mask (used in <strong>Multi-Head Attention</strong>, both in the Encoder and the Decoder, see <a href="#4">[overall architecture]</a>) and subsequent mask (used in <strong>Masked Multi-Head Attention</strong>, only in the Decoder, see <a href="#4">[overall architecture]</a>). A <strong>padded mask</strong> <a name="5"></a> is to mask out the paddings in the sentence, for example: for an input: $[3,\,5,\,8,\,9,\,4,\,0,\,0,\,0]$, the three zeros at the end are just place holders thus should not be considered real words. A <strong>subsequent mask</strong> <a name="6"></a>, however is only used in the decoder, and is of shape: $seq\_len\_outputs\times seq\_len\_outputs$ where $seq\_len\_outputs$ is the sentence length of the “<strong>Outputs</strong>” fed into the Decoder (see <a href="#4">[overall architecture]</a>). A general sense of the <strong>subsequent mask</strong> is this ($seq\_len\_outputs=3$):</p> <div> $$ \begin{bmatrix} 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 0 \end{bmatrix} $$ </div> <p>where the diagonal and below are all set to 0 (or 1) and above the diagonal are all set to 1 (or 0).<a name="7"></a></p> <h3 id="c-addnorm">c. Add&amp;Norm</h3> <p>This layer simply leverages the residual connection from <a href="https://arxiv.org/abs/1512.03385">[paper]</a> to add a direct path between two sub-layers to avoid gradient vanishing, which can be implemented by: $LayerNorm(x+Sublayer(x))$ where $x$ is the input of the sub-layer and $Sublayer(x)$ is its output.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="c1"># This is the example from the encoder block
</span>    	<span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">att</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">pad_mask</span><span class="p">))</span>
        <span class="c1"># att: Q=inputs, K=inputs, V=inputs
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">feedforward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
    
    
        <span class="c1"># This is the example from the decoder block
</span>        <span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">masked_att</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">upper_triangle_mask</span><span class="p">))</span>
        <span class="c1"># masked_att: Q=inputs, K=inputs, V=inputs, mask=upper_triangle_mask
</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">att</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">pad_mask</span><span class="p">))</span>
        <span class="c1"># att: Q=inputs, K=encoder_outputs, V=encoder_outputs, mask=pad_mask
</span>        
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">feedforward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
</code></pre></div></div> <h3 id="d-feed-forward">d. Feed Forward</h3> <p>The full name of this layer is called <strong>Position-wise Feed-Forward Networks</strong> in the origin paper, since it’s applied to each position separately. And it is computed by:</p> \[FFN(x)=max(0,\,xW_1+b_1)W_2+b_2\] <p>The implementation is very simple:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>


<span class="c1"># implements: max(0, x·W_1 + b_1)·W_2 + b_2
</span><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">inner_layer_dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">):</span>  <span class="c1"># in paper: dim=512, inner_layer_dim=2048
</span>        <span class="nf">super</span><span class="p">(</span><span class="n">FeedForward</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">inner_layer_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">inner_layer_dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nc">W_2</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">W_1</span><span class="p">(</span><span class="n">inputs</span><span class="p">))))</span>
</code></pre></div></div> <h2 id="final-model">Final Model</h2> <p>With the aforementioned four basic components, we can now build our Transformer model! First, we need to construct the encoder/decoder block for the Encoder/Decoder, and then aggregate them together to get our final model. At last, we will also look a little deeper into the decoding strategies when <strong>testing</strong>.</p> <p>A encoder block is:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>


<span class="k">class</span> <span class="nc">TransformerBaseEncoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1"># the input is of size: (batch_size, seq_len, dim)
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">Attention</span><span class="p">,</span> <span class="n">FeedForward</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerBaseEncoderBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">att</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">Attention</span><span class="p">)</span>
        <span class="c1"># for reusable components, we pass it in as parameter and use deepcopy()
</span>        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="c1"># normalized_shape:
</span>        <span class="c1"># If a single integer is used, it is treated as a singleton list, and this module will normalize over the last
</span>        <span class="c1"># dimension which is expected to be of that specific size.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">feedforward</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">FeedForward</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">pad_mask</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">att</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">pad_mask</span><span class="p">))</span>
        <span class="c1"># att: Q=inputs, K=inputs, V=inputs
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">feedforward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
</code></pre></div></div> <p>where the modules (such as <strong>Attention</strong>, <strong>FeedForward</strong>) are the components you see from the previous section. And by stacking these blocks together, we can get <strong>the Encoder</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TransformerBaseEncoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_encoder_block</span><span class="p">,</span> <span class="n">encoder_block</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerBaseEncoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoder_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">encoder_block</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_encoder_block</span><span class="p">)])</span>
        <span class="c1"># nn.ModuleList receives a list containing modules and stores them
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="c1"># inputs are the embedded source sentences
</span>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">encoder_blocks</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="nf">block</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span>
</code></pre></div></div> <p>where the <strong>encoder_block</strong> is the above-defined <strong>TransformerBaseEncoderBlock</strong> module, and by calling its “<strong>forward()</strong>” method, we can get the final output of the entire Transformer encoder.</p> <p>Similarly, a decoder block is defined as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>


<span class="k">class</span> <span class="nc">TransformerBaseDecoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">Attention</span><span class="p">,</span> <span class="n">FeedForward</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerBaseDecoderBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">masked_att</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">Attention</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">att</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">Attention</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feedforward</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">FeedForward</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">pad_mask</span><span class="p">,</span> <span class="n">upper_triangle_mask</span><span class="p">):</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">masked_att</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">upper_triangle_mask</span><span class="p">))</span>
        <span class="c1"># masked_att: Q=inputs, K=inputs, V=inputs, mask=upper_triangle_mask
</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">att</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">pad_mask</span><span class="p">))</span>
        <span class="c1"># att: Q=inputs, K=encoder_outputs, V=encoder_outputs, mask=pad_mask
</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">feedforward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
</code></pre></div></div> <p>And <strong>the Decoder</strong> is then implemented as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TransformerBaseDecoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_decoder_block</span><span class="p">,</span> <span class="n">decoder_block</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerBaseDecoder</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">decoder_block</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_decoder_block</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">pad_mask</span><span class="p">,</span> <span class="n">upper_triangle_mask</span><span class="p">):</span>
        <span class="c1"># inputs are the embedded target sentences (training) / the newest embedded decoded word (test)
</span>        <span class="k">for</span> <span class="n">decoder_block</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">decoder_blocks</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="nf">decoder_block</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">pad_mask</span><span class="p">,</span> <span class="n">upper_triangle_mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span>
</code></pre></div></div> <p>where the <strong>decoder_block</strong> is just the above-defined <strong>TransformerBaseDecoderBlock</strong></p> <p>Finally, we can define our Transformer as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">model.Embedding</span> <span class="kn">import</span> <span class="n">TransformerEmbedding</span>
<span class="kn">from</span> <span class="n">model.Attention</span> <span class="kn">import</span> <span class="n">Attention</span>
<span class="kn">from</span> <span class="n">model.FeedForward</span> <span class="kn">import</span> <span class="n">FeedForward</span>
<span class="kn">from</span> <span class="n">model.TransformerEncoder</span> <span class="kn">import</span> <span class="n">TransformerBaseEncoderBlock</span><span class="p">,</span> <span class="n">TransformerBaseEncoder</span>
<span class="kn">from</span> <span class="n">model.TransformerDecoder</span> <span class="kn">import</span> <span class="n">TransformerBaseDecoderBlock</span><span class="p">,</span> <span class="n">TransformerBaseDecoder</span>


<span class="k">class</span> <span class="nc">TransformerBase</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_encoder_block</span><span class="p">,</span> <span class="n">num_decoder_block</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">num_vocab_src</span><span class="p">,</span> <span class="n">num_vocab_tgt</span><span class="p">,</span>
                 <span class="n">dim</span><span class="p">,</span> <span class="n">inner_layer_dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">PAD</span><span class="p">,</span> <span class="n">BOS</span><span class="p">,</span> <span class="n">EOS</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerBase</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">PAD</span> <span class="o">=</span> <span class="n">PAD</span>
        <span class="n">self</span><span class="p">.</span><span class="n">BOS</span> <span class="o">=</span> <span class="n">BOS</span>
        <span class="n">self</span><span class="p">.</span><span class="n">EOS</span> <span class="o">=</span> <span class="n">EOS</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_vocab_tgt</span> <span class="o">=</span> <span class="n">num_vocab_tgt</span>

        <span class="n">attention</span> <span class="o">=</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">)</span>
        <span class="n">feedforward</span> <span class="o">=</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">inner_layer_dim</span><span class="o">=</span><span class="n">inner_layer_dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">)</span>
        <span class="n">encoder_block</span> <span class="o">=</span> <span class="nc">TransformerBaseEncoderBlock</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">Attention</span><span class="o">=</span><span class="n">attention</span><span class="p">,</span>
                                                    <span class="n">FeedForward</span><span class="o">=</span><span class="n">feedforward</span><span class="p">)</span>
        <span class="n">decoder_block</span> <span class="o">=</span> <span class="nc">TransformerBaseDecoderBlock</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">Attention</span><span class="o">=</span><span class="n">attention</span><span class="p">,</span>
                                                    <span class="n">FeedForward</span><span class="o">=</span><span class="n">feedforward</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">embedding_src</span> <span class="o">=</span> <span class="nc">TransformerEmbedding</span><span class="p">(</span><span class="n">num_vocab</span><span class="o">=</span><span class="n">num_vocab_src</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">PAD</span><span class="o">=</span><span class="n">PAD</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding_tgt</span> <span class="o">=</span> <span class="nc">TransformerEmbedding</span><span class="p">(</span><span class="n">num_vocab</span><span class="o">=</span><span class="n">num_vocab_tgt</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">PAD</span><span class="o">=</span><span class="n">PAD</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="nc">TransformerBaseEncoder</span><span class="p">(</span><span class="n">num_encoder_block</span><span class="o">=</span><span class="n">num_encoder_block</span><span class="p">,</span> <span class="n">encoder_block</span><span class="o">=</span><span class="n">encoder_block</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="nc">TransformerBaseDecoder</span><span class="p">(</span><span class="n">num_decoder_block</span><span class="o">=</span><span class="n">num_decoder_block</span><span class="p">,</span> <span class="n">decoder_block</span><span class="o">=</span><span class="n">decoder_block</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">num_vocab_tgt</span><span class="p">)</span>
</code></pre></div></div> <p>The main components in our Transformer are: <strong>embedding layers</strong> of the source (<strong>inputs</strong>) and target (<strong>outputs</strong>); <strong>the Encoder</strong> and <strong>the Decoder</strong>; <strong>a projection layer</strong> $\mathbf{R}^{dim}\longrightarrow \mathbf{R}^{target\,vocab} $ that maps the final output to the vocabulary of the target.</p> <p>Note that in <a href="#2">[2]</a> and <a href="#3">[3]</a> there are two embedding layers for source and target respectively. In <a href="#1">[1]</a> there is only one embedding layer shared for source and target.</p> <p>Before we define the forward pass of our model, let’s first met some masking functions that helps us:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_pad_mask</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">PAD</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># padded places are True, real words are False
</span>
<span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">create_upper_triangle_mask</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># x shape: (batch_size, seq_len)
</span>    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)).</span><span class="nf">triu</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">bool</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># unseen words are True, available words are False
</span></code></pre></div></div> <p>The above two functions produce the aforementioned two different kinds of masks, namely <a href="#5">[padded mask]</a> and <a href="#6">[subsequent mask]</a>. And now we define the forward pass of our model as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># inputs shape: (batch_size, seq_len)
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">inputs_mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">create_pad_mask</span><span class="p">(</span><span class="n">inputs</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># of shape: (batch_size, 1, 1, seq_len_inputs) &lt;---&gt; in Attention,
</span>    <span class="c1"># when masking, (Q·K.transpose) is of shape: (batch_size, num_heads, seq_len_q, seq_len_kv)
</span>    <span class="c1"># where the seq_len_kv should equal inputs' seq_len
</span>    <span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding_src</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">from_encoder</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs_mask</span><span class="p">)</span>
    <span class="c1"># shape: (batch_size, seq_len, dim)
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">teacher_forcing</span><span class="p">(</span><span class="n">inputs_mask</span><span class="o">=</span><span class="n">inputs_mask</span><span class="p">,</span> <span class="n">from_encoder</span><span class="o">=</span><span class="n">from_encoder</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span>
<span class="c1"># the outputs are the log probabilities of shape: (batch_size, seq_len, vocab)
</span>
<span class="k">def</span> <span class="nf">teacher_forcing</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs_mask</span><span class="p">,</span> <span class="n">from_encoder</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">pad_mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">create_pad_mask</span><span class="p">(</span><span class="n">targets</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">upper_triangle_mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">create_upper_triangle_mask</span><span class="p">(</span><span class="n">targets</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">upper_triangle_mask</span> <span class="o">=</span> <span class="n">upper_triangle_mask</span> <span class="o">+</span> <span class="n">pad_mask</span>  <span class="c1"># batch_size, seq_len, seq_len
</span>    <span class="c1"># now the dtype is int, should convert to bool later
</span>    <span class="c1"># the upper_triangle_mask should not only mask the unseen words but also the paddings
</span>    <span class="n">targets</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding_tgt</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">from_encoder</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">inputs_mask</span><span class="p">,</span> <span class="n">upper_triangle_mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">bool</span><span class="p">())</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">projection</span><span class="p">(</span><span class="n">outputs</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># shape: (batch_size, seq_len, dim ---&gt; vocab)
</span>
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div> <p>During training the Transformer adapts the <strong>teacher-forcing training strategy</strong>, which means the decoder at time-step $t$ will always be fed the ground truth output token $y_t$ instead of using its own prediction $\hat{y}_t$ from time-step $t-1$. Another interesting observation is that <strong>the final output is not the probabilities but the log of them</strong>. (since <strong>F.log_softmax()</strong> is used instead of <strong>F.softmax()</strong>) This is because for loss computing we use the “<strong>nn.KLDivLoss</strong>”, which expects the predictions to be log probabilities. Finally we need to talk about the masks used here: <strong>a</strong>. the <strong>padded mask</strong> for the Encoder is used to mask the paddings in the inputs; <strong>b</strong>. the <strong>subsequent mask</strong> used in the Decoder is a little bit different from what we discussed <a href="#7">[here]</a>, it is actually the combination of a standard <strong>subsequent mask</strong> and a <strong>padded mask</strong>, which function is <strong>not only</strong> to prevent from attending to future tokens ($y_{t+1},\,y_{t+2},\,…$) at time-step $t$ <strong>but also</strong> to mask out the paddings; <strong>c</strong>. in <a href="#2">[2]</a> and <a href="#3">[3]</a> the <strong>padded mask</strong> in the Decoder is used for masking the paddings in source (input, which serves as <strong>K</strong> in <strong>Multi-Head Attention</strong>), however that of <a href="#1">[1]</a> is used for masking the paddings in targets (outputs, which serves as <strong>Q</strong> in <strong>Multi-Head Attention</strong>), and here I follow the implementation of <a href="#2">[2]</a> and <a href="#3">[3]</a>.</p> <p>At last, we now introduce implementation of the inference:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">max_generating_len</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">decoding_method</span><span class="o">=</span><span class="sh">'</span><span class="s">greedy</span><span class="sh">'</span><span class="p">,</span> <span class="n">beam_size</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">inputs_mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">create_pad_mask</span><span class="p">(</span><span class="n">inputs</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding_src</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">from_encoder</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs_mask</span><span class="p">)</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">greedy</span><span class="p">(</span><span class="n">inputs_mask</span><span class="o">=</span><span class="n">inputs_mask</span><span class="p">,</span> <span class="n">from_encoder</span><span class="o">=</span><span class="n">from_encoder</span><span class="p">,</span> <span class="n">max_generating_len</span><span class="o">=</span><span class="n">max_generating_len</span><span class="p">)</span> \
        <span class="k">if</span> <span class="n">decoding_method</span> <span class="o">==</span> <span class="sh">'</span><span class="s">greedy</span><span class="sh">'</span> <span class="k">else</span> \
        <span class="n">self</span><span class="p">.</span><span class="nf">beam_search</span><span class="p">(</span><span class="n">from_encoder</span><span class="o">=</span><span class="n">from_encoder</span><span class="p">,</span> <span class="n">max_generating_len</span><span class="o">=</span><span class="n">max_generating_len</span><span class="p">,</span> <span class="n">beam_size</span><span class="o">=</span><span class="n">beam_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sentences</span>
</code></pre></div></div> <p>for the decoding methods (“<strong>greedy</strong>” and “<strong>beam_search</strong>”), I did a minor adjustment to the decoding methods provided in <a href="#1">[1]</a> (basically the same, and it is already very excellent in <a href="#1">[1]</a>). And here I just introduce the idea of implementations for these two methods.</p> <p>For “<strong>greedy</strong>”:</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/pytorch_learning_1/pic2.png" style="zoom:80%;"/></div> <p>For “<strong>beam search</strong>”:</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/pytorch_learning_1/pic3.png" style="zoom:76%;"/></div> <h2 id="reference">Reference</h2> <p><a name="0"></a></p> <p>The following projects/websites are used as code references:</p> <p>[1] <a href="https://github.com/L-Zhe/SDISS">SDISS</a> is a Transformer-based sentence simplification model <a name="1"></a></p> <p>[2] <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a> is a post about the Transformer and its implementation <a name="2"></a></p> <p>[3] <a href="https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec">How to code The Transformer in Pytorch</a> is a post about the Transformer and its implementation <a name="3"></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[coding skill]]></summary></entry><entry><title type="html">Paper Summary 3</title><link href="https://a-chicharito-s.github.io/blog/2022/paper-summary-3/" rel="alternate" type="text/html" title="Paper Summary 3"/><published>2022-02-05T16:40:16+00:00</published><updated>2022-02-05T16:40:16+00:00</updated><id>https://a-chicharito-s.github.io/blog/2022/paper-summary-3</id><content type="html" xml:base="https://a-chicharito-s.github.io/blog/2022/paper-summary-3/"><![CDATA[<head> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script> </head> <h2 id="introduction">Introduction</h2> <p>This is the third paper summary focusing on text summarizations, which I think is a good topic to get to know about natural language generation (NLG) and a challenging field since summarizing the documents has fewer identifications (for example chat-bot can use the time-series information …, t-1, t, t+1, …) to rely on.</p> <h2 id="pre-trained-models">Pre-trained models</h2> <ul> <li><strong>Paper</strong>:</li> </ul> <h3 id="mass-masked-sequence-to-sequence-pre-training-for-language-generation---paper"><strong>MASS: Masked Sequence to Sequence Pre-training for Language Generation</strong> <a href="https://arxiv.org/abs/1905.02450">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong>:</li> </ul> <p>​ Other large models like BERT only pre-train the encoder or decoder, MASS is proposed to pre-train the encoder-decoder architecture jointly. Thus a better representation extractor and language model will be obtained.</p> <ul> <li> <p><strong>Solution</strong>:</p> <p>​ The idea is quite simple, the model adopts a seq-to-seq model and masks some tokens (e.g.: $x_3, x_4, x_5$) in the encoder and uses those unmasked tokens replaced with a special token [MASK] (e.g. $x_1, x_2,\,_,\,_,\,_,\,x_6, x_7\longrightarrow [MASK],\,[MASK],\,_,\,_,\,_,\,[MASK],\,[MASK]$) to reconstruct the masked tokens. The whole model is based on a Transformer architecture and pre-trained on monolingual data, later fine-tuned on three tasks: NMT, text summarization, response generation. <strong>The pre-training data is from WMT News Crawl datasets</strong>.</p> </li> </ul> <p>​ <strong>Highlights</strong> ​ The shining point of this is the idea of jointly training encoder and decoder to extract high-quality representation and gain a competitive language model. To be more specific, it inherits a lot of training ideas from BERT (such as masking words) and I think the value of this work is that it pre-trains the decoder together, making not only discriminative but also generative tasks possible to have a powerful pre-trained reference.</p> <ul> <li><strong>Architecture</strong>:</li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_3/pic1.png"/></div> <ul> <li><strong>Paper</strong>:</li> </ul> <h3 id="unified-language-model-pre-training-for-natural-language-understanding-and-generation---paper">Unified Language Model Pre-training for Natural Language Understanding and Generation <a href="https://arxiv.org/abs/1905.03197">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong>:</li> </ul> <p>​ This model uses three different language modeling tasks (namely: unidirectional, bidirectional, sequence-to-sequence modeling) to improve the ability of a Transformer-based architecture.</p> <ul> <li> <p><strong>Solution</strong>:</p> <p>​ The <strong>unidirectional</strong> language modeling lets the model attend to words from one direction (left to right / right to left) ;</p> <p>​ The <strong>bidirectional</strong> language modeling lets the model attend to words from both directions (like BERT) ;</p> <p>​ The <strong>sequence-to-sequence modeling</strong> lets the model attend to words to be predicted (e.g.: $y_2$) using its context (e.g.: $x_1, x_2, x_3\,and\,y_1$)</p> <p>​ And when pre-training, the above three objectives (for bidirectional Next Sentence Prediction (NSP) is used as well) are used averagely (1/3 for each) in every epoch. For fine-tuning on different tasks, [SOS] of the sentences are used or the concatenation of sentences is fed into the UniLM. And during testing, UniLM is used in a seq-to-seq architecture to decode for NLG tasks. <strong>The pre-training data is from English Wikipedia and BookCorpus</strong>.</p> </li> </ul> <p>​ <strong>Highlights</strong> ​ The highlight of this work I think is the proposal of sequence-to-sequence modeling and training three language modeling objectives together. The usage of sequence-to-sequence modeling makes it more diverse to explore the context of sentences and thus possible to fine-tune for the NLG tasks.</p> <ul> <li><strong>Architecture</strong>:</li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_3/pic2.png"/></div> <ul> <li><strong>Paper</strong>:</li> </ul> <h3 id="bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension---paper">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension <a href="https://arxiv.org/abs/1910.13461">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong>:</li> </ul> <p>​ BART can be viewed as a pre-trained large auto-encoder model. It is trained by first corrupting texts with a denoising function and then reconstructing the original text with the model. It uses the BERT encoder and the decoder from GPT. And since it naturally has a decoder, it is easy to be adapted to NLG tasks.</p> <ul> <li> <p><strong>Solution</strong>:</p> <p>​ In detail, BART’s decoder block performs an additional cross-attention operation with the outputs from the encoder, and compared with BERT, BART doesn’t add the feed-forward network before word prediction. <strong>The pre-training data is a combination of books and Wikipedia data</strong>.</p> <p>​ It proposes five different pre-training objectives to train BART, namely: Token Masking, Token Deletion, Text Infilling, Sentence Permutation, Document Rotation.</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_3/pic4.png"/></div> <p>For <strong>Token Masking</strong>, it simply follows BERT ;</p> <p>For <strong>Token Deletion</strong>, it randomly deletes tokens and the model will decide which positions of the inputs are missing ;</p> <p>For <strong>Text Infilling</strong>, it randomly selects a span with length from a Poisson distribution ($\lambda=3$) and replaces them with a single [MASK] token. Interestingly, when the length is 0, it becomes an insertion of the [MASK] token. (see above, the second sentence “DE.” becomes “D_E.”)</p> <p>For <strong>Sentence Permutation</strong>, it shuffles the sentences (indicated by a full stop) in a document.</p> <p>For <strong>Document Rotation</strong>, firstly a token is chosen randomly and then used as a pivot to rotate the document.</p> <p>When <strong>fine-tuning</strong>, for classification tasks, the same sentence sequence is fed into the encoder and decoder, and the final hidden state is used for prediction; for generation tasks, the BART architecture naturally has a decoder; for NMT especially, BART uses a randomly initialized encoder (same architecture as the pre-trained encoder in BART) to serve as the encoder for another language, thus BART can preserve its pre-trained knowledge on the monolingual data.</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_3/pic5.png"/></div> <p>For comparison, the authors also propose several pre-training objectives: <strong>a</strong>. Language Model (a left-to-right transformer) ; <strong>b</strong>. Permuted Language Model (XLNet based, sample 1/6 tokens and generate them auto-regressively in a random order) ; <strong>c</strong>. Masked Language Model (BERT based) ; <strong>d</strong>. Multitask Language Model (UniLM based, 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, 1/3 with the first 50% unmasked and the rest 50% of a left-to-right mask) ; <strong>e</strong>. Masked Seq-to-Seq (MASS based, a span containing 50% words is masked then a seq-to-seq model is trained to predict them) And for <strong>b</strong>, <strong>c</strong>, <strong>d</strong>, two-stream attention is used (see paper <a href="https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf">[here]</a>).</p> </li> </ul> <p>​ <strong>Highlights</strong> ​ I really like this work since it uses the simple idea of denoising auto-encoder (<strong>DAE</strong>) and shows a good example of combining two powerful architectures (BERT encoder + GPT decoder). The various forms of denoising functions are also very creative (especially the <strong>Text Infilling</strong> function, which can be seen as the combination of <strong>Token Masking</strong> and the reverse version of <strong>Token Deletion</strong>).</p> <ul> <li><strong>Architecture</strong>:</li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_3/pic3.png"/></div> <ul> <li><strong>paper</strong></li> </ul> <h3 id="pegasus-pre-training-with-extracted-gap-sentences-for-abstractive-summarization---paper">PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization <a href="https://arxiv.org/abs/1912.08777">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong>:</li> </ul> <p>​ PEGASUS is specifically designed for abstractive summarization, where instead of masking some words, it masks a whole sentence and tries to generate them during pre-training, which gives the model the ability to understand the document at sentence-level, thus achieving a better performance.</p> <ul> <li> <p><strong>Solution</strong>:</p> <p>​ The model is Transformer encoder-decoder based, and two pre-training objectives are used: namely Gap Sentence Generation (<strong>GSG</strong>) and Masked Language Model (like BERT). In the proposed GSG, it selects some “gap” sentences and uses one single token [MASK1] to mask the whole sentence (for <strong>MLM</strong> it uses [MASK2] to mask some words). And there’re three selecting strategies for GSG: <strong>1</strong>. randomly select $m$ sentences (<strong>Random</strong>); <strong>2</strong>. select the first $m$ sentences (<strong>Lead</strong>); <strong>3</strong>. select the top-$m$ importance sentences by calculating the ROUGE-F1 score between the selected sentences and the rest in the document (<strong>Principal</strong>), where sentences are either scored independently against each other (<strong>Ind</strong>) or sequentially by greedily maximizing the ROUGE-F1 of the selected top-$m$ sentences and the rest (<strong>Seq</strong>). When calculating ROUGE-F1, the original setting is by double-counting identical n-grams (<strong>Orig</strong>), and can be calculated by consider n-grams as a set (<strong>Uniq</strong>).</p> <p>​ Thus, for <strong>Principal</strong>, there are four selecting criteria ({<strong>Ind</strong>/<strong>Seq</strong>}, {<strong>Orig</strong>/<strong>Uniq</strong>}).</p> <p>​ The model is pre-trained on C4 and HugeNews and the two pre-training objectives are jointly used as shown in <a href="#1"><strong>Architecture</strong></a>.</p> </li> </ul> <p>​ <strong>Highlights</strong> ​ The reason behind the success of PEGASUS is the simultaneous usage of sentence-level mask and word-level mask, which enables the model to understand the meaning of sentences (word-level mask, doing language modeling) and to comprehend the content of the sentence in the document (sentence-level mask, doing sentence-level comprehension). These two objectives make the model suitable for summarizing over documents while maintaining good “reading” ability.</p> <ul> <li><strong>Architecture</strong>:<a name="1"></a></li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_3/pic6.png"/></div> <h2 id="survey">Survey</h2> <ul> <li><strong>Paper</strong>:</li> </ul> <h3 id="a-survey-on-dialogue-summarization-recent-advances-and-new-frontiers---paper">A Survey on Dialogue Summarization: Recent Advances and New Frontiers <a href="https://arxiv.org/abs/2107.03175">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong>:</li> </ul> <p>​ Summarizing dialogues into a shorter and salient version is of great value and research interests, and there lacks a comprehensive survey; thus this work is proposed.</p> <ul> <li> <p><strong>Main content</strong>:</p> <p>​ The survey focuses on dialogue summarization models w.r.t. the domains that the models are working on. Namely, Meeting, Chat, Medical Dialogue, Customer Service, Email Threads, and Others.</p> <p>​ For <strong>Meeting Summarization</strong>, the various interaction signals and the existence of multi-modal materials (such as video, image) can be considered besides the texts;</p> <p>​ For <strong>Chat Summarization</strong>, the problem lies in the complex conversational interactions (for example, in a chatting room, multiple participants may talk about on one topic and then move to another). Thus, it is difficult to keep track of the content of the conversation, such difficulty leads to factual inconsistency;</p> <p>​ For <strong>Email Threads Summarization</strong>, it is an asynchronous multi-party communication involving texts exchanging among multiple participants. The inherently available action items (such as “A send to B”) and the coarse-grained intents at email level can be paid more attention to;</p> <p>​ For <strong>Customer Service Summarization</strong>, there is conversation between a agent and a customer; thus, understanding the roles of speaker, the change of topics are very important. Moreover, some insights and characteristics from relative tasks can be considered (for example, consider “slot” and “intent” in summarization inspired by slot filling and intent detection tasks) ;</p> <p>​ For <strong>Medical Dialogue Summarization</strong>, it is important to be faithful to the dialogue since the summarization should capture precise information about facts, such as the name of a drug, the therapy, and the diagnoses. Thus, an extractive way combined with some slight abstractive manner is preferred. Note that Medical Dialogue Summarization still face the challenge of factual inconsistency;</p> <p>​ For <strong>Other Types of Dialogue Summarization</strong>, summarization tasks focusing domains in podcasts, online discussion forums, legal debates and reader comment threads have been proposed. There is task tackling personalized chat summarization problem;</p> </li> </ul> <p>​ <strong>Highlights</strong> ​ I think the most valuable part of this survey is the proposed possible frontiers: <strong>a</strong>. Faithfulness (related to factual inconsistent problem, and is as well potential in replacing ROUGE to serve as a new criterion) ; <strong>b</strong>. Multi-modal (may be very helpful for summarizations involving multimedia, such as <strong>Meeting Summarization</strong>) ; <strong>c</strong>. Multi-domain (can be divided into <strong>Macro</strong> and <strong>Micro</strong>, for <strong>Macro</strong>, it uses general domain summarization tasks, e.g. <strong>News</strong> to help a specific summarization task, e.g. <strong>Meeting</strong>. For <strong>Micro</strong>, it uses specific domain summarization tasks, e.g. <strong>Meeting</strong> to help another specific task, e.g. <strong>Email Threads</strong>. ) ;</p> <h2 id="models-based-on-pre-trained-models-and-everything-else">Models based on pre-trained models and everything else</h2> <ul> <li><strong>Paper</strong>:</li> </ul> <h3 id="text-summarization-with-pretrained-encoders---paper"><strong>Text Summarization with Pretrained Encoders</strong> <a href="https://arxiv.org/abs/1908.08345">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong>:</li> </ul> <p>​ Leverage BERT to get a powerful text summarization model for both extractive and abstractive tasks.</p> <ul> <li> <p><strong>Solution</strong>:</p> <p>​ To leverage BERT for summarization tasks, a few modifications are done to the embeddings of the original BERT. First, the [CLS] token is added to every input sentence to collect the information of each sentence. Second, unlike BERT, this paper apply interval segment embeddings to distinguish each sentence, in detail, [$E_A$] for odd sentences and [$E_B$] for even sentences. (in BERT there are usually two sentences, thus [$E_A$] and [$E_B$] don’t appear alternatively).</p> <p>​ With the above modified BERT encoder (BERTSUM), several Transformer layers are stacked above BERTSUM and then classified for extractive summarization task. For abstractive task, the BERTSUM encoder is used together with Transformer decoders in a seq-to-seq model. A two-stage fine-tuning approach is proposed by first fine-tune on the extractive task and then fine-tune it on the abstractive task.(According to the authors, “<em>Previous work (Gehrmann et al., 2018</em> {see <a href="https://aclanthology.org/D18-1443/">[paper]</a>}<em>; Li et al., 2018</em> {see <a href="https://aclanthology.org/D18-1205/">[paper]</a>}) <em>suggests that using extractive objectives can boost the performance of abstractive summarization.</em>”)</p> </li> </ul> <p>​ <strong>Highlights</strong> ​ The idea is quite simple, only a few modifications have been done to leverage BERT for summarization tasks. However it goes without saying that this work is important, since it shows the potential of BERT in summarization tasks and provides a useful general framework for both extractive and abstractive tasks.</p> <ul> <li><strong>Architecture</strong>:</li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_3/pic7.png"/></div> <ul> <li><strong>Paper</strong>:</li> </ul> <h3 id="refsum-refactoring-neural-summarization---paper"><strong><em>RefSum</em>: Refactoring Neural Summarization</strong> <a href="https://arxiv.org/abs/2104.07210">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong>:</li> </ul> <p>​ The aggregation of different SOTA summarization models can complement each other and achieve potential performance gain. However, the aggregation techniques (<strong>Meta stage</strong>) such as <em>stacking</em>, <em>re-ranking</em> are separated from the training of models (<strong>Base Stage</strong>), which leads to following limits:</p> <p><strong>a</strong>. <em>Ad-hoc Methods</em>: previous models are designed for a specific scenario.</p> <p><strong>b</strong>. <em>Base-Meta Learning Gap</em>: Summarization and combination of models use different architectures.</p> <p><strong>c</strong>. <em>Train-Test Distribution Gap</em>: Let $Hypo_{base}$, $Hypo^{‘}_{base}$ denote the training / test outputs from summarization models in <strong>Base Stage</strong>, and $Hypo_{meta}$, $Hypo^{‘}_{meta}$ (both are the outputs from <strong>Base Stage</strong>) denote that of the aggregation model in <strong>Meta Stage</strong>. The gap lies between the distributions of $Hypo_{meta}$, $Hypo^{‘}_{meta}$.</p> <p>​ To address the above challenges, this work proposes a general framework that can both summarize and aggregate.</p> <ul> <li> <p><strong>Solution</strong>:</p> <p>​ The previous works can be denoted as follows:</p> \[\mathcal{C}=\{C_i\,|\,C_i=BASE^{i}\,(D,\,\tau,\,\mathcal{S},\,\theta^{base})\}\longrightarrow C^{*}=META\,(D,\,\mathcal{C},\,\theta^{meta})\] <p>​ Where $\mathcal{C}$ is the set of candidate summaries. Each summary $C_i$ comes from summarization on document $D$ w.r.t. different parameterization choices on the system $BASE\,(·)$ and $\tau,\,\theta$ denote the training method and decoding strategy, respectively. The combination model $META\,(·)$ complements the candidates to produce the final summary $C^{*}$.</p> <p>​ The proposed model is called <strong><em>Refactor</em></strong>, which unifies the <strong>Base Stage</strong> and <strong>Meta Stage</strong>:</p> \[C^{*}=REFACTOR\,(D,\,\mathcal{C},\,\theta^{refactor})\] <p>where $REFACTOR\,(·,\,\theta^{refactor})$ is the <em>Refactor</em> model.</p> <p>​ The model adapts a pre-train then fine-tune learning pattern, for <strong>pre-training</strong>, the model serves as a scoring function:</p> </li> </ul> \[\begin{split} C^{*}&amp;=REFACTOR\,(D,\,\mathcal{C},\,\theta^{refactor}) \\ &amp;= \underset{c_i\,\in\,\mathcal{C}}{\operatorname{argmax}}\,(SCORE(\textbf{D},\,\textbf{C}_i)) \end{split}\] <p>​ where $\textbf{D},\,\textbf{C}_i$ are representations encoded by BERT and $SCORE(·)$ is a similarity scoring function implemented by BERT and Transformer blocks. The candidate set $\mathcal{C}$ is constructed by enumerating possible combinations of sentences in document $D$ where unreasonable combinations are pruned to control the sentence quality in $\mathcal{D}$. And when <strong>fine-tuning</strong>, the previous pre-trained model takes outputs from some base systems to fit the distribution of specific types of data. As a unified framework, the objective is a ranking loss: $L=\sum_{i}\sum_{j&gt;i}max(0,\,SCORE(\textbf{D},\,\textbf{C}_j)-SCORE(\textbf{D},\,\textbf{C}_i)+(j-i)*\lambda_c)$ where $ROUGE(C_i,\,\hat{C})&gt;ROUGE(C_j,\,\hat{C})$ for $i&lt;j$ and $\hat{C}$ is the reference summary. In application, the model can serve as either a model in <strong>Base Stage</strong> generating summaries or a combination model in <strong>Meta Stage</strong> complementing different candidates.</p> <p>​ <strong>Highlights</strong> ​ This architecture is quite simple and effective. The idea of unification is very inspiring, which can be am alternative perspective in the future work.</p> <ul> <li><strong>Architecture</strong>:</li> </ul> <p>​ No graphic illustration in the paper</p> <ul> <li><strong>Paper</strong>:</li> </ul> <h3 id="exploring-multitask-learning-for-low-resource-abstractive-summarization---paper"><strong>Exploring Multitask Learning for Low-Resource Abstractive Summarization</strong> <a href="https://arxiv.org/abs/2109.08565">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong>:</li> </ul> <p>​ Many tasks combined together can help improve the performance of abstractive tasks (especially in a low resource scenario); thus it’s worth exploring what kind of tasks can benefit abstractive summarization.</p> <ul> <li> <p><strong>Solution</strong>:</p> <p>​ For a low-resource dataset (with only a few training data), this paper proposes four auxiliary tasks to perform multitask learning, namely, extractive summarization, concept detection, paraphrase detection, language modeling. These tasks will help to improve the performance of abstractive summarization. And two popular framework, BERT and T5 are used for experiment.</p> <p>​ For BERT, in <strong>extractive summarization</strong>, a linear layer is added after the encoder’s outputs to classify which sentence is selected with input format of $[CLS]\,DW_1,\,…,DW_n\,[SEP]SW_1,\,…,SW_m$ where $DW_i$ is the $i$-th word of the document and $SW_j$ is the $j$-th word of the sentence to be classified. For <strong>concept detection</strong>, the model classify whether the word in a sequence is part of a concept extracted by a TF-IDF algorithm and the <strong>paraphrasing detection</strong> task asks the model to decide whether two input sentences $[CLS]\,Sent_1\,[SEP]\,Sent_2$ express the same idea with different phrasing. The <strong>language modeling</strong> task is directly from BERT. Note that when training, the <strong>paraphrasing detection</strong> task uses the MSRP dataset to train (which introduces new data) and the rest tasks all take the same dataset to do multitask learning (which in fact provides a multi-view towards the same data for the model). The overall training strategy for BERT is shown as follows.</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_3/pic8.png" style="zoom:80%;"/></div> <p>​ And the following figure is the training settings for T5.</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_3/pic9.png" style="zoom:60%;"/></div> <p>And when training, the multiple tasks are trained consecutively, in detail, after task 1 is trained, task 2 is to be trained and son on.</p> </li> </ul> <p>​ <strong>Highlights</strong> ​ I really like the idea of proposing multitask learning for low-resource data, since the limitations on the amount of available data requires us to utilize the data as much as possible, leveraging multitask learning gives the model multiple views towards the same data, thus, exploring the data throughly. However, besides the brilliant idea of multitask learning on low-resource data, the patterns this paper found that <strong>paraphrasing detection</strong> can help improve the performance of abstractive summarization under low-resource settings are not that <strong>CONVINCING</strong>, <strong><em>since when doing paraphrasing detection,it is performed on the MSRP dataset, which introduces new data to the model besides the original ones</em></strong>.</p> <ul> <li><strong>Architecture</strong></li> </ul> <p>see above</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[paper summary]]></summary></entry><entry><title type="html">Treasure Hunting 2</title><link href="https://a-chicharito-s.github.io/blog/2021/Treasure-hunting-2/" rel="alternate" type="text/html" title="Treasure Hunting 2"/><published>2021-12-07T16:40:16+00:00</published><updated>2021-12-07T16:40:16+00:00</updated><id>https://a-chicharito-s.github.io/blog/2021/Treasure-hunting-2</id><content type="html" xml:base="https://a-chicharito-s.github.io/blog/2021/Treasure-hunting-2/"><![CDATA[<head> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script> </head> <h2 id="conditional-poisson-stochastic-beam-search">Conditional Poisson Stochastic Beam Search</h2> <p>Today I’m going to introduce the paper ‘<strong>Conditional Poisson Stochastic Beam Search</strong>’ by <em>Clara Meister</em>, <em>Afra Amini</em>, <em>Tim Viera</em>, and <em>Ryan Cotterell</em>, you can find the paper <a href="https://arxiv.org/abs/2109.11034">[here]</a></p> <p>And different from the previous ‘<strong>Treasure Hunting</strong>’ episodes, in this very episode, instead of introducing the paper following its structure, I re-arrange the paper and introduce it in a more logical way that I consider myself better to understand. (<strong>And to be honest the paper is really tough for me !</strong>) Though still some parts of the paper remain unclear for me, I try my best to illustrate its main ideas.</p> <p>Note that: ‘<strong>xxx</strong>’ means quoted from the paper; <strong>xxx</strong> is to underline; <strong><em>sub_title</em></strong> is the sub title I named to help understand the structure of the paper; ‘<em>xxx</em>’ is the question / comments I raised to help understand the idea/logic of the paper; And the formulas are all from the paper (some minor modification for consistence with my own signs are done), and for writing fluency, I sometimes introduce concepts as ‘We define …’ instead of ‘The authors define …’</p> <h3 id="deterministic-vs-stochastic">Deterministic vs. Stochastic</h3> <ul> <li><strong><em>What is beam search ?</em></strong></li> </ul> <p><strong>Beam search</strong> is a very important decoding strategy for NLP tasks involving generation (e.g., NMT, text generation), they’re usually done in the following way:</p> <p>For example, we’re going to generate a sentence with a vocabulary $\mathcal{V}=\{I, like, spring, EOS\}$, where $BOS$ only indicates the end of the sentence. At the beginning of generation, we start with a $BOS$ token which indicates the beginning of the sentence. And beam search aims to find the top-$K$ possible decoding options at each time step, thus, at step 1, we may have a probability of ${I=0.6, like=0.1, spring=0.3}$, indicating the likelihood of a word from $\mathcal{V}$ being placed at the step ( at step 1, which means after $BOS$), and let the $K=2$ , then the decoding process can be roughly illustrated as: <a name="1"></a></p> <div> $$\begin{split} \{BOS\} &amp; \stackrel{step 1}{\longrightarrow}\{BOS+I={\color{red}0.6},BOS+like=0.1,BOS+spring={\color{red}0.3},BOS+EOS=0.0\} \\ &amp; \stackrel{step 2}{\longrightarrow}\{I+like={\color{red}0.6\times 0.9},I+I=0.6\times 0.05,I+spring=0.6\times 0.05,I\,+EOS=0.6\times 0.0; \\ &amp; \,\,\,\,\,\,\,\,\,\,\,\,\,\,spring+like={\color{red}0.3\times 0.8},spring+I=0.8\times 0.05,spring+spring=0.8\times 0.1, \\ &amp; \,\,\,\,\,\,\,\,\,\,\,\,\,\,spring\,+EOS=0.8\times 0.05;\} \\ &amp; \stackrel{step 3}{\longrightarrow}\,\,\,\,... \end{split}$$ </div> <p>Where ‘$+$’ means appending the word after the sequence from previous step, and <strong>for simplicity $BOS$ is removed after step 1</strong>. And we can see that each step the preserved top-$K$ beams (with highest top-$K$ probability of being generated, marked <font color="red">red</font>) will have a possible extension over $\mathcal{V}$, generating $K\times |\mathcal{V}|$ candidates, and then top-$K$ high probability beams are selected then (in above case, at step 2, they are: ‘<strong><em>I like</em></strong>’ and ‘<strong><em>spring like</em></strong>’).</p> <ul> <li><strong><em>What’s the drawbacks of beam search ? And can we solve them ?</em></strong></li> </ul> <p>With the above brief introduction about beam search, now we face a very tough question: <strong><em>What if the $K+1$-th beam at step $t$ is actually a better beam at step $t+1$ ?</em></strong> That means, by <strong>deterministically</strong> select top-$K$ best options out of $K\times |\mathcal{V}|$ at each time step, <strong>we may lose some better candidates</strong> ! And we of course want a generalized version of beam search, which can include beam search as a special case while has some <strong>stochasticity</strong>. $\longrightarrow$ and this leads to the answer: <strong>by sampling !</strong> Instead of deterministically choose top-$K$ beams at each time step, we <strong>stochastically</strong> sample a set with $K$ beams out of a base set with $K\times |\mathcal{V}|$ beams, and of cause a set with top-$K$ beams (in this sense, it means <strong>beam search</strong>) will still have a probability being sampled, thus we can say the <strong>stochastic sampling strategy</strong> is the generation of the <strong>deterministic</strong> beam search.</p> <ul> <li><strong><em>How the problem is formed ?</em></strong></li> </ul> <p>And the authors formulate the <strong>beam search</strong> problem as follows:</p> <p>the generation process can be written as:</p> <div> $$p(\textbf{y})=\prod \limits_{t=1}^{\|\textbf{y}\|}p(y_t|\textbf{y}_{&lt;t})$$ </div> <p>‘<strong>where $\textbf{y}$ is a member of a set of well-formed outputs $\mathcal{Y}$</strong> ‘. And $\textbf{y}=\{y_1,\,y_2,\,… \}$ where $y_k \in \mathcal{Y}$, $\textbf{y}_{&lt; t}=\{y_1,\,y_2,\,…,y_{t-1},\,y_t\}$. And in the following discussion, a max generation length $T$ for the sentence is considered.</p> <p>To solve the problem of $\textbf{y}^{*}=\underset{y\in \mathcal{Y}}{argmax}\,\,log\,p(\textbf{y})$, the beam search is then formulated as:</p> \[\begin{eqnarray*} Y_0 &amp;=&amp; {BOS} \tag{1} \\ Y_t &amp;=&amp; \underset{Y_t^{'}\subseteq B_t}{argmax}\,\,Q_t({Y_t^{'}}\,\|\,Y_{t-1}) \tag{2} \\ re&amp;turn\,Y_T \end{eqnarray*}\] <p>Where:</p> <div> $$Q_t(Y_t\,\|\,Y_{t-1})\overset{def}{\propto} \begin{cases} \prod \limits_{n=1}^{N}w_n &amp; \text{if |Y|=K}\\ 0&amp; \textbf{otherwise} \end{cases}\qquad \qquad \qquad \qquad \qquad (3)$$ </div> <p>Note that $Q_t(Y_t\,|\,Y_{t-1})$ is only assigned value when $|Y_t|=K$, and the though the assigned value is written as $\prod \limits_{n=1}^{N}w_n$ , it actually means for those $w_n$’s belonging to the set $Y_t$ . For example, if $K=3,\,N=9$ and ${w_1,\,w_4,\,w_5}$ belongs to $Y_t$ , then $\prod \limits_{n=1}^{N}w_n$ indicates $w_1\times w_4\times w_5$ . <a name="2"></a></p> <p>And now let’s continue to sort out some undefined concepts:</p> <p>if we define steps as $t=1,\,2,\,…T$, and $Y_{t-1}\,\circ\,V\overset{def}{=}{\textbf{y}\,\circ\,y\,|\,\textbf{y}\in Y_{t-1}\,\,\textbf{and}\,\,y\in V}$ , where $\circ$ means concatenation (which is the ‘$+$’ in the <a href="#1">above-mentioned case</a> ), and also: $B_t\overset{def}{=}Y_{t-1}\,\circ\,V$, thus $B_t$ is actually: ${\textbf{y}_{\leq t}^{(1)},\,…\textbf{y}_{\leq t}^{(N)}}$ where $N=K\times |\mathcal{V}|$ (except when $t=0$ since there is only a choice of $|\mathcal{V}|$ words for $BOS$), again <strong>for simplicity, ${\textbf{y}_{\leq t}^{(1)},\,…\textbf{y}_{\leq t}^{(N)}}$ is represented as ${1,\,2,\,…N}$ .</strong> And $w_n\,(=p(\textbf{y}_{\leq t}^{(n)}))$ indicates the probability of generation under the model (e.g., $spring+like={\color{red}0.3\times 0.8}$)</p> <h3 id="conditional-poisson-stochastic-beams">Conditional Poisson Stochastic Beams</h3> <ul> <li><strong><em>why called ‘conditional Poisson stochastic beam search’ ?</em></strong></li> </ul> <p>With the above mentioned definitions, a normalization to ‘<strong>the time-step dependent set function</strong>’ $Q_t(Y_t\,|\,Y_{t-1})$ (which makes it a distribution) can derive the ‘<strong>sample-without-replacement</strong>’ (‘<strong>without-replacement</strong>’ means one element after being chosen, can’t be chosen again) version of beam search:</p> <div> $$\begin{eqnarray*} Y_0 &amp;=&amp; {BOS} \tag{4} \\ Y_t &amp;\sim&amp; Q_t({Y_t^{'}}\,|\,Y_{t-1}) \tag{5} \\ re&amp;turn\,Y_T \end{eqnarray*}$$ </div> <p>And ‘<strong>This recursion corresponds to performing conditional Poisson sampling (CPS; Hájek 1964;see App. A for overview), a common sampling-without-replacement design (Tillé, 2006)<sup>3</sup>, at every time step.</strong>’ where in <strong>3</strong> the authors explain: ‘<strong>A sampling design is a probability distribution over sets of samples.</strong>’ And that’s why the proposed work is referred to as ‘<strong>conditional Poisson stochastic beam search</strong>’ (CPSBS)</p> <ul> <li><strong><em>How is it performed in detail ?</em></strong></li> </ul> <p>First of all, to understand the probability of a size $K$ set $Y_T$ is sampled, its marginal probability can be written as follows:</p> <div> $$P(Y_T)\,=\sum_{Y_1}...\sum_{Y_{T-1}}\prod \limits_{t=1}^{T}Q_t\,(\,Y_t\,|\,Y_{t-1})\qquad \qquad \qquad \qquad \qquad (6)$$ </div> <p><a name="5"></a></p> <p>And the summation is actually computing the marginal distribution out of a joint distribution. The above marginal distribution tells us that: for the final beam set $Y_T$ of size $K$, there’re roughly (<strong>less than</strong>): $|\text{#}Y_1|\times |\text{#}Y_2|\times ··· \times |\text{#}Y_{T-1}| \times |\text{#}Y_T|$ available values to be assigned with, where $|\text{#}Y_t|$ denotes the number of possible values for set $Y_t$ at time-step $t$ . And the authors state that: ‘<strong>Note the structural zeros of $Q_t$ prevent any incompatible sequence of beams</strong>’ , which can be answered by the following example:</p> <p>For a $K=2$ CPSBS with a vocabulary $\mathcal{V}={1,\,2,\,…\,,7}$ . If at $t=1$ , $Y_1$ can be: ${BOS+1,\,BOS+3}$ , then at $t=2$, $Y_2$ can be: ${BOS+12,\,BOS+15,\,BOS+32,\,BOS+34}$ .</p> <p><strong>However</strong>, note that $Q_2(Y_2=BOS+12\,|\,Y_1=BOS+3)$ and $Q_2(Y_2=BOS+15\,|\,Y_1=BOS+3)$ are both incompatible, vice versa. And this <strong>explains</strong> <strong>why</strong> $Q_t=0$ can prevent ‘<strong>incompatible sequence of beams</strong>’ and <strong>why</strong> the assignable values are <strong>less than</strong> the multiplication of available values at each time step.</p> <p>And also, for a given $Y_T^{(m)}=\{\textbf{y}_{\leq T}^{(m_1)}\,,\,\textbf{y}_{\leq T}^{(m_2)}\,,\,…,\,\textbf{y}_{\leq T}^{(m_K)}\}$, it’s generation probability can be simply computed as (<strong>no need to compute the summation</strong>, since the stochastic sample at each time-step should be deterministic in order to generate $Y_T^{(m)}$, to be specific, it means only $BOS+1$ can generate $BOS+12$ and $BOS+15$) : $P(Y_T=Y_T^{(m)})\,=\prod \limits_{t=1}^{T}Q_t\,(\,Y_t=\{\textbf{y}_{\leq t}^{(m_1)}\,,\,\textbf{y}_{\leq t}^{(m_2)}\,,\,…,\,\textbf{y}_{\leq t}^{(m_K)}\}\,|\,Y_{t-1}=\{\textbf{y}_{\leq t-1}^{(m_1)}\,,\,\textbf{y}_{\leq t-1}^{(m_2)}\,,\,…,\,\textbf{y}_{\leq t-1}^{(m_K)}\})$</p> <p>Thus, for a given final beam set $Y_T^{(m)}$ , we can actually compute its generation probability.</p> <ul> <li><strong><em>How is CPSBS performed ?</em></strong></li> </ul> <p>Now we formally look deeper into how CPSBS is performed at time-step $t$ . Before we start, bear in mind that we have a few things to do to perform CPSBS: <strong>1</strong>. the previous set function $Q_t(Y_t\,|\,Y_{t-1})$ is a scoring function (see <a href="#2">here</a>), we need to convert it into a distribution to perform sampling; <strong>2</strong>. an efficient and general algorithm should be there for us to perform sampling at each time-step; With these two preliminaries acknowledged, we now see how CPSBS is performed by the authors:</p> <p>Step 1: Normalize $Q_t(·\,|\,Y_{t-1})$ .</p> <p>We know that now $Q_t(·\,|\,Y_{t-1})$ should be able to sample a $Y_t^{any}$ containing $K$ beams based on the previous size-$K$ set $Y_{t-1}$ , and since we are actually selecting $K$ beams out of $N=K \times |\mathcal{V}|$ , there are actually $\binom{N}{K}$ options for us to sample such a size-$K$ set $Y_t^{any}$ , thus, $p(Y_t^{any})$ should be modified by the summation of the probabilities of size-$K$ sets that are possible to be sampled, and the normalization constant is defined as:</p> <div> $$Z_t\overset{def}{=}\sum_{Y_t\subseteq B_t,\,|Y_t|=K} \prod \limits_{n=1}^{N}w_n\qquad \qquad \qquad \qquad \qquad (7)$$ </div> <p><a name="4"></a></p> <p>Where the notation $\prod \limits_{n=1}^{N}w_n$ still follows the meaning of <a href="#2">this</a> . And following Kulesza and Taskar (2012, see <a href="https://www.nowpublishers.com/article/Details/MAL-044">here</a>), an iterative algorithm can be proposed: ( For detailed pseudocode please refer to the App. C of the paper)</p> <div> $$W\binom{n}{k}=\begin{cases} 1&amp; \text{if k=0 or n=k}\\ W\binom{n-1}{k}+w_nW\binom{n-1}{k-1}&amp; \text{if k} \in (0,n)\\ 0&amp; \text{otherwise} \end{cases}\qquad \qquad \qquad \qquad \qquad (8)$$ </div> <p><a name="3"></a></p> <p>And $Z_t=W\binom{N}{K}$.</p> <p>Step 2: Sample from $Q_t(·\,|\,Y_{t-1})$ (normalized) .</p> <p>After the distribution $Q_t(·\,|\,Y_{t-1})$ is normalized, the following algorithm is proposed by the authors:</p> <p>1: $Y_t \longleftarrow \emptyset$ (<em>Initialization</em>)<br/> <br/> 2: <strong>for</strong>$\,\,n=N,\,…\,,1$ :<br/></p> <p>   $\qquad k\longleftarrow K-|Y_t|$ (<em>Number of remaining elements</em>)<br/></p> <p>      Add the $n^{th}$ element of $B_t$ to $Y_t$ with probability:<br/></p> <p>                               $\frac{w_n\,W\binom{n-1}{k-1}}{W\binom{n}{k}}$<br/></p> <p>3: <strong>return</strong> $Y_t$ (Guaranteed to have size $K$)<br/></p> <p>And I explain the <strong>why</strong> the probability is $\frac{w_n\,W\binom{n-1}{k-1}}{W\binom{n}{k}}$ and <strong>why</strong> it is guaranteed to have size $K$ as follows:</p> <p><em>why the probability is $\frac{w_n\,W\binom{n-1}{k-1}}{W\binom{n}{k}}$ ?</em></p> <p>We can consider $\frac{w_n\,W\binom{n-1}{k-1}}{W\binom{n}{k}}$ as the probability of the $n^{th}$ element of $B_t$ being <strong>included</strong> in the final $Y_t$ . With <a href="#3">(8)</a> , we can derive:</p> <div> $$\frac{w_n\,W\binom{n-1}{k-1}}{W\binom{n}{k}}=\frac{W\binom{n}{k}-W\binom{n-1}{k}}{W\binom{n}{k}}=1-\frac{W\binom{n-1}{k}}{W\binom{n}{k}}$$ </div> <p>Where we consider $\frac{W\binom{n-1}{k}}{W\binom{n}{k}}$ as the probability of the $n^{th}$ element of $B_t$ being <strong>excluded</strong> in the final $Y_t$ . And according <a href="#4">(7)</a> and <a href="#3">(8)</a>, we can interpret $W\binom{n}{k}$ as the total probability of choosing the remaining $k$ of the total $K$-to-be-chosen elements <strong>out of</strong> the available element set : ${n,\,n-1,\,…\,1}$, and since the elements are chosen in a reverse order (from $N$ to $1$), thus $W\binom{n-1}{k}$ is then the total probability of choosing the remaining $k$ of the total $K$-to-be-chosen elements <strong>out of</strong> the available element set : ${n-1,\,…\,1}$ where element $n$ is excluded. Thus, the probability of the $n^{th}$ element of $B_t$ being <strong>excluded</strong> in the final $Y_t$ is: probability of choosing $k$ elements without element $n$ / probability of choosing $k$ elements considering element $n$ (though it may not be necessarily chosen), which is: $\frac{W\binom{n-1}{k}}{W\binom{n}{k}}$ .</p> <p><em>why it is guaranteed to have size $K$ ?</em></p> <p>Let’s consider an extreme situation: for the first $N-K$ elements, namely ${N,\,…\,,K+1}$, no elements are added to $Y_t$ , and now $n=k\,(=K)$, in this sense, according to <a href="#3">(8)</a>, the next element $K$, is added with a probability $\frac{w_n\,W\binom{n-1}{k-1}}{W\binom{n}{k}}=1-\frac{W\binom{n-1}{k}}{W\binom{n}{k}}$ ( <strong>though this only holds when $k\in (0,\,n)$</strong> ) where $W\binom{n}{k}=1$ and $W\binom{n-1}{k}=0$, thus, with a deterministic probability ‘1’ , the element $n$ will then be added, and $n,\,k$ again equals $K-1$, which means the above selecting process goes on again until all the remaining elements are added.</p> <h3 id="statistical-estimation-with-cpsbs">Statistical Estimation with CPSBS</h3> <ul> <li><strong><em>A supplementary: what is inclusion probability ?</em></strong></li> </ul> <p>For a certain beam $\textbf{y}_{\leq t}^{(n)}$ at time-step $t$, what’s its probability of being included in the $Y_t$ of this time-step? (i.e., $Pr(\textbf{y}_{\leq t}^{(n)}\in Y_t)$, which is called the <strong>inclusion probability</strong>) To understand this, we denote the inclusion probability of $\textbf{y}_{\leq t}^{(n)}$ ( <em>w.r.t.</em> $Q_t(·\,|\,Y_{t-1})$ ) as :</p> <div> $$\pi_{Q_t}(\textbf{y}_{\leq t}^{(n)}\,\|\,Y_{t-1})\overset{def}{=}\sum_{Y_t}Q_t(Y_t\,\|\,Y_{t-1})\,\mathbb{1}(\textbf{y}_{\leq t}^{(n)}\in Y_t)\qquad \qquad \qquad \qquad \qquad (9)$$ </div> <p>Where $Y_t$ ranges over all the possible size-$K$ set sampled from the base set $B_t$ , and $\mathbb{1}(\textbf{y}_{\leq t}^{(n)}\in Y_t)$ is an indicator, equals <strong>one</strong> if the desired beam $\textbf{y}_{\leq t}^{(n)}$ is in $Y_t$ , <strong>zero otherwise</strong>. And if at time-step $t$ we choose $w_n$ to make $\pi_{Q_t}(\textbf{y}_{\leq t}^{(n)}\,|\,Y_{t-1})\approx p(\textbf{y}_{\leq t}^{(n)})$ . It can recover beam search as we anneal the chosen weights: $w_n\rightarrow w_n^{1/\tau}$ as $\tau \rightarrow 0$, and the conditional Poisson distribution will assign probability 1 to the set containing the top-$K$ beams at time-step $t$ . And finding such $w_n$ s resulting a desired inclusion probability is possible, though requires solving a numerical optimization problem (Aires, 1999 {see <a href="https://link.springer.com/article/10.1023/A:1010091628740">[paper]</a>}; Grafström, 2009 {see <a href="https://www.sciencedirect.com/science/article/pii/S037837580800387X?via%3Dihub">[paper]</a>}) , thus the authors use an approximation of $w_n=p(\textbf{y}_{\leq t}^{(n)})/(1-p(\textbf{y}_{\leq t}^{(n)}))$ which yields good approximation in both theory and practice as reported in (Hájek, 1981 {see [[paper]]}; Bondesson et al. {see <a href="https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9469.2006.00497.x">[paper]</a>}, 2006; Aires, 1999 {see <a href="https://link.springer.com/article/10.1023/A:1010091628740">[paper]</a>}) .</p> <ul> <li><strong><em>How do we estimate statistical features of CPSBS ?</em></strong></li> </ul> <p>With above-mentioned sampling process of CPSBS at each time-step known, now we of course can’t help thinking about some questions like: ‘How to calculate one specific beam, say: $\textbf{y}^{(m)}$’s entropy ?’ or ‘How do we compute the BLEU score (see <a href="https://aclanthology.org/P02-1040.pdf">BLEU</a>) of $\textbf{y}^{(m)}$ ?’ . And unlike beam search where $\textbf{y}^{(m)}$ appears deterministically, in CPSBS, $\textbf{y}^{(m)}$ can be in many different $Y_T$ s, which leads us to the statistical estimation of the CPSBS. In a more mathematical way, as the authors state: ‘<strong>Let be $f:\,\mathcal{Y}\rightarrow \mathbb{R}^{d}$ , we seek to approximate its expected value under $p$:</strong>’</p> <div> $$\mathbb{E}_{\textbf{y}\sim p}[f(\textbf{y})]=\sum_{\textbf{y}\in \mathcal{Y}}p(\textbf{y})f(\textbf{y})\qquad \qquad \qquad \qquad \qquad (10)$$ </div> <p>And a traditional way is the Monte Carlo estimator: $G_{MC}\overset{def}{=}\frac{1}{M}\sum_{m=1}^{M}f(\textbf{y}^{(m)})$ where $\textbf{y}^{(m)}\overset{i.i.d}{\sim}p$ , and the authors argue that: ‘<strong>However, in the special case of sampling from a finite population—which is extremely common in NLP—it can be very wasteful. For example, if a distribution is very peaked, it will sample the same item repeatedly; this could lead to inaccurate approximations for some $f$. As a consequence, the mean square error (MSE) of the estimator with respect to $\mathbb{E}_{\textbf{y}\sim p}[f(\textbf{y})]$ can be quite high for small M.</strong>’ And since the sampling process of CPSBS is not independent (which means $\textbf{y}\nsim p$) , a <strong>Horvitz–Thompson estimator</strong> (see <a href="https://www.jstor.org/stable/2280784">[paper]</a> here) is used to estimate the expectation of a certain $f$ over $Y_T\sim P$ , where $P$ is <a href="#5">this</a> :</p> <div> $$G_{HT}\overset{def}{=}\sum_{\textbf{y}\in Y_T}\frac{p(\textbf{y})}{\pi_P(\textbf{y})}f(\textbf{y})\qquad \qquad \qquad \qquad \qquad (11)$$ </div> <p><a name="10"></a></p> <ul> <li><strong><em>Estimate the inclusion probability $\pi_P(y)$</em></strong></li> </ul> <p>As equation (11) mentions, to use the Horvitz–Thompson estimator we need to know the probability of generation: $p(y)$ (which is rather easy since it simply equals that of $Y_T$) and the inclusion probability: $\pi_P(y)$ . However, though computing $\pi_P(y)$ can give us an ‘<strong>unbiased</strong>’ HT estimator (for detailed information please refer to App. B. of the paper), <strong>to actually compute such a value is almost impossible</strong> (see the summations in the following <a href="#6">equation</a>). And of course, we have two ways: <strong>Naive Monte Carlo</strong> and <strong>Importance sampling</strong> to estimate the following inclusion probability:</p> <div> $$\pi_P(y)=\sum_{Y_T}P(Y_T)\,\mathbb{1}(\textbf{y}\in Y_T)=\sum_{Y_1}···\sum_{Y_T}\prod \limits_{t=1}^{T}Q_t\,(\,Y_t\,|\,Y_{t-1})\,\mathbb{1}(\textbf{y}_{\leq t}\in Y_t)\qquad \qquad \qquad \qquad \qquad (12)$$ </div> <p><a name="6"></a></p> <p>The Naive Monte Carlo estimator:</p> <p>We can defined the Naive Monte Carlo estimator as follows:</p> <div> $$\hat{\pi}_P^{MC}(\textbf{y})\overset{def}{=}\frac{1}{M}\sum_{m=1}^{M}\mathbb{1}(\textbf{y}\in Y_T^{(m)})\qquad \qquad \qquad \qquad \qquad (13)$$ </div> <p><a name="7"></a></p> <p>Where $Y^{(m)}\sim P$ . And ‘<strong>$\hat{\pi}_p^{MC}$ is an unbiased estimator of $\pi_P$ with variance : $\mathbb{V}[\hat{\pi}_P^{MC}]=\frac{1}{M}(\pi_P(\textbf{y})-\pi_P(\textbf{y})^{2})$ . Meanwhile $1/\hat{\pi}_P^{MC}$ is a <em>consistent estimator</em> of $1/\pi_P$ with <em>asymptotic variance</em> : $\mathbb{V}_a[\frac{1}{\hat{\pi}_P^{MC}}]=\frac{1}{M}(\frac{1}{\pi_P(\textbf{y})^{3}}-\frac{1}{\pi_P(\textbf{y})^{2}})$</strong>’ . (for proof please refer to the App. B.2. of the paper)</p> <p>And we can easily see the problems of the Naive Monte Carlo estimator. For a desired beam $\textbf{y}$ with a very low $\pi_P(\textbf{y})$ , the asymptotic variance of its estimated reverse is very high, thus resulting a less accurate estimate. Also, consider the summation in <a href="#7">(13)</a> , most of the sampled $Y_T^{(m)}$ s may not contain the desired beam $\textbf{y}$ , thus the sampling process will be time-consuming in order to have one $Y_T^{(m)}$ contain the desired beam $\textbf{y}$ , and most cases are the estimate of rare desired beam $\textbf{y}$ s tend to be zero.</p> <p>Importance sampling:</p> <p>To solve the problem of sampling lots of $Y_T^{(m)}$ s to contain the desired beam $\textbf{y}$ . We can actually make the sampled set include the desired beam $\textbf{y}$ , which the authors call: <strong>hindsight samples</strong> : $\tilde{Y}_1,\,\tilde{Y}_2,\,…,\tilde{Y}_T$ where they all contain the desired beam $\textbf{y}$ . And the hindsight samples can be generated through a <strong>proposal distribution</strong> conditioned on $\textbf{y}$ :</p> <div> $$\tilde{Q}_t(\tilde{Y}_t\,|\,\tilde{Y}_{t-1}\,,\textbf{y})\overset{def}{=}\frac{Q_t(\tilde{Y}_t\,|\,\tilde{Y}_{t-1})}{\pi_{Q_t}(\textbf{y}_{\leq t}\,|\,\tilde{Y}_{t-1})}\qquad \qquad \qquad \qquad \qquad (14)$$ </div> <p><a name="9"></a></p> <p>And according to the authors, the proposal distribution can be done by a simply modification to the CPSBS algorithm: ‘<strong>where $w(\textbf{y})$ corresponding to $\textbf{y}_{\leq t}^{(n)}$ is plcaed at the beginning and added to $Y_t$ deterministically</strong>’. For simplicity, the fact $\tilde{Y}$ and $\tilde{Q}$ are conditioned on $\textbf{y}$ are omitted. And the following lemma is proposed by the authors (see the proof in the App. B.2. of the paper):</p> <div> $$\tilde{P}(\tilde{Y}_1,\,...,\tilde{Y}_T)=\frac{P(\tilde{Y}_1,\,...,\tilde{Y}_T)}{\prod \limits_{t=1}^{T}\pi_{Q_t}(\textbf{y}_{\leq t}\,|\,\tilde{Y}_{t-1})}\qquad \qquad \qquad \qquad \qquad (15)$$ </div> <p><a name="8"></a></p> <p>Where $\tilde{P}(\tilde{Y}_1,\,…,\tilde{Y}_T)\overset{def}{=}\prod \limits_{t=1}^{T}\tilde{Q}_t(\tilde{Y}_t\,|\,\tilde{Y}_{t-1})$ is the <em>joint proposal distribution</em> . And $P(\tilde{Y}_1,\,…,\tilde{Y}_T)\overset{def}{=}\prod \limits_{t=1}^{T}Q_t(\tilde{Y}_t\,|\,\tilde{Y}_{t-1})$ is defined as the <em>joint probability of the beams under the original distribution $Q_t$</em> . And both $P$ and $\tilde{P}$ conditioning on $Y_0$ are omitted.</p> <p>And the computation for <a href="#8">(15)</a> makes use of the fact that the inclusion probability $\pi_{Q_t}(\textbf{y}_{\leq t})$ for a given $Q_t$ at each time-step can be computed with dynamic programming: (see the pseudocode in App. C. of the paper)</p> <div> $$\begin{eqnarray*}\pi_{Q_t}(\textbf{y}_{\leq t}^{(n)}\,|\,Y_{t-1}) &amp;\overset{def}{=}&amp; \sum_Y Q_t(Y_t)\,\mathbb{1}(\textbf{y}_{\leq t}^{(n)}\in Y_t) \\ &amp;=&amp; \frac{w_n}{Z}\frac{\partial Z}{\partial w_n} \tag{16} \end{eqnarray*}$$ </div> <p>Where $\textbf{y}_{\leq t}^{(n)}$ indicates the $n$-th candidate beam out of the $N$ beams. And for $\tilde{Y}_T^{(m)}\sim \tilde{P}$ where $\tilde{P}$ is defined by the proposal distribution in <a href="#9">(14)</a> . The inclusion probability in the HT estimator mentioned in <a href="#10">(11)</a> can be estimated as:</p> <div> $$\hat{\pi}_P^{IS}(\textbf{y})\overset{def}{=}\frac{1}{M}\sum_{m=1}^{M}\prod \limits_{t=1}^{T}\pi_{Q_t}(\textbf{y}_{\leq t}\,|\,\tilde{Y}_{t-1}^{(m)})\qquad \qquad \qquad \qquad \qquad (17)$$ </div> <p><a name="11"></a></p> <p>And the above estimation can be derived from:</p> <div> $$\begin{eqnarray*} \sum_{Y_T}P(Y_T)\,\mathbb{1}(\textbf{y}\in Y_T)&amp;=&amp;\sum_{Y_T}...\sum_{Y_1}P(Y_1,\,...\,,Y_T)\,\mathbb{1}(\textbf{y}\in Y_T)\\ &amp;=&amp;\sum_{\tilde{Y}_T}...\sum_{\tilde{Y}_1}P(\tilde{Y}_1,\,...\,,\tilde{Y}_T)\,\frac{\tilde{P}(\tilde{Y}_1,\,...\,,\tilde{Y}_T)}{\tilde{P}(\tilde{Y}_1,\,...\,,\tilde{Y}_T)} \\ &amp;=&amp;\sum_{\tilde{Y}_T}...\sum_{\tilde{Y}_1}\tilde{P}(\tilde{Y}_1,\,...\,,\tilde{Y}_T)\,\frac{P(\tilde{Y}_1,\,...\,,\tilde{Y}_T)}{\tilde{P}(\tilde{Y}_1,\,...\,,\tilde{Y}_T)} \\ &amp;\overset{lemma}{=}&amp;\sum_{\tilde{Y}_T}...\sum_{\tilde{Y}_1}\tilde{P}(\tilde{Y}_1,\,...\,,\tilde{Y}_T)\,\prod \limits_{t=1}^{T}\pi_{Q_t}(\textbf{y}_{\leq t}\,|\,\tilde{Y}_{t-1}) \\ \end{eqnarray*}$$ </div> <p>Which indicates that <a href="#11">(17)</a> inherits unbiasedness from the Naive Monte Carlo estimator in <a href="#7">(13)</a> . And the following properties can be observed from the Importance Sampling strategy in <a href="#11">(17)</a> :</p> <p>‘<strong><em>$\hat{\pi}_p^{IS}$ is an unbiased estimator of $\pi_P$ . Meanwhile $1/\hat{\pi}_P^{IS}$ is a consistent estimator of $1/\pi_P$ with an upper bound on asymptotic variance : $\mathbb{V}_a[\frac{1}{\hat{\pi}_P^{MC}}]\leq\frac{1}{M}\frac{r-1}{\pi_P(\textbf{y})^{2}}$ where an assumption that “for all $\tilde{Y}_1,\,…\,,\tilde{Y}_T$ the following bound: $\frac{\prod \limits_{t=1}^{T}\pi_{Q_t}(\textbf{y}_{\leq t}\,|\,\tilde{Y}_{t-1})}{\pi_P(\textbf{y})}\leq r$ holds” is made</em></strong>’ (for proof please refer to the App. B.2. of the paper). And the authors also mention that when $\prod \limits_{t=1}^{T}\pi_{Q_t}(\textbf{y}_{\leq t}\,|\,\tilde{Y}_{t-1})$ approximates the real $\pi_P(\textbf{y})$ , the variance of the Importance Sampling estimate is relatively smaller that that of the Naive Monte Carlo, ‘<strong>which is often the case for estimators when a proposal distribution is chosen judiciously (Rubinstein and Kroese, 2016).</strong>’ (see <a href="https://www.wiley.com/en-us/Simulation+and+the+Monte+Carlo+Method%2C+3rd+Edition-p-9781118632161">[paper]</a>)</p> <h3 id="experiments">Experiments</h3> <p>To test the performance of CPSBS and its HT estimator, they are tested on the sentence-level BLEU together with the Monte Carlo estimator, Sum and Sample estimator, an estimator for Stochastic Beam Search. To observe behaviors of the HT estimator under both high- and low-entropy setting, the model’s distribution is annealed as: $p_\tau(y_t\,|\,\textbf{y}_{&lt;t})\propto p\,(y_t\,|\,\textbf{y}_{&lt;t})^{\frac{1}{\tau}}$ .</p> <ul> <li><strong><em>Additional estimators to be compared with</em></strong></li> </ul> <p>For <strong>Monte Carlo</strong> sampling strategy with sample size $K$, the estimator: $G_{MC}\overset{def}{=}\frac{1}{M}\sum_{m=1}^{M}f(\textbf{y}^{(m)})$ is used to estimate the expectation of $f$ under the model where: $\textbf{y}^{(1)},\,…\,,\textbf{y}^{(K)}\overset{i.i.d.}{\sim}p$ . And this estimator is also <strong>used as a baseline by computing 50 times with a sample size of 200 each time</strong>.</p> <p>For <strong>Sum and Sample</strong>, the sum-and-sample estimator is an unbiased estimator which takes { a deterministically chosen set $Y$ of size $K-1$ (obtained using beam search in this experiment) and a sampled $\textbf{y}^{‘}$ from the remaining set $supp(p)\backslash Y$ } as input, the estimator can be written as : $G_{SAS}\overset{def}{=}\sum_{k=1}^{K-1}p(\textbf{y}^{(k)})f(\textbf{y}^{(k)})+\left( 1-\sum_{k=1}^{K-1}p(\textbf{y}^{(k)})\right)f(\textbf{y}^{‘})$ .</p> <p>For <strong>Stochastic Beam Search</strong>, which is a similar Sample-Without-Replacement algorithm built on the beam search making use of the truncated Gumbel random variables at each time-step. And a estimator following the Horvitz-Thompson scheme is built (similar to <a href="#10">(11)</a>).</p> <p>And for a more efficient estimation, a truncated distribution is used which preserves the 99% of the probability mass to accelerate the computation of <a href="#4">(7)</a>, which is ‘<strong>similar to the process in nucleus sampling (Holtzman et al., 2020)</strong>’ (see <a href="https://openreview.net/forum?id=rygGQyrFvH">[paper]</a> here). See the differences here:</p> <p><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_2/pic4.png" alt=""/></p> <ul> <li><strong><em>the function $f$ s whose expectation is to be computed</em></strong></li> </ul> <p><strong>BLEU score estimation</strong>:</p> <p>the score is defined as : $f(\textbf{y})=BLEU(\textbf{x},\,\textbf{y})$ where if for NMT models, $\textbf{x}$ is the reference translation. The sampling process is repeated 20 times and the mean and variance are plotted for each sample size. And see in <a href="#12">Figure 2.a</a> about the RMSE of the BLUE estimators that when temperature is not relatively high, CPSBS has a quite low RMSE, when the temperature is high, CPSBS becomes biased, which is ‘<strong>similar to Kool et al. (2019)’s observations</strong>’ (see <a href="https://proceedings.mlr.press/v97/kool19a.html">[paper]</a> here).</p> <p><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_2/pic1.png" alt=""/></p> <p><strong>Conditional Entropy Estimation</strong>:</p> <p>To estimate a model’s conditional entropy: $f(\textbf{y})=-log\,p(\textbf{y}\,|\,\textbf{x})$ where $\textbf{x}$ can be seen as the initial information necessary to generate the first set of beams, i.e., $Y_1$ . And see in <a href="#12">Figure 2.b</a> to see the RMSE for the conditional entropy estimation.</p> <p><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_2/pic3.png" alt=""/></p> <p><strong>RMSE</strong>:</p> <p>The RMSE evaluations for above-mentioned BLUE and conditional entropy are:</p> <p><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_2/pic2.png" alt=""/><a name="12"></a></p> <ul> <li><strong><em>Additional experiments</em></strong></li> </ul> <p><strong>Diverse Sampling</strong>:</p> <p>To test the diversity of the sampled translations $Y_T\sim P$, where $w_n=p(\textbf{y}_{\leq t}^{(n)})/(1-p(\textbf{y}_{\leq t}^{(n)}))$ at each time-step $t$ as suggested, an $n$-gram diversity metric is proposed: $D=\sum_{n=1}^{4} \frac{\text{#unique}\,\text{n-grams}\,in\,\text{K}\,\text{strings}}{\text{#}\,\text{n-grams}\,in\,\text{K}\,\text{strings}}$ and three decoding strategy: SBS, DiverseBS and ancestral sampling are compared with CPSBS, results are as follows:</p> <p><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_2/pic5.png" alt=""/></p> <p><strong>Decoding time evaluation</strong>:</p> <p>By setting different temperature and sample size, the decoding time for CPSBS, SBS and MC are evaluated:</p> <p><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_2/pic6.png" alt=""/></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[treasure hunting]]></summary></entry><entry><title type="html">Treasure Hunting 1</title><link href="https://a-chicharito-s.github.io/blog/2021/Treasure-hunting-1/" rel="alternate" type="text/html" title="Treasure Hunting 1"/><published>2021-11-29T16:40:16+00:00</published><updated>2021-11-29T16:40:16+00:00</updated><id>https://a-chicharito-s.github.io/blog/2021/Treasure-hunting-1</id><content type="html" xml:base="https://a-chicharito-s.github.io/blog/2021/Treasure-hunting-1/"><![CDATA[<head> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script> </head> <h2 id="birds-eye-probing-for-linguistic-graph-structures-with-a-simple-information-theoretic-approach">Bird’s Eye: Probing for Linguistic Graph Structures with a Simple Information-Theoretic Approach</h2> <p>Here I’m going to present you the first episode of my ‘<strong>Treasure Hunting</strong>’ series, unlike the ‘<strong>Paper Summary</strong>’ series, in which a group of papers will be roughly analyzed based on main ideas and motivations. In this series, one paper will be deeply analyzed at a time, focusing not only on the main idea, motivation, shining points but also the writing skills, experimental results etc.</p> <p>Today I’m going to introduce the paper ‘<strong>Bird’s Eye: Probing for Linguistic Graph Structures with a Simple Information-Theoretic Approach</strong>’ by <em>Yifan Hou</em> and <em>Mrinmaya Sachan</em>, you can find the paper <a href="https://aclanthology.org/2021.acl-long.145.pdf">[here]</a></p> <p>Note that: ‘<strong>xxx</strong>’ means quoted from the paper; <strong>xxx</strong> is to underline; <strong><em>sub_title</em></strong> is the sub title I named to help understand the structure of the paper; ‘<em>xxx</em>’ is the question / comments I raised to help understand the idea/logic of the paper;</p> <h3 id="abstract">Abstract</h3> <ul> <li><strong><em>Where dose the ‘graph’ come from ?</em></strong></li> </ul> <p>The author in this section first introduce us that ‘<strong>history of representing our prior understanding of language in the form of</strong> <strong>graphs</strong>’ is ‘<strong>rich</strong>’, indicating that ‘graph’ can be a useful tool to present a sentence, with this idea bear in mind, it’s then natural to ask that $\longrightarrow$ ‘<em>How much of the information presented by the graph is encoded by a model? (e.g., the word embedding)</em>’.</p> <ul> <li><strong><em>What dose ‘probe’ mean ?</em></strong></li> </ul> <p>And this question actually serves as the motivation of the <strong>probing</strong> filed of NLP $\longrightarrow$ which is to determine how much information (e.g., syntactic, semantic, which can be represented by <strong>graph</strong>s ) is learnt by the model. And as how intuition goes, the more information encoded, the better the model. Thus, finding a <strong>good</strong>, <strong>stable</strong> probing method that can provide interpretation to the existing models is very important.</p> <ul> <li><strong><em>Why we need information theory based probe model ?</em></strong></li> </ul> <p>However, the previous work is to train a probe model, say, based on accuracy (see <a href="#1">Probe model based on training for accuracy</a> for a detailed explanation), which is very unstable (see the discussion in xxx), thus, a probing model based on detecting <strong>mutual information</strong> between the graph and the embeddings of the to-be-detected model, which can preform steadily, is proposed by the authors.</p> <ul> <li><strong><em>What can the proposed model do ?</em></strong></li> </ul> <p>From a ‘<strong>Bird’s Eye</strong>’, the model can detect the encoded information for a complete sentence (e.g., the syntactic/semantic information of ‘I like the sunny weather today !’). From a ‘<strong>Worm’s Eye</strong>’, the model can detect the encoded information for a sub part of the sentence (e.g., the syntactic/semantic information of ‘the sunny weather’).</p> <h3 id="introduction">Introduction</h3> <ul> <li><strong><em>Background</em></strong></li> </ul> <p>Graphs are used in many NLP areas and is able to present language structure and its meaning. And there’re probe methods aiming to understand how much information is encoded by pre-trained models</p> <ul> <li><strong><em>Drawbacks / motivation</em></strong><a name="6"></a></li> </ul> <p>For existing methods, for example: the ‘<strong><em>structural probe</em></strong>’, is a method applying a linear transformation to predict the features (e.g., distance between two words, depth of a word) of a paring tree. However it fails to answer the question $\longrightarrow$ ‘<em>Dose the pre-trained model encode the entire graph ?</em>’ (since it only predicts some features of the parsing tree) and is only applicable to trees instead of general graphs. Moreover, a probe model based on training the accuracy is under the concern that <strong>it is trying to solve the task to improve acc instead of doing probing.</strong></p> <ul> <li><strong><em>Proposed method</em></strong></li> </ul> <p>For ‘<strong>Bird’s Eye</strong>’, the graph (adjacency matrix) is first converted into graph embeddings ($z_1,z_2,z_3,z_4$), and then the mutual information between graph embeddings ($z_1,z_2,z_3,z_4$) and word representations ($x_1,x_2,x_3,x_4$) is calculated by the model, which is the probe results of the to-be-probed model. For ‘<strong>Worm’s Eye</strong>’, the calculation process is quite like that of the ‘<strong>Bird’s Eye</strong>’, except for the following difference: ‘<strong>Worm’s Eye</strong>’ probes the local structure and ‘<strong>Bird’s Eye</strong>’ probes the entire structure.<a name="2"></a></p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_1/pic1.png"/></div> <h3 id="birds-eye-probe">Bird’s Eye Probe</h3> <ul> <li><strong><em>Assumption</em></strong></li> </ul> <p>By calculating the Mutual Information (MI) $I(\mathcal{X};\mathcal{G})$, where $\mathcal{X}$ is the sentence embeddings (e.g., $n \times d$, for which there’re $n$ words in the sentence, and the embedding dimension for each word is $d$ ), and $\mathcal{G}$ is the corresponding graph structure of that sentence. There’s an assumption about the alignment of words in $\mathcal{X}$ and $\mathcal{G}$ $\longrightarrow$ e.g., the #1 node in $\mathcal{G}$ is the #1 word in the sentence (see <a href="#2">here</a>). And for graphs having inconsistent number of words (usually smaller than) with the sentence, ‘<strong>an aligner might be needed in some cases (Banarescu et al., 2013).</strong>’</p> <ul> <li><strong><em>Challenges</em></strong></li> </ul> <p>The three challenges are: <a name="5"></a></p> <ol> <li> <p>it’s hard to estimate MI between <strong>discrete graphs</strong> (e.g., $\mathcal{G}$ which is an discrete adjacency matrix) and <strong>continuous features</strong> (e.g., $\mathcal{X}$ which is with continuous embeddings), since there’s no unified definition of the MI between them.</p> </li> <li> <p>MI estimation for <strong>large sample sizes</strong> or <strong>high dimensions</strong> don’t scale well.</p> </li> <li> <p>same MI values don’t imply same information encoded, e.g., if $I(\mathcal{X};\mathcal{G})=I(\mathcal{X};\mathcal{G~’})$ and $H(\mathcal{G})\neq H(\mathcal{G~’})$, where $G$ is the syntactic tree and $\mathcal{G~’}$ is the semantic graph. Though the MI values are the same, the uncertainty of the two graphs are different, thus it’s hard to simply say that information from the two graphs (<strong>which are formed by different mechanisms</strong>) has been equally encoded. (e.g., the #1, #3 words from $\mathcal{G}$ are well encoded and #4 word from $\mathcal{G~’}$ may be well encoded)</p> </li> </ol> <ul> <li><strong><em>Why and How the graph embedding is done ?</em></strong></li> </ul> <p><strong>Why</strong> the adjacency matrix (e.g. $n \times n$) needs to be converted to graph embedding (e.g., $n \times d$) ? It’s because that the adjacency matrix is <strong>discrete</strong> (no unified definition about MI between discrete and continuous variables) and <strong>sparse</strong> (computationally wasting), thus a new representation for the graph (i.e., <strong>graph embedding</strong>) should be computed. And <strong>How</strong> the graph embedding is computed ? In this paper, the authors used <strong>DeepWalk + skip-gram</strong> (input: one-hot vector $\mathbb1_v$ for a word, output: continuous vectors $\mathbb1_{v~’}$ for neighboring words), and to get well-learnt graph embeddings, the co-occurrence $L(\theta)$ is to be maximized:</p> <div> $$L(\theta)=\prod_{v\in V} \prod_{v^{'}\in N_v}P(\mathbb1_{v~'}|\mathbb1_v;\theta)$$ </div> <p>And the graph embeddings ($z_{v_k}$) is later concatenated together, which is</p> <div> $$\mathcal{Z}=[z_{v_1};z_{v_2};...;z_{v_n}]$$ </div> <ul> <li><strong><em>The math behind the model</em></strong></li> </ul> <p>Under some conditions (w.r.t. <a href="#3">Invariant property of MI</a>), there exists: $\mathcal{G}=f^{-1}(f(\mathcal{G}))$ where $f$ is an invertible function and $\mathcal{Z}=f(\mathcal{G})$. Then the following should hold: $I(\mathcal{X};\mathcal{Z})\approx I(\mathcal{X};\mathcal{G})$, which is the bedrock for converting <strong>discrete</strong>, <strong>sparse</strong> adjacency matrix $\mathcal{G}$ to <strong>continuous</strong> graph embedding $\mathcal{Z}$.</p> <p>And to estimate $I(\mathcal{X};\mathcal{Z})=KL(\mathbb{P}_{xz}||\mathbb{P}_x\times \mathbb{P}_z)$ under high dimensions, an approximation (see <a href="#4">Compression lemma lower bound</a>) is used:</p> <div> $$KL(P||Q)\geq \underset {T\in\mathcal{F}}{sup}\,\mathbb{E_\mathbb{P}}[T]-log(\mathbb{E_\mathbb{Q}}[e^T])$$ </div> <p>Thus:</p> <div> $$I(\mathcal{X};\mathcal{Z})=KL(\mathbb{P}_{xz}||\mathbb{P}_x\times \mathbb{P}_z)\geq \underset {T\in\mathcal{F}}{sup}\,\mathbb{E_{\mathbb{P}_{xz}}}[T]-log(\mathbb{E_{\mathbb{P}_x\times\mathbb{P}_z}}[e^T])$$ </div> <p>And in that sense, <strong>the task becomes</strong> to find a prefect $T$ that can approximate the real $I(X;Z)=KL(\mathbb{P}_{xz}||\mathbb{P}_x\times \mathbb{P}_z)$ as close as possible based on $\underset {T\in\mathcal{F}}{sup}\,\mathbb{E_{\mathbb{P}_xz}}[T]-log(\mathbb{E_{\mathbb{P}_x\times\mathbb{P}_z}}[e^T])$. And the objective function of which is: (where $T$ is simply a MLP)</p> <div> $$\underset {\theta\in\Theta}{max}(\,\mathbb{E_{\mathbb{P}_xz^{(n)}}}[T_\theta]-log(\mathbb{E_{\mathbb{P}_x^{(n)}\times\mathbb{P}_z^{(n)}}}[e^{T_\theta}]))$$ </div> <p>Where ${\mathbb{P}_{xz}^{(n)}}$, ${\mathbb{P}_{x}^{(n)}}$ and ${\mathbb{P}_{z}^{(n)}}$ are empirical joint, marginal distributions over a sample, which contains $n$ pairs of (sentence, graph). And for detailed training progress, see in <a href="#18">How the estimator is trained</a>.</p> <ul> <li><strong><em>Solution to <a href="#5">challenge 3</a></em></strong></li> </ul> <p>To make the estimated MI value more general and not being affected by the entropy of the graph, the formalism of the graph etc. <strong>Two control bounds</strong> $I(\mathcal{R};\mathcal{Z})$, $I(\mathcal{Z};\mathcal{Z})$ are introduced to map the MI value $I(\mathcal{X};\mathcal{Z})$ to $[0, 1]$, where $I(\mathcal{R};\mathcal{Z})\leq I(\mathcal{X};\mathcal{Z})\leq I(\mathcal{Z};\mathcal{Z})$, and the relative MI value for $I(\mathcal{X};\mathcal{Z})$ is:</p> <div> $$MIG(\mathcal{G})=\frac{\hat{I}(\mathcal{X};\mathcal{Z})-\hat{I}(\mathcal{R};\mathcal{Z})}{\hat{I}(\mathcal{Z};\mathcal{Z})-\hat{I}(\mathcal{R};\mathcal{Z})}$$ </div> <p>where the $\hat{I}(..;..)$ is the estimate from the model, $\mathcal{R}$ is a random variable independent of the graph $\mathcal{G}$ , and $\hat{I}(\mathcal{Z};\mathcal{Z})$ is actually calculated as $\hat{I}(\mathcal{Z+\epsilon};\mathcal{Z})$ where $\epsilon$ is the noise added to $\mathcal{Z}$ to prevent $\hat{I}(\mathcal{Z};\mathcal{Z})$ from going to infinity.</p> <p>And by scaling $I(\mathcal{X};\mathcal{Z})$ to $[0, 1]$, the gap between $KL(\mathbb{P}_{xz}||\mathbb{P}_x\times \mathbb{P}_z)$ and $\underset {T\in\mathcal{F}}{sup}\,\mathbb{E_{\mathbb{P}_xz}}[T]-log(\mathbb{E_{\mathbb{P}_x\times\mathbb{P}_z}}[e^T])$ is reduced, since for $(\mathcal{R}, \mathcal{Z})$, the gap is $I(\mathcal{R},\mathcal{Z})-\hat{I}(\mathcal{R},\mathcal{Z})=-\hat{I}(\mathcal{R},\mathcal{Z})$, and by scaling, this ‘gap’ is subtracted from $\hat{I}(\mathcal{X},\mathcal{Z})$ and $\hat{I}(\mathcal{Z},\mathcal{Z})$. Thus, reducing the error brought during training the MLP to approximate function $T$.</p> <ul> <li><strong><em>Worm’s Eye probe for local structure</em></strong></li> </ul> <p>The MI estimate for the local structure $\mathcal{G_s}$ is calculated as:</p> <div> $$MIL(\mathcal{G_S})=1-\frac{\hat{I}(\mathcal{X};\mathcal{Z^{'}})-\hat{I}(\mathcal{R};\mathcal{Z})}{\hat{I}(\mathcal{Z};\mathcal{Z})-\hat{I}(\mathcal{R};\mathcal{Z})}$$ </div> <p>Where $\mathcal{G_s}={V_s, E_s}$, for all the nodes in $V_s$ of the sub-graph $\mathcal{G_s}$, a noise (<strong>kept the same</strong>) is added to corresponding embedding $z_s$. Thus a corrupted graph embedding $\mathcal{Z^{‘}}$ is obtained with the to-be-tested local structure masked (noise added). By calculating the MI estimate for the corrupted graph embedding $\mathcal{Z^{‘}}$, it tells us how much the graph without the local structure $\mathcal{G_s}$ has in common with the embedding $\mathcal{X}$, and <strong>one minus that tells</strong> us how much the local structure $\mathcal{G_s}$ has in common with the embedding $\mathcal{X}$.</p> <h3 id="probing-for-syntactic-and-semantic-graph-structures">Probing for Syntactic and Semantic Graph Structures</h3> <p>For syntactic graph, the Stanford dependency syntax tree is used, and the direction of the edges, the labels (e.g., SUBJ, ATTR etc.) are ignored. And AMR (Abstract Meaning Representation) is used for semantic graph, and an off-the-shelf aligner (see paper <a href="https://aclanthology.org/D14-1048.pdf">[here]</a>) is used for possible situations when there’s no 1-to-1 relation between nodes and words. The direction of edges, labels are ignored as well.</p> <h3 id="experiment">Experiment</h3> <ul> <li><strong><em>Two things to do in experiments</em></strong></li> </ul> <p>To test the performance and effectiveness of the MI based model, there’ re two things to do: <strong>1. To test</strong> the assumption of $I(\mathcal{X};\mathcal{Z})\approx I(\mathcal{X};\mathcal{G})$. Which is to see if $\mathcal{Z} \in \mathbb{R}^{n\times d}$ can be transformed back to $\mathcal{G} \in\mathbb{R}^{n\times n}$, if so, it can be said that the information in $\mathcal{Z}$ can well cover the information in $\mathcal{G}$, thus proving the foundation of the model is right (since without it we won’t be able to compute the MI between discrete and continuous variables); <strong>2. To use</strong> the trained probe to detect how much syntactic / semantic information (presented by graphs) is encoded by the pre-trained models (<strong>with</strong> contextual information), to contrast, the static embeddings (<strong>without</strong> contextual information) like GloVe is used.</p> <ul> <li><strong><em>Testing effectiveness of the graph embedding</em></strong><a name="10"></a></li> </ul> <p>Given two nodes $(v_n, v_m)$, whose graph embeddings are $(z_{v_n},z_{v_m})$. These embeddings are concatenated then fed into the MLP to predict whether there is a link (edge) between $(v_n, v_m)$ or not. After all $n$ nodes’ links are predicted, the AUC score is calculated to show the effectiveness of the graph embedding. Different levels of MLP is used as well, and ‘0’ means linear transformation.</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_1/pic2.png"/></div> <p><a name="5"></a></p> <ul> <li><strong><em>Probe the entire structure</em></strong></li> </ul> <p>To probe the syntactic and semantic information encoded in the embeddings, the $MIG$ score is reported.</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_1/pic3.png"/></div> <p>And generally, <strong>non-contextual</strong> embeddings (GloVe, ELMo-0) perform worse on both syntax and semantics compared with <strong>contextual</strong> embeddings (Bert-base, Bert-large), and the gap between them on semantic information (right sub-figure) is smaller compared with that of syntactic information (left sub-figure). In that sense, <strong>none of them seems to be capable of capturing the semantic information well</strong>.</p> <ul> <li><strong><em>Probe local structure</em></strong></li> </ul> <p><strong>Probe the POS information in PTB (Penn Treebank dataset):</strong> POS relations are node labels and the following 5 POS tags are probed: $IN,\,NNP,\,DT,\,JJ,\,NNS$. Corresponding to-be-probed node’s graph embedding is added with a noise consistent with other tags., and the $MIL$ score is reported. ($NNP$: singular proper nouns, $JJ$: adjectives, $NNS$: plural nouns)</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_1/pic4.png"/></div> <p><strong>Probe the universal dependency relations in PTB (Penn Treebank dataset):</strong> the universal dependency relations in PTB is similar to that of the POS, and $prep,\,pobj,\,det,\,nn,\,nsubj$ are probed with the same method (adding a consistent noise to corresponding graph embedding). ($prep$: prepositional modifiers, $pobj$: object of a preposition, $nn$: noun compound modifier, $nsubj$: nominal subject)</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_1/pic5.png"/></div> <p><strong>Probe the semantics in AMR graph:</strong> There’re 3 relations to be probed: $arg,\,general,\,op$, and the corresponding graph embeddings are corrupted with 50% noise. (since the total 6 relations of AMR are not evenly distributed). The $MIL$ scores are reported as follows:</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_1/pic6.png"/></div> <ul> <li><strong><em>Probe using model based on accuracy</em></strong></li> </ul> <p>To contrast with the MI based probing model, based on the arguments in <em>structural probe</em> (see in <a href="#1">Probe model based on training for accuracy</a>), that ‘powerful models such as Bert will easily capture the syntactic and semantic information, thus a simple probing model should be designed’, the authors also trained a acc-based probe model. However, note in <a href="#5">here</a>, a simple linear transformation could not even restore the original graph structure from the graph embeddings, suggesting the effectiveness of a simple acc-based probe model (e.g., MLP) may have problems probing. And for <strong>probing the entire structure</strong> based on an accuracy model:</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_1/pic7.png"/></div> <p>We can see that probing model based on accuracy highly rely on the model’s structure (in this case, number of hidden layers), leading to inconsistency about the probing results. (which calls back to the previous discussion about <a href="#6">the drawbacks of acc-based probe model</a>). And for <strong>probing the local structure</strong> using the acc-based model, similar results can be observed. (<strong>Note that</strong> the model is first trained to probe the entire structure on training set, then AUC score for each relation is calculated based on the results from test set, and there’s no noise add. For a perturbation setting, refer to <a href="#7">A perturbation setting for acc-based probe</a>)</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_1/pic8.png"/></div> <ul> <li><strong><em>Discussion about the hyperparameter along with efficiency</em></strong></li> </ul> <p>Deeper MLPs (used for modeling the function $T$) can achieve tighter lower bound thus having a better estimation of the MI value, meanwhile ‘<strong>less efficient than shallow ones. Thus the selection of MI estimator’s complexity is a tradeoff.</strong>’ And for potential users, the authors mentioned that can use different graph embedding ways (get better graph embedding), or use sampling strategy (get better MI estimator) to achieve possible precision improvements.</p> <h3 id="related-work">Related work</h3> <p>Mainly introduced the concept of probing: ‘<strong>Syntax and Semantics Probing</strong>’ and then probing using information theory: ‘<strong>Information Theoretic Probe</strong>’ and how to estimate mutual information: ‘<strong>Mutual Information Estimation</strong>’.</p> <h3 id="limitations-and-future-work">Limitations and Future work</h3> <p>The limitations for the work, analyzed by the authors, are two: ‘<strong>First, a graph embedding is used, and some structure information could be lost in this process.</strong>’, ‘<strong>Second, training a MI estimation model is difficult.</strong>’. And the authors suggest that future work can be done based on this (<strong>graph embedding + MI estimator</strong>) framework, exploring methods towards better graph embeddings or MI estimation. And the authors also consider <strong>the importance of probing</strong>, which may serve as detecting sensitive contents, and helps to deploy neural networks in a better way by gaining syntactic and semantic interpretations for the model.</p> <h3 id="writing-skill-analysis">Writing skill analysis:</h3> <p>I personally quite enjoy the writing style in this paper, first all, it has a very clear Abstract which gives a explicit explanation about e.g., ‘what is probe?’, ‘why we’re doing probe?’, and points out the drawbacks of the existing methods (using ‘<strong>However</strong>’). And apart from the well-organized Abstract, the authors have the intention to do ‘<em>callbacks</em>’, e.g.: in <strong>2.2 Mutual Information Estimation</strong>, they wrote: ‘<strong>our objective of the neural network is to find an optimal function in $\mathcal{F}$ and estimate MI, rather than use prediction accuracy. Besides, the neural network is very simple (MLP).</strong>’ ,which not only highlighted the simplicity of the proposed method but also again, implied the drawbacks of previous models (‘<strong>use prediction accuracy</strong>’). Which is an important technique to use in writings.</p> <h3 id="shining-points-and-drawbacks">Shining points and Drawbacks:</h3> <p><strong>Shining points</strong>:</p> <p>First of all, the idea behind this work is quite interesting, which tackles the problem of ‘acc-based probe model may probe based on the complexity of the model instead of the structure’, and by proposing the MI-based probe model, there’re following solutions to solve the problems. Logically, the writing is fluent and reasonable.</p> <p>Also, the proposed framework is flexible and serves the motivations well, by firstly encoding <strong>sparse adjacency matrix</strong> into <strong>continuous graph embedding</strong>, which together with the continuous word embeddings will be used to compute the MI. ($\longrightarrow$ solves the problem that it’s unclear to compute the MI between discrete and continuous variables) And later the compression lower bound is used to approximate the MI between graph and word embeddings, which can be modeled by a MLP. ($\longrightarrow$ solves the problem of estimating MI). And two control bounds are used to map the MI value between [0, 1]. ($\longrightarrow$ solves the problem of standalone MI value may unable to reflect information encoded)</p> <p>Last but not least, the way how the authors proposed to probe the local structure is also novel, which corrupts corresponding graph embeddings, and then calculate the relative MI that is mapped to [0, 1], which indicates how much information <strong>except</strong> for the to-be-probed local structure is encoded, thus, one minus that will be the MI for the local structure.</p> <p><strong>Drawbacks</strong>:</p> <p>The first drawback I think is the graph embedding algorithms, since the authors point out that the previous acc-based probe models rely on its complexity instead of the graph structure, and MLPs of different layers are used to prove this, I think the authors should show the results of different graph embedding algorithms (and of course I think different graph embedding algorithms will result different MI estimations), and show that the performance is not easily affected.</p> <p>The second one I think is about the assumption made when trying to convert discrete adjacency matrix $\mathcal{G}\in\mathbb{R}^{n\times n}$ to continuous graph embedding $\mathcal{Z}\in\mathbb{R}^{n\times d}$, which is: $I(\mathcal{X};\mathcal{Z})\approx I(\mathcal{X};\mathcal{G})$. However, if look carefully into the <a href="#3">Invariant property of MI</a>, you will see that if $\mathcal{G}$ and $\mathcal{Z}$ are homeomorphic, the following should hold: $I(\mathcal{X};\mathcal{Z})= I(\mathcal{X};\mathcal{G})$, and in this case $\mathcal{G}$ is actually in $\mathbb{R}^{n}$ while $\mathcal{Z}$ is actually in $\mathbb{R}^{d}$, which <strong>by dimension</strong>, are impossible to be homeomorphic. <strong>One explanation is that</strong> this is the reason why the authors used ‘$\approx$’’ instead of ‘$=$’ , however, <strong>in this sense, it’s hard to know whether it holds mathematically or holds engineeringly</strong>. <strong>Another explanation is that</strong> if we assume that there exists a gold graph embedding $\mathcal{Z}_g\in\mathbb{R}^{n\times d}$ , which can be constructed from the discrete adjacency matrix $\mathcal{G}$ , and $\mathcal{Z}_g$ includes every information stored in $\mathcal{G}$ , thus, the way of doing graph embedding $\mathcal{Z}\in\mathbb{R}^{n\times d}$ out of $\mathcal{G}$ is just the mimicking of $\mathcal{Z}_g$ , for which is in the same space $\mathbb{R}^{d}$ with $\mathcal{Z}_g$ . Thus, $\mathcal{Z}$ and $\mathcal{Z}_g$ can be homeomorphic, and the Invariant property of MI may hold. <strong>However, this still raise another question, dose the gold $\mathcal{Z}_g$ exist ? ( although the authors did the <a href="#10">restoring experiment</a> suggesting its existence) and how is it achieved ?</strong></p> <h3 id="supplementary">Supplementary</h3> <p><strong>Probe model based on training for accuracy</strong> <a name="1"></a></p> <p>To gain a clue about the acc-based probe model, I here simply introduce the main methodology of the paper ‘<strong>A Structural Probe for Finding Syntax in Word Representations</strong>’ <a href="https://aclanthology.org/N19-1419.pdf">[paper]</a>, i.e. ‘<em>structural probe</em>’</p> <p>In this work, the authors assume that <strong>if the tree structure of a sentence is encoded, then the square of the L2 distance between embeddings of two words can be linearly transformed into the distance between the same two words in the tree structure</strong>. And the following distance can be defined:</p> <div> $$d_B(h_i^l,h_j^l)^2=(B(h_i^l-h_j^l))^T(B(h_i^l-h_j^l))$$ </div> <p>where $B\in\mathbb{R}^{k\times m}$ is the linear transformation which is to be learnt during training, $h_k^l$ is the embedding of the $k$-th word in the $l$-th sentence. And the training objective is:</p> <div> $$\underset {B}{min}\sum_l\frac{1}{|s^l|^2}\sum_{i,j}|d_{T^l}(w_i^l,w_j^l)-d_B(h_i^l,h_j^l)^2)|$$ </div> <p>where $|s^l|$ is the length of the sentence.</p> <p>Also, if the tree structure of a sentence is encoded, the <em>parse depth</em> $||w_i||$ of word $w_i$ can be probed too, and to train the probe model to have an accurate estimation, a new distance is defined as: $||h_i||_A=(Bh_i)^T(Bh_i)$, then $B$ is learnt with a similar training objective to that of distance probe.</p> <p><strong>Invariant property of MI</strong> <a name="3"></a></p> <p>This property comes from the paper ‘<strong>Estimating Mutual Information</strong>’ (see <a href="https://arxiv.org/pdf/cond-mat/0305641.pdf">[paper]</a>), where:</p> <p>If $X^{‘}=F(X)$ and $Y^{‘}=G(Y)$ are homeomorphisms (see more in <a href="#8">here</a>) and $J_X=||\partial X/\partial X^{‘}||$, $J_Y=||\partial Y/\partial Y^{‘}||$, then we have:</p> <div> $$\mu^{'}(x^{'},y^{'})=J_X(x^{'})J_Y(y^{'})\mu(x,y)$$ </div> <p>And thus, by definition: $I(X^{‘},Y^{‘})=\int\int d_{x^{‘}}d_{y^{‘}}\mu^{‘}(x^{‘},y^{‘})log\frac{\mu^{‘}(x^{‘},y^{‘})}{\mu_x^{‘}(x^{‘})\mu_y^{‘}(y^{‘})}=\int\int d_{x}d_{y}\mu(x,y)log\frac{\mu(x,y)}{\mu_x(x)\mu_y(y)}=I(X;Y)$</p> <p>And for the derivation of $\mu^{‘}(x^{‘},y^{‘})=J_X(x^{‘})J_Y(y^{‘})\mu(x,y)$, please consider the following:</p> <p>since $X^{‘}=F(X)$ is homeomorphism, thus, we have: $|p(x)\,d_x|=|q(x^{‘})\,d_{x{‘}}|$, the distribution of the new variable $X^{‘}$can be written as: $q(x^{‘})=p(x)|{d_x}/{d_{x^{‘}}}|$, which is called <strong>the Jacobian transformation</strong>. Similar results can be obtained for joint distributions, where for example, $|{d_x}/{d_{x^{‘}}}|$ becomes $|\partial x/\partial x^{‘}|$.</p> <p><em>Homeomorphism</em><a name="8"></a></p> <p>It’ a continuous function between topological spaces whose reverse is continuous as well. If between two spaces there is a homeomorphism, their topological properties are preserved during the mapping. There some examples: 1. open interval $(a, b), a\lt b$ is homeomorphic to real numbers $\mathbb{R}$; 2. $\mathbb{R}^m,\mathbb{R}^n$ are not homeomorphic if $m\neq n$. For more information, please refer to <a href="https://en.wikipedia.org/wiki/Homeomorphism#Properties">[wiki]</a></p> <p><strong>Compression lemma lower bound</strong> <a name="4"></a></p> <p>This bound is derived from ‘<strong>On Bayesian bounds</strong>’ (see <a href="https://dl.acm.org/doi/10.1145/1143844.1143855">[paper]</a>)</p> <p>First of all, let $\mathcal{H}$ be the set of predictors under consideration, and $h$ is one of them. Further define that $\phi(h)$ is any measurable function on $\mathcal{H}$, $P$ and $Q$ are any distributions on $\mathcal{H}$. Then we have:</p> <div> $$\begin{eqnarray*} E_Q[\phi(h)]-logE_P[exp(\phi(h))]\leq KL(Q||P) \tag{1} \\ \underset{\phi}{sup}\,(E_Q[\phi(h)]-logE_P[exp(\phi(h))]\leq KL(Q||P)) \tag{2} \end{eqnarray*} $$ </div> <p>For (1) we have:</p> <div> $$\begin{eqnarray*} E_Q[\phi(h)]&amp;=&amp;E_Q[log(\frac{dQ(h)}{dP(h)}\dot\,\,exp(\phi(h))\dot\,\,\frac{dP(h)}{dQ(h)})] \\ &amp;=&amp;KL(Q||P)+E_Q[log(exp(\phi(h))\dot\,\,\frac{dP(h)}{dQ(h)})] \\ &amp;\leq&amp; KL(Q\|\|P)+logE_Q[exp(\phi(h))\dot\,\,\frac{dP(h)}{dQ(h)}] \\ &amp;=&amp;KL(Q\|\|P)+logE_P[exp(\phi(h))] \end{eqnarray*} $$ </div> <p>Where the ‘$\leq$’ comes from <strong>Jensen’s inequality</strong>. (For concave it’s the reverse version of the normally known $f(E[X])\leq E[f(X)]$, for $f$ is a convex function)</p> <p>For (2), just simply let $\phi(h)=log(\frac{dQ(h)}{dP(h)})$, then it achieves the upper bound, thus:</p> <div> $$E_Q[\phi(h)]-logE_P[exp(\phi(h))]=E_Q[\phi(h)]=E_Q[log(\frac{dQ(h)}{dP(h)})]=KL(Q||P)$$ </div> <p><strong>How the estimator is trained</strong> <a name="18"></a></p> <p>For the training objective: $\underset {\theta\in\Theta}{max}(\,\mathbb{E_{\mathbb{P}_xz^{(n)}}}[T_\theta]-log(\mathbb{E_{\mathbb{P}_x^{(n)}\times\mathbb{P}_z^{(n)}}}[e^{T_\theta}]))$, there’re two terms to be optimized: one is the joint distribution term: $\mathbb{E_{\mathbb{P}_xz^{(n)}}}[T_\theta]$ , the other is the marginal distribution term: $log(\mathbb{E_{\mathbb{P}_x^{(n)}\times\mathbb{P}_z^{(n)}}}[e^{T_\theta}])$ .</p> <p>And for <strong>the joint distribution term</strong>, the word representation $\mathcal{X}$ is concatenated with the graph embedding $\mathcal{Z}$, and then put into the MLP to compute a scalar (see <a href="#9">here</a>), then average of scalars is computed as the expectation $\mathbb{E_{\mathbb{P}_xz^{(n)}}}[T_\theta]$.</p> <p>For <strong>the marginal distribution term</strong>, the representation $\mathcal{X}$ is randomly shuffled and then concatenated with the graph embedding $\mathcal{Z}$, and the same MLP is used to compute a scalar for marginal distribution, and then its exponential is taken and the average of the exponentials is computed as the expectation $\mathbb{E_{\mathbb{P}_x^{(n)}\times\mathbb{P}_z^{(n)}}}[e^{T_\theta}]$.</p> <p><em>How the scalar for one sentence is computed</em> <a name="9"></a></p> <p>For one sentence, say with 10 words, its word presentation $\mathcal{X}\in\mathbb{R}^{10\times 768}$, where 768 is the hidden size of each word, and its graph embedding $\mathcal{Z}\in\mathbb{R}^{10\times 128}$, where 128 is the embedding size of each node (indicating corresponding word), they are encoded into the same space with a dimension of 32: $\mathcal{X}\in\mathbb{R}^{10\times 768}$ $\longrightarrow$ $\mathcal{X}^{‘}\in\mathbb{R}^{10\times 32}$, $\mathcal{Z}\in\mathbb{R}^{10\times 128}$ $\longrightarrow$ $\mathcal{Z}^{‘}\in\mathbb{R}^{10\times 32}$, and then $\mathcal{X}^{‘}$ and $\mathcal{Z}^{‘}$ are concatenated to be a representation of $\mathbb{R}^{10 \times 64}$, and this intermediate space is then projected to a scalar, which is to project: $\mathbb{R}^{10 \times 64}$ $\longrightarrow$ $\mathbb{R}^{10 \times 1}$, and the average of the 10 scalars is computed.</p> <p><strong>A perturbation setting for acc-based probe</strong><a name="7"></a></p> <p>The word embedding is corrupted with the same noise, and for each relation, same amount of words are corrupted. And <strong>Worm’s Eye</strong> is used to predict the relations with the corrupted word embeddings. The results are as follows:</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/detailed_paper_1/pic9.png"/></div>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[treasure hunting]]></summary></entry><entry><title type="html">Keras Jewelries</title><link href="https://a-chicharito-s.github.io/blog/2021/keras-jewelries/" rel="alternate" type="text/html" title="Keras Jewelries"/><published>2021-11-24T16:40:16+00:00</published><updated>2021-11-24T16:40:16+00:00</updated><id>https://a-chicharito-s.github.io/blog/2021/keras-jewelries</id><content type="html" xml:base="https://a-chicharito-s.github.io/blog/2021/keras-jewelries/"><![CDATA[<head> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script> </head> <p>After making a horrible mistake which makes a two-week experiment nothing, I start to think maybe it’s time to summarize some typical coding skills as well as some reminders for <strong>Keras</strong></p> <h3 id="1-wrap-in-function-instead-of-sub-classing">1. Wrap in function instead of sub-classing</h3> <p>Consider the following scenario: you need to get an intermediate output of your model, which can be done by:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">old_model</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">old_model</span><span class="p">.</span><span class="n">get_layer</span><span class="p">[</span><span class="sh">'</span><span class="s">wanted_layer</span><span class="sh">'</span><span class="p">].</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div> <p>However, if your model is created by sub-classing:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">my_model</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="sh">'</span><span class="s">your_arguments_here</span><span class="sh">'</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">my_model</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="sh">'</span><span class="s">some custom properties here</span><span class="sh">'</span>
        <span class="sh">'</span><span class="s">make sure all layers, e.g.,CNN, are created here, and then later used in call()</span><span class="sh">'</span>
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="sh">'</span><span class="s">your defined computation process</span><span class="sh">'</span>
        <span class="k">return</span> <span class="n">y</span>
        <span class="sh">'</span><span class="s">y is the computed output</span><span class="sh">'</span>
</code></pre></div></div> <p>you’ll find that the ‘<strong>new_model</strong>’ will throw an error indicating that since it can’t infer the inputs shapes and what’s more, if you add ‘<strong>shape=()</strong>’ parameter to the ‘<strong>call()</strong>’ method or add ‘<strong>Input()</strong>’ layer in the ‘<strong>__init__()</strong>’ method, it will either not work or throw another error indicating you can’t add ‘<strong>Input()</strong>’ layer in the ‘<strong>__init__()</strong>’ method</p> <p>Thus, I suggest using the following way to create a model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_model</span><span class="p">(</span><span class="sh">'</span><span class="s">your_arguments_here</span><span class="sh">'</span><span class="p">):</span>
    <span class="n">x_1</span> <span class="o">=</span> <span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="sh">'</span><span class="s">shape of input x_1</span><span class="sh">'</span><span class="p">))</span>
    <span class="bp">...</span>
    <span class="n">x_n</span> <span class="o">=</span> <span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="sh">'</span><span class="s">shape of input x_n</span><span class="sh">'</span><span class="p">))</span>
    <span class="sh">'</span><span class="s">computation process here</span><span class="sh">'</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_1</span><span class="p">,...,</span><span class="n">x_n</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">y_1</span><span class="p">,..,</span><span class="n">y_m</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div> <p>In this sense, the model is then able to infer the inputs’ shapes when getting the intermediate outputs</p> <h3 id="2-long-directory-may-lead-to-failure-in-saving-model">2. Long directory may lead to failure in saving model</h3> <p>Funny story, when you’re trying to save a model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="sh">'</span><span class="s">your_model_name.hd5</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>However you can save it in a non-‘.hd5’ way, see <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models">[here]</a>. And sometimes (might be with lots of parameters), the saving process will fail and throw an error, which after online search, can be solve by shorten the saving directory, like:</p> <p>C:\Users\xxx\projects\tf_projects\keras_projects\my_project $\longrightarrow$ C:\Users\xxx\my_project</p> <h3 id="3-when-youre-padded-calculating-mean-involving-its-mask">3. When you’re padded, calculating ‘Mean’ involving its mask</h3> <p>For example, when you’re dealing with the padded sentences:</p> <p>​ input: [34, 52, 1257, 872, 0, 0, 0] (4 actual words, 3 paddings), with shape: (7, )</p> <p>​ input’s mask: [1, 1, 1, 1, 0, 0, 0]</p> <p>​ after embedding, it becomes with the shape: (7, embedding_dim)</p> <p>And now say, you would like to calculate the mean of the embeddings of words, you may write:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <p>However this is actually <strong>WRONG</strong> ! Since the calculation of the above code is actually:</p> <p>​ ($sum$( <strong>4</strong> embeddings for <strong>words</strong>) + $sum$( <strong>3</strong> embeddings for <strong>paddings</strong>)) / <strong>7</strong></p> <p>And what we want to calculate is:</p> <p>​ $sum$( <strong>4</strong> embeddings for words) / <strong>4</strong></p> <p>So what we should do is to make use of the mask:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keep_dim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>     
</code></pre></div></div> <h3 id="4-deal-carefully-with-length">4. Deal carefully with length</h3> <p>Sometimes you may encounter the following error:</p> <p>‘ValueError: all input arrays must have the same shape’</p> <p>This is when doing padding, you forget to control the length (e.g., forget to truncate, forget to unify the max length)</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[coding skill]]></summary></entry><entry><title type="html">Paper Summary 2(special edition)</title><link href="https://a-chicharito-s.github.io/blog/2021/paper-summary-2/" rel="alternate" type="text/html" title="Paper Summary 2(special edition)"/><published>2021-11-17T16:40:16+00:00</published><updated>2021-11-17T16:40:16+00:00</updated><id>https://a-chicharito-s.github.io/blog/2021/paper-summary-2</id><content type="html" xml:base="https://a-chicharito-s.github.io/blog/2021/paper-summary-2/"><![CDATA[<head> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script> </head> <h3 id="introduction">Introduction</h3> <p>This is the second paper summary focusing on the ‘best paper award’ winners of IJCAI (2015~2022), and if you want to see some other best papers of AI related major conferences, please refer to <a href="https://jeffhuang.com/best_paper_awards/#ijcai">[here]</a>. And as a <strong>special edition</strong> of the ‘paper summary’ series, here I only focus on the ‘<strong>abstract</strong>’ and ‘<strong>introduction</strong>’ part (which I may quote directly from the paper) instead of the architecture.</p> <h2 id="2020">2020</h2> <ul> <li><strong>Paper</strong> <h4 id="a-multi-objective-approach-to-mitigate-negative-side-effects-paper">A Multi-Objective Approach to Mitigate Negative Side Effects <a href="https://www.ijcai.org/proceedings/2020/0050.pdf">[paper]</a></h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“Agents operating in unstructured environments <strong>often</strong> <strong>create negative side effects (NSE)</strong>$^1$ that may not {$\longleftarrow$ <em>1. explain current problems, which is ‘NSE’</em>} be easy to identify at design time. <strong>We examine</strong>$^2$ how various forms of human feedback or autonomous exploration can be used to learn a penalty function associated with NSE during system deployment. {$\longleftarrow$ <em>2. what research we did towards the problem</em>} <strong>We formulate the problem</strong>$^3$ of mitigating the impact of NSE as a multi-objective Markov decision process with lexicographic reward preferences and slack. <strong>The slack denotes</strong>$^3$ the maximum deviation from an optimal policy with respect to the agent’s primary objective allowed in order to mitigate {$\longleftarrow$ <em>3. the main work we’ve done in this paper</em>} NSE as a secondary objective. <strong>Empirical evaluation</strong> <strong>of our approach</strong>$^4$ shows that the proposed framework can successfully mitigate NSE and that different feedback mechanisms introduce different biases, which influence the identification of NSE.” {$\longleftarrow$ <em>4. the results/improvement gains of our work</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>A detailed explanation of why ‘NSE’ will happen:</p> <blockquote> <p>“but inevitably details in the environment that are unrelated to the agent’s primary objective are ignored” “As a result of the limited fidelity of $\tilde{M}$ , the agent’s actions may have unmodeled, undesirable negative side effects (NSE) in some states”</p> </blockquote> </li> <li> <p>What we focus on:</p> <blockquote> <p>“We focus on NSE that are undesirable but not prohibitive.”</p> </blockquote> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_2/pic1.png"/></div> </li> <li> <p>The importance of our work (to eliminate ‘NSE’) and the efforts from previous work:</p> <blockquote> <p>“Learning to detect and minimize NSE is critical for safe deployment of autonomous systems” “Existing works (outlined in Table 1) mitigate NSE by recomputing the reward function for the agent’s primary objective”</p> </blockquote> </li> <li> <p>Our work (following quotes are with sequential-time order) and a summary on the contribution:</p> <blockquote> <p>“We propose a multi-objective approach that exploits …”$^1$ “The agent’s primary objective is to achieve its assigned task, while the secondary objective is to minimize NSE.”$^2$ “We investigate the efficiency of different feedback approaches …”$^3$ “Our primary contributions are: (1) …”</p> </blockquote> <p>in ‘1’—the main methods and why are using them are discussed</p> <p>in ‘2’—how the proposed methods work</p> <p>in ‘3’—how are the experiments and ideas behind them</p> </li> </ol> </li> <li><strong>Paper</strong> <h4 id="synthesizing-aspect-driven-recommendation-explanations-from-reviews-paper"><strong>Synthesizing Aspect-Driven Recommendation Explanations from Reviews</strong> <a href="https://www.ijcai.org/proceedings/2020/0336.pdf">[paper]</a></h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“Explanations help to make sense of recommendations, increasing the likelihood of adoption. <strong>However,</strong> <strong>existing approaches</strong> to explainable recommendations tend to rely on rigid, standardized templates, customized only via fill-in-the-blank aspect {$\longleftarrow$ <em>1. point out the existing limits of the field</em>} sentiments. <strong>For more</strong> flexible, literate, and varied explanations covering various aspects of interest, <strong>we synthesize</strong> an explanation by selecting snippets from reviews, while optimizing for representativeness and coherence. <strong>To fit target users’ aspect preferences</strong>, <strong>we contextualize</strong> the opinions based on a compatible explainable recommendation model. {$\longleftarrow$ <em>2. introduce the motivations and our solution</em>} <strong>Experiments</strong> on datasets of several product categories showcase the efficacies of our method as compared to baselines based on templates, review summarization, selection, and text generation.” {$\longleftarrow$ <em>3. explain experiment results (on which how it showed the model is good)</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <p>This is a quite different introduction since it actually has had itself organized by using the bold, descriptive titles in ‘Introduction’</p> <ol> <li> <p>Introducing the task:</p> <blockquote> <p>“Explainable recommendations are motivated by the need for …”</p> </blockquote> </li> <li> <p>Explain the existing problems and previous attempts towards the problem</p> <blockquote> <p>“<strong>Problem</strong>. An explanation is typically generated post hoc to the recommendation model.” “For instance, EFM [Zhang et al., 2014] has standardized templates for positive and negative opinions, each time substituting only the [aspect], e.g.,:”</p> </blockquote> </li> <li> <p>Introducing our contributions: (which includes the brief introduction of the contents of each following subsection)</p> <blockquote> <p>“<strong>Contributions.</strong> We make several contributions in this work.”</p> </blockquote> </li> </ol> </li> </ul> <h2 id="2019">2019</h2> <ul> <li><strong>Paper</strong> <h4 id="boosting-for-comparison-based-learning--paper"><strong>Boosting for Comparison-Based Learning</strong> <a href="https://www.ijcai.org/proceedings/2019/0255.pdf">[paper]</a></h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“We <strong>consider the problem of classification in a</strong> <strong>comparison-based setting</strong>: given a set of objects, we only have access to triplet comparisons of the form object $x_i$ is closer to object $x_j$ than to object {$\longleftarrow$ <em>1. specify the setting of the task/ introducing background</em>} $x_k$. <strong>In this paper we introduce</strong> TripletBoost, a new method that can learn a classifier just from such triplet comparisons. <strong>The main idea</strong> is to aggregate the triplets information into weak classifiers, which can subsequently be boosted to a strong classifier. {$\longleftarrow$ <em>2. the motivation/idea of this work</em>} Our method has <strong>two main advantages</strong>: (i) it is applicable to data from any metric space, and (ii) it can deal with large scale problems using only passively obtained and noisy triplets. We derive theoretical generalization guarantees and a lower bound on the number of necessary triplets, and <strong>we empirically show that</strong> our method is both competitive with state of the art approaches and resistant to noise.” {$\longleftarrow$ <em>3. the gains/improvements of our methods</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Introduce what is ‘comparison-based learning’:</p> <blockquote> <p>“In the past few years the problem of comparison-based learning has attracted growing interest …”</p> </blockquote> </li> <li> <p>Specifically explain the proposed setting/ assumption/ question in this work:</p> <blockquote> <p>“We address the problem of classification with noisy triplets that have been obtained in a passive manner: the examples lie in an unknown metric space, …”</p> <p>“Another interesting question in this context is that of the minimal number of triplets required to successfully learn a classifier. …”</p> </blockquote> </li> <li> <p>Further introduction to our proposed method, and why they are efficient:</p> <blockquote> <p>“In this paper we propose TripletBoost …”</p> <p>“Our method is based on the idea that the triplets can be aggregated into simple triplet classifiers, …”</p> <p>“From a theoretical point of view we prove that …”</p> <p>“From an empirical point of view we demonstrate that”</p> </blockquote> </li> </ol> </li> </ul> <p>and for ‘triplet classifiers’, there is a demonstration:</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_2/pic2.png" style="zoom:75%;"/></div> <h2 id="2018">2018</h2> <ul> <li><strong>Paper</strong> <h4 id="a-degeneracy-framework-for-graph-similarity--paper">A Degeneracy Framework for Graph Similarity <a href="https://www.ijcai.org/proceedings/2018/0360.pdf">[paper]</a></h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>The problem of</strong> accurately measuring the similarity between graphs is at the core of many applications in a variety of disciplines. <strong>Most existing</strong> <strong>methods</strong> for graph similarity focus either on local or on global properties of graphs. <strong>However, even if</strong> graphs seem very similar from a local or a global perspective, they may exhibit different structure at {$\longleftarrow$ <em>1. point out the problems for measuring graph similarity</em>} different scales. <strong>In this paper, we present</strong> a general framework for graph similarity which takes into account structure at multiple different scales. <strong>The proposed framework capitalizes</strong> on the well-known k-core decomposition of graphs in order to build a hierarchy of nested subgraphs. <strong>We apply</strong> <strong>the framework to derive</strong> variants of four graph kernels, namely graphlet kernel, shortest-path kernel, Weisfeiler-Lehman subtree kernel, and pyramid match graph kernel. <strong>The framework is not</strong> <strong>limited to</strong> graph kernels, but can be applied to any {$\longleftarrow$ <em>2. present the work and its main appealing properites</em>} graph comparison algorithm. The proposed framework <strong>is evaluated on</strong> several benchmark datasets for graph classification. In most cases, the corebased kernels achieve significant improvements in terms of classification accuracy over the base kernels, while their time complexity remains very attractive.” {$\longleftarrow$ <em>3. summary of the gains of the proposed method based on</em> <em>experiment</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Introducing graph and the importance of the kernel (background):</p> <blockquote> <p>“Graphs are well-studied structures which are …”</p> <p>“So far, kernel methods have emerged as one of the most effective tools for graph classification, and …”</p> </blockquote> </li> <li> <p>A general introduction of the kernel and some lacks:</p> <blockquote> <p>“Most graph kernels in the literature are instances of …”</p> <p>“Most existing graph kernels can thus be divided into two classes. …”</p> <p>“Therefore, existing graph kernels focus mainly on either local or global properties of graphs. In practice, it would be desirable to have a kernel that can take structure into account at multiple different scales”</p> </blockquote> </li> <li> <p>Summary on our work:</p> <blockquote> <p>“In this paper, we propose a framework for comparing structure in graphs at a range of different scales.”</p> <p>“More specifically, the contributions of this paper are threefold:”</p> </blockquote> </li> </ol> </li> <li><strong>Paper</strong> <h4 id="commonsense-knowledge-aware-conversation-generation-with-graph-attention--paper"><strong>Commonsense Knowledge Aware Conversation Generation with Graph Attention</strong> <a href="https://www.ijcai.org/proceedings/2018/0643.pdf">[paper]</a></h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>Commonsense knowledge is vital to</strong> many natural language processing tasks. <strong>In this paper, we</strong> <strong>present</strong> a novel open-domain conversation generation model to demonstrate how large-scale commonsense knowledge can facilitate language understanding {$\longleftarrow$ <em>1. explain what this work did in general</em>} and generation. <strong>Given a user post</strong>, the model retrieves relevant knowledge graphs from a knowledge base and <strong>then</strong> encodes the graphs with a static graph attention mechanism, which augments the semantic information of the post and thus supports better understanding of the post. <strong>Then, during</strong> <strong>word generation</strong>, the model attentively reads the retrieved knowledge graphs and the knowledge triples within each graph to facilitate better generation through a dynamic graph attention mechanism. {$\longleftarrow$ <em>2. the more detailed working flow process of the proposed method</em>} <strong>This is the first attempt</strong> that uses large-scale commonsense knowledge in conversation generation. <strong>Furthermore, unlike existing models</strong> that use knowledge triples (entities) separately and independently, our model treats each knowledge graph as a whole, which encodes more structured, connected {$\longleftarrow$ <em>3. show the originality and advantages of this work</em>} semantic information in the graphs. <strong>Experiments</strong> <strong>show that</strong> the proposed model can generate more appropriate and informative responses than state of- the-art baselines.” {$\longleftarrow$ <em>4. summary the performance gain of the model shown by</em><br/> <em>experiments</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Introducing the background:</p> <blockquote> <p>“Semantic understanding, particularly when facilitated by commonsense knowledge or world facts, is essential to many natural language processing tasks”</p> <p>“Recently, a variety of neural models has been proposed for conversation generation”</p> </blockquote> </li> <li> <p>Explain motivation (an example is given):</p> <blockquote> <p>“A model can understand conversations better and thus respond more properly if it can access and make full use of large-scale commonsense knowledge”</p> <p>“For instance, to understand a post-response pair “Don’t order drinks at the restaurant , ask for free water” and “Not in Germany. Water cost more than beer. Bring you own water bottle”, we need commonsense knowledge such as (water, At Location, restaurant), (free, Related To, cost), etc.”</p> </blockquote> </li> <li> <p>Drawbacks of previous work:</p> <blockquote> <p>“First, they are highly dependent on …”</p> <p>“Second, they usually …”</p> </blockquote> </li> <li> <p>Introduction to our methods (and a graph is provided to help better understand the concept of introducing commonsense knowledge) :</p> <blockquote> <p>“To address the two issues, we propose …”</p> <p>“We use a large-scale commonsense knowledge …”, “To fully leverage the retrieved graphs …”</p> <p>“In summary, this paper makes the following contributions: …”</p> </blockquote> </li> </ol> </li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_2/pic3.png" style="zoom:75%;"/></div> <ul> <li><strong>Paper</strong> <h4 id="from-conjunctive-queries-to-instance-queries-in-ontology-mediated-querying--paper"><strong>From Conjunctive Queries to Instance Queries in Ontology-Mediated Querying</strong> <a href="https://arxiv.org/pdf/2010.11848.pdf">[paper]</a></h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>We consider</strong> ontology-mediated queries (OMQs) based on expressive description logics of the $\mathcal{ALC}$ family and (unions) of conjunctive queries, studying the rewritability into OMQs based on instance {$\longleftarrow$ <em>1. explain generally what this work has done</em>} queries (IQs). <strong>Our results include</strong> exact characterizations of when such a rewriting is possible and tight complexity bounds for deciding rewritability. We also give a tight complexity bound for the related problemof deciding whether a given MMSNP sentence is equivalent to a CSP.” {$\longleftarrow$ <em>2. the results observed from this work</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Explains a few (which means many terms aren’t specified) concepts and previous work:</p> <blockquote> <p>“An ontology-mediated query (OMQ) is …” “An important step into this direction has been made by …”</p> </blockquote> </li> <li> <p>Motivations of this work:</p> <blockquote> <p>“There are two additional motivations to study the stated question. The first one comes from …” “The second motivation stems from …”</p> </blockquote> </li> <li> <p>A detailed introduction about what this work has done:</p> <blockquote> <p>“The main aim of this paper is to study the rewritability …” “Regarding IQ-rewritability as a decision problem, we show NP-completeness for the case of …” “We also consider …”</p> </blockquote> </li> </ol> <p><em>I really need to comment here:</em></p> <p><strong>Now I understand the suffering of reading something you truly unable to understand … (even if you searched online ** **and had a grasp about what it is, and that’s basically all you know about it).</strong> However, if given time, I still will be happy to dig deeper into this area and at least understand what this field is doing.</p> </li> <li><strong>Paper</strong> <h4 id="reasoning-about-consensus-when-opinions-diffuse-through-majority-dynamics--paper"><strong>Reasoning about Consensus when Opinions Diffuse through Majority Dynamics</strong> <a href="https://www.ijcai.org/proceedings/2018/0007.pdf">[paper]</a></h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>Opinion diffusion is</strong> studied on social graphs where agents hold binary opinions and where social pressure leads them to conform to the opinion manifested by the majority of their neighbors. <strong>Within</strong> {$\longleftarrow$ <em>1. Explain main concepts</em>} <strong>this setting</strong>, questions related to whether a minority/ majority can spread the opinion it supports to all the other agents are considered. <strong>It is shown that</strong>, no matter of the underlying graph, there is always a group formed by a half of the agents that can annihilate the opposite opinion. <strong>Instead, the influence</strong> <strong>power of minorities depends on</strong> certain features of the given graph, which are NP-hard to be identified. <strong>Deciding whether</strong> the two opinions can coexist in some stable configuration is NP-hard, too.” {$\longleftarrow$ <em>2. introduce the background</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Explain the background by a detailed example:</p> <blockquote> <p>“Consider the following prototypical scenario. …”</p> </blockquote> </li> <li> <p>Introduce the motivation of this work and previous work:</p> <blockquote> <p>“Our goal is to analyze these questions under the lens of algorithm design and computational complexity.” “Indeed, while the study of opinion diffusion, originated in [Granovetter, 1978], has …”</p> </blockquote> </li> <li> <p>Give a detailed overview of the following sections:</p> <blockquote> <p>“In this paper we fill this gap. In more details, we first …” “Moreover, we evidence in Section 4 that …” “Finally, we address the question …”</p> </blockquote> </li> </ol> </li> <li><strong>Paper</strong> <h4 id="r-svm-robust-learning-with-privileged-information--paper"><strong>R-SVM+: Robust Learning with Privileged Information</strong> <a href="https://www.ijcai.org/proceedings/2018/0334.pdf">[paper]</a></h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>In practice, the circumstance that training and test data are</strong> <strong>clean is not always satisfied.</strong> The performance of existing methods in the learning using privileged information (LUPI) paradigm may be seriously challenged, due to the lack of clear strategies {$\longleftarrow$ <em>1. Introducing the background</em>} to address potential noises in the data. <strong>This paper proposes</strong> a novel Robust SVM+ (RSVM+) algorithm based on a rigorous theoretical analysis. Under the SVM+ framework in the LUPI paradigm, we study the lower bound of perturbations of both example feature data and privileged feature data, which will mislead the model to make wrong decisions. By maximizing the lower bound, tolerance of the learned model over perturbations will be increased. Accordingly, a novel regularization function is introduced to upgrade a variant form of SVM+. The objective function of RSVM+ is transformed into a quadratic programming problem, which can be efficiently {$\longleftarrow$ <em>2. the detailed process of the proposed methods</em>} optimized using off-the-shelf solvers. <strong>Experiments</strong> on real world datasets demonstrate the necessity of studying robust SVM+ and the effectiveness of the proposed algorithm.” {$\longleftarrow$ <em>3. the gains proved by the experiments</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Introducing the background and concepts:</p> <blockquote> <p>“This auxiliary information can be widely found in human teaching and learning process. For example …” “Inspired by this fact, Vapnik and Vashist [Vapnik and Vashist, 2009] introduced the paradigm of learning using privileged information (LUPI) …” “Since this auxiliary information will not be available at the test stage, it is referred to as privileged information.” “As one of the most popular classifiers, support vector machine (SVM) was first upgraded …”</p> </blockquote> </li> <li> <p>Drawbacks of the current work:</p> <blockquote> <p>“These methods have largely advanced the developments on LUPI. However, their successes are usually achieved …”</p> </blockquote> </li> <li> <p>Contribution of this work:</p> <blockquote> <p>“In this paper, we derive a novel Robust SVM+ (R-SVM+) …” “In this way, the capability of the learned model to tolerate …” “Experimental results demonstrate …”</p> </blockquote> </li> </ol> </li> <li><strong>Paper</strong> <h4 id="sentigan-generating-sentimental-texts-via-mixture-adversarial-networks---paper"><strong>SentiGAN: Generating Sentimental Texts via Mixture Adversarial Networks</strong> <a href="https://www.ijcai.org/proceedings/2018/0618.pdf">[paper]</a></h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“Generating texts of different sentiment labels <strong>is getting</strong> <strong>more and more attention</strong> in the area of natural language generation. <strong>Recently, Generative Adversarial</strong> <strong>Net (GAN) has shown</strong> promising results in text generation. <strong>However,</strong> the texts generated by GAN usually <strong>suffer from</strong> the problems of poor quality, <strong>lack of</strong> diversity and mode collapse. In this {$\longleftarrow$ <em>1. introduce background and drawbacks</em>} paper, we propose a novel framework - SentiGAN, <strong>which has</strong> multiple generators and one multi-class discriminator, <strong>to address the above problems</strong>. <strong>In</strong> <strong>our framework</strong>, multiple generators are trained simultaneously, aiming at generating texts of different sentiment labels without supervision. <strong>We propose</strong> a penalty based objective in the generators to force each of them to generate diversified examples of a specific sentiment label. <strong>Moreover, the use of</strong> multiple generators and one multi-class discriminator can make each generator focus on generating its own examples of a specific sentiment label {$\longleftarrow$ <em>2. a detailed introduction of the method and corresponding reasons</em>} accurately. <strong>Experimental results</strong> on four datasets demonstrate that our model consistently outperforms several state-of-the-art text generation methods in the sentiment accuracy and quality of generated texts.” {$\longleftarrow$ <em>3. performance gain shown by the experiments</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Introducing backgrounds:</p> <blockquote> <p>“Unsupervised text generation is an important …” “Generative Adversarial Nets (GANs) …”</p> </blockquote> </li> <li> <p>challenges may encounter:</p> <blockquote> <p>“However, there are a few challenges to be addressed …”</p> </blockquote> </li> <li> <p>detailed work-flow of the methods and a summary:</p> <blockquote> <p>“We propose a new text generation framework …” “We use a well-performed sentiment classifier as evaluator to …” “The major contributions …”</p> </blockquote> </li> </ol> </li> <li><strong>Paper</strong> <h4 id="what-game-are-we-playing-end-to-end-learning-in-normal-and-extensive-form-games---paper"><strong>What Game Are We Playing? End-to-end Learning in Normal and Extensive Form Games</strong> <a href="https://www.ijcai.org/proceedings/2018/0055.pdf">[paper]</a></h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>Although recent work</strong> in AI has made great progress in solving large, zero-sum, extensive-form games, <strong>the underlying assumption in most past</strong> <strong>work</strong> is that the parameters of the game itself are known to the agents. <strong>This paper deals with the relatively</strong> <strong>under-explored but</strong> equally important “inverse” setting, where the parameters of the underlying game are not known to all agents, but must {$\longleftarrow$ <em>1. introduce background and drawbacks of recent work</em>} be learned through observations. <strong>We propose</strong> a differentiable, end-to-end learning framework for addressing this task. In particular, <strong>we consider</strong> a regularized version of the game, equivalent to a particular form of quantal response equilibrium, <strong>and develop</strong></p> <p>1) a primal-dual Newton method for finding such equilibrium points in both normal and extensive form games; and 2) a backpropagation method that lets us analytically compute gradients of all relevant game parameters through the solution itself. This ultimately lets us learn the game by training in an end-to-end fashion, effectively by integrating a “differentiable game solver” into the loop of larger {$\longleftarrow$ <em>2. a detailed explanation of proposed methods</em>} deep network architectures. <strong>We demonstrate</strong> the effectiveness of the learning method in several settings including poker and security game tasks.” {$\longleftarrow$ <em>3. demonstrate gains and effectiveness of the methods</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Introduce the background of the task/motivation:</p> <blockquote> <p>“Recent work in …” “However, virtually all this progress in game …” “In contrast, in many real world scenarios, …” “For example, in security games, we may want to …”</p> </blockquote> </li> <li> <p>A detailed work-flow of what we did in this paper:</p> <blockquote> <p>“In this paper, we propose …” “The crux of our approach is …” “We demonstrate the effectiveness of our approach …”</p> </blockquote> </li> </ol> </li> </ul> <h2 id="2017">2017</h2> <ul> <li><strong>Paper</strong> <h4 id="foundations-of-declarative-data-analysis-using-limit-datalog-programs---paper"><strong>Foundations of Declarative Data Analysis Using Limit Datalog Programs</strong> <a href="https://www.ijcai.org/proceedings/2017/0156.pdf">[paper]</a></h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>Motivated</strong> by applications in declarative data analysis, <strong>we study</strong> $Datalog_\mathbb{Z}$—an extension of positive Datalog with arithmetic functions over integers. This language is known to be undecidable, so we propose two fragments. In <em>limit</em> $Datalog_\mathbb{Z}$ predicates are axiomatised to keep minimal/maximal numeric values, allowing us to show that fact entailment is coNExpTime-complete in combined, and coNP-complete in data complexity. <strong>Moreover,</strong> an additional <em>stability</em> requirement causes the complexity to drop to ExpTime and PTime, respectively. <strong>Finally,</strong> we show that stable $Datalog_\mathbb{Z}$ can express many useful data analysis tasks, and so our results provide a sound foundation for the development of advanced information systems.” {$\longleftarrow$ <em>1. the motivation and main work as well as gains are shown in sequential order</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Background introduction:</p> <blockquote> <p>“Analysing complex datasets is currently a hot topic …” “It has recently been argued that data analysis should be <em>declarative</em> …” “An essential ingredient of declarative data analysis is …” “This extensive body of work, however, focuses primarily …”</p> </blockquote> </li> <li> <p>General introduction to the work and a summary over the contributions:</p> <blockquote> <p>“To develop a sound foundation for …, we study …” “In limit $Datalog_\mathbb{Z}$, all intensional predicates with …” “We provide a direct semantics for …” “Our contributions are as follows.”</p> </blockquote> </li> </ol> </li> </ul> <h2 id="2016">2016</h2> <ul> <li><strong>Paper</strong> <h4 id="hierarchical-finite-state-controllers-for-generalized-planning-corrected-version---paper"><strong>Hierarchical Finite State Controllers for Generalized Planning (Corrected Version)</strong> <a href="https://arxiv.org/pdf/1911.02887.pdf">[paper]</a></h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>Finite State Controllers (FSCs) are</strong> an effective way to represent sequential plans compactly. By imposing appropriate conditions on transitions, <strong>FSCs</strong> <strong>can also represent</strong> generalized plans that solve a range of planning problems from a given domain. {$\longleftarrow$ <em>1. Introducing main concepts</em>} <strong>In this paper</strong> we introduce the concept of hierarchical FSCs for planning by allowing controllers to call other controllers. <strong>We show that</strong> hierarchical FSCs can represent generalized plans more compactly than individual FSCs. <strong>Moreover,</strong> our call mechanism makes it possible to generate hierarchical FSCs in a modular fashion, or even to apply recursion. <strong>We also introduce</strong> a compilation that enables a classical planner to generate hierarchical FSCs that solve challenging generalized planning problems. The compilation takes as input a set of planning problems from a given domain and outputs a single classical planning problem, whose solution corresponds to a hierarchical FSC.” {$\longleftarrow$ <em>2. a detailed illustration of what this work has done</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Introducing backgrounds:</p> <blockquote> <p>“Finite state controllers (FSCs) are …” “Even FSCs have limitations, however. Consider …”<br/> <strong>(and a graphic example was given here to help understand the drawbacks)</strong></p> </blockquote> </li> <li> <p>Introducing our work (and reasonable ideas/motivations behind it) and summarize the efforts</p> <blockquote> <p>“In this paper we introduce a novel formalism for …” “To illustrate this idea, Figure 2 shows an example hierarchical FSC …”<br/> <strong>(another graphic example was given here to help understand the idea)</strong> “Intuitively, by repeatedly …” “Compared to previous work on the automatic generation of FSCs for planning the contributions of this paper are:”</p> </blockquote> </li> </ol> </li> </ul> <h2 id="2015">2015</h2> <ul> <li><strong>Paper</strong> <h4 id="bayesian-active-learning-for-posterior-estimation---paper"><strong>Bayesian Active Learning for Posterior Estimation</strong> <a href="https://www.cs.cmu.edu/~schneide/kandasamyIJCAI15activePostEst.pdf">[paper]</a></h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>This paper studies</strong> active posterior estimation in a Bayesian setting when the likelihood is expensive to evaluate. <strong>Existing techniques</strong> for posterior estimation are based on generating samples representative of the posterior. <strong>Such methods do not</strong> consider efficiency in terms of likelihood evaluations. <strong>In order to</strong> be query efficient we treat posterior estimation {$\longleftarrow$ <em>1. background introduction and drawbacks of existing methods / motivations</em>} in an active regression framework. <strong>We propose</strong> <br/> two myopic query strategies to choose where to evaluate the likelihood and implement them using {$\longleftarrow$ <em>2. main work of this paper</em>} Gaussian processes. <strong>Via experiments</strong> on a series of synthetic and real examples <strong>we demonstrate</strong> <strong>that our approach is significantly more</strong> query efficient than existing techniques and other heuristics for posterior estimation.” {$\longleftarrow$ <em>3. gains shown by experiments</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Background introduction:</p> <blockquote> <p>“Computing the posterior distribution of parameters given observations is a central problem in statistics.” “In some cases, we only have access to …”</p> </blockquote> </li> <li> <p>Goal/motivation of the paper:</p> <blockquote> <p>“Our goal is an efficient way to estimate …” “Given observations, we wish to make inferences about …”</p> </blockquote> </li> <li> <p>Contribution (and it’s actually bold-lined in the paper as a subtitle):</p> <blockquote> <p>“<strong>Our contribution</strong> is to propose …” “We propose two myopic query strategies on …” “and we demonstrate the efficacy …”</p> </blockquote> </li> </ol> </li> <li><strong>Paper</strong> <h4 id="recursive-decomposition-for-nonconvex-optimization---paper"><strong>Recursive Decomposition for Nonconvex Optimization</strong> <a href="https://arxiv.org/pdf/1611.02755.pdf">[paper]</a></h4> <p> </p> </li> <li><strong>Abstract</strong></li> </ul> <p>“<strong>Continuous optimization is</strong> an important problem in many areas of AI, including vision, robotics, probabilistic inference, and machine learning. <strong>Unfortunately,</strong> <strong>most</strong> real-world optimization problems are nonconvex, causing standard convex techniques to find only local optima, <strong>even with</strong> extensions like random restarts and simulated annealing. We observe that, <strong>in many cases</strong>, the local modes of the objective function have combinatorial structure, and thus ideas from combinatorial optimization can {$\longleftarrow$ <em>1. the background (drawbacks thus motivations)</em>} be brought to bear. <strong>Based on this</strong>, we propose a problem-decomposition approach to nonconvex optimization. Similarly to DPLL-style SAT solvers and recursive conditioning in probabilistic inference, <strong>our algorithm, RDIS, recursively</strong> sets variables so as to simplify and decompose the objective function into approximately independent subfunctions, until the remaining functions are simple enough to be optimized by standard techniques like gradient descent. <strong>The variables to set are chosen by</strong> graph partitioning, ensuring decomposition whenever {$\longleftarrow$ <em>2. A detailed introduction of the method</em>} possible. <strong>We show analytically</strong> that RDIS can solve a broad class of nonconvex optimization problems exponentially faster than gradient descent with random restarts. <strong>Experimentally,</strong> RDIS outperforms standard techniques on problems like structure from motion and protein folding.” {$\longleftarrow$ <em>3. gains shown by the experiments</em>}</p> <ul> <li> <p><strong>Introduction</strong></p> <ol> <li> <p>Background:</p> <blockquote> <p>“AI systems that interact with the real world often have to solve …” “However, most continuous optimization problems in …”</p> </blockquote> </li> <li> <p>Methods and challenges to do this work:</p> <blockquote> <p>“In this paper we propose that …” “We thus propose a novel nonconvex optimization algorithm, which …” “The main challenges in applying …” “For example, consider …”</p> </blockquote> </li> <li> <p>Contributions:</p> <blockquote> <p>“We first define local structure and then present our algorithm, …” “In our analysis, we show …”</p> </blockquote> </li> </ol> </li> </ul> <h2 id="summary">Summary:</h2> <p>After analyzing <strong>Abstract / Introduction</strong> of the best paper award winner from 2015 ~ 2020, considering the:</p> <p><strong>structure</strong> (how / in what order the story is told, e.g., background, motivation, etc.) <strong>relations</strong> (how the corresponding part in Abstract and Introduction interacts with each other, e.g., callbacks.) <strong>writing skills</strong> (when and where and in what form an example should be provided)</p> <p>We epitomize the common patterns shown in these papers and form a writing guidance:</p> <p>For <strong>Abstract</strong>:</p> <p>​ functionality of the <strong>Abstract</strong>:</p> <p>​ Give a brief view of your work, including:</p> \[\begin{cases} \text{$\textbf{Introduction of your task / motivation}$ (some concepts, drawbacks)}\\ \text{$\textbf{A general view of how your work is done}$ (your key methods and why you do it in that way)}\\ \text{$\textbf{What your experiments told you}$ (usually stand as a proof for the efficiency of your work)} \end{cases}\] <p>​ <strong>note that it’s better you don’t insert any pictures here, the job is to explain your work in simplest words ​ without introducing any new concepts, thus some terms can be used here, and the confusions will be ​ solved in ‘Introduction’</strong></p> <ol> <li> <p>Explain the background of your work (<strong>basic concepts</strong>, <strong>drawbacks</strong> which later implicitly serve as the motivation of your work)</p> <blockquote> <p>e.g.: xxx is … (give a brief view of what you’re doing here), however, xxx is … (explain drawbacks of the existing methods)</p> <p><strong>note that the twist (‘however’) is very important</strong></p> </blockquote> </li> <li> <p>A general introduction of the work (you can go with the sequential style or logical style etc.)</p> <blockquote> <p>For sequential style: (in this style you basically are following the architecture of your work)</p> <p>​ e.g.: we … (what you did first), then … (what you did based on your first step)</p> <p>For logical style: (in this style you basically are following the logic of how you solve problem)</p> <p>​ e.g.: we … to … (the first advancement you’re making), for …. (another advancement you’re making)</p> <p>​ e.g.: we … to … (the first advancement you’re making), however …, (new problems raised by the first advancement)</p> <p>​ thus …. (another advancement you’re making to solve the problem)</p> <p>And of course you always can <strong>write in a way you feel the story is told straight</strong></p> <p><strong>note that the reason / goal (to …) of what you’re doing is very important</strong></p> </blockquote> </li> <li> <p>Show and summarize the experimental results</p> <blockquote> <p>e.g.: we demonstrate that …(on what your experimented), experiment results show our … can … (superiority of your work)</p> </blockquote> </li> </ol> <p>For <strong>Introduction</strong>:</p> <p>​ functionality of the <strong>Introduction</strong>:</p> <p>​ A detailed ‘<strong>Abstract</strong>’, which explains in details of what you did, in some sense, the abstract is like the guidance of the ​ <strong>Introduction</strong>, where general aspects are introduced in <strong>Abstract</strong>, and a more detailed explanation of your work is done ​ in <strong>Introduction</strong></p> <ol> <li> <p>Background introduction: (the <strong>history</strong> of the subfield of this work, <strong>drawbacks</strong>, and an <strong>illustration</strong> (graphic / textual) to better understand the task / problem)</p> <blockquote> <p>e.g.: … has been an important … (general introduction of your field), … in … proposed …(origin of your field here), recently … (some advancement here)</p> </blockquote> </li> <li> <p><strong>Drawbacks</strong> of the previous work and thus our <strong>proposal</strong> (how it’s down more specifically) as well as <strong>its challenges</strong>:</p> <blockquote> <p>e.g.: …, however …, to solve the problem …, we … (here you can (not necessity) follow the introductory style used in your <strong>Abstract</strong>)</p> <p><strong>note that explaining challenges and how the proposals solve them respectively is very important</strong></p> </blockquote> </li> <li> <p>How your experiments are carried and a summary over your main contribution:</p> <blockquote> <p>e.g.: we show … that … .</p> <p>​ Our main contributions are as follows: (it’s better to write this in a new line)</p> <p><strong>note that the summarization of your work should be short and overall</strong></p> </blockquote> </li> </ol>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[paper summary]]></summary></entry><entry><title type="html">Paper Summary 1</title><link href="https://a-chicharito-s.github.io/blog/2021/paper-summary-1/" rel="alternate" type="text/html" title="Paper Summary 1"/><published>2021-11-08T16:40:16+00:00</published><updated>2021-11-08T16:40:16+00:00</updated><id>https://a-chicharito-s.github.io/blog/2021/paper-summary-1</id><content type="html" xml:base="https://a-chicharito-s.github.io/blog/2021/paper-summary-1/"><![CDATA[<head> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script> </head> <ul> <li><strong>Paper</strong>:<a name="1"></a></li> </ul> <h3 id="label-specific-dual-graph-neural-network-for-multi-label-text-classification---paper"><strong>Label-Specific Dual Graph Neural Network for Multi-Label Text Classification</strong> <a href="https://aclanthology.org/2021.acl-long.298/">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong>:</li> </ul> <p>​ Label-specific semantic information is ignored to help distinguish similar classes when doing multi-label text classification</p> <ul> <li><strong>Solution</strong>:</li> </ul> <p>​ Used two GCNs, <strong>one</strong> encodes the information using the representation from the previous layer (which is an attention mechanism ​ with randomly initialized label information as part of the attention score over the outputs from a BiLSTM), the adjacency matrix is a ​ statistical matrix whose element A<sub>ij</sub> is the conditional probability of a sample belonging to class<sub>i</sub> while it belongs to class<sub>j</sub> .</p> <p>​ The <strong>other</strong> takes the output of the previous GCN and then construct the adjacency matrix dynamically</p> <p>​ Explanations are: ​ (1). the first GCN encoded label information with the training sentences. ​ (2). however the adjacency matrix for the first GCN is a statistical co-occurrence matrix, which may result in long-tail distribution ​ problem, thus the second GCN dynamically construct its adjacency matrix to capture interactive relations between components.</p> <p>​ <strong>Highlights</strong></p> <p>​ Personally I think the shining point of this work lies in its second GCN <strong>where the adjacency matrix is dynamically</strong> ​ <strong>constructed</strong>, which in some kind of sense enables the GCN to <strong>capture hidden interactions</strong> and thus better help the features from ​ different nodes to merge.</p> <ul> <li><strong>Architecture</strong>:</li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_1/pic1.png?token=AOW4OJW5XAOAWIR77BROPNTBQ7EBI" style="zoom:70%;"/></div> <ul> <li><strong>Paper</strong></li> </ul> <h3 id="concept-based-label-embedding-via-dynamic-routing-for-hierarchical-text-classification---paper"><strong>Concept-Based Label Embedding via Dynamic Routing for Hierarchical Text Classification</strong> <a href="https://aclanthology.org/2021.acl-long.388.pdf">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong></li> </ul> <p>​ The concepts can be used to classify different sub-labels within a same class in Hierarchical Text Classification and previous works ​ focus on how to learn better text representations or simply use label information however ignored ‘concept’. (in the following graph ​ they’re ‘design’ and ‘distributed’, <strong>which are not included in the origin hierarchy</strong>)</p> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_1/pic2.png?token=AOW4OJQZU2WVPH55XUX6BF3BQ7EBS"/></div> <ul> <li><strong>Solution</strong></li> </ul> <p>​ Text is encoded and passed to stacked blocks called CCM (Concept-based Classifier Module), and a <strong>CCM</strong> module is consisted of ​ a EK Encoder (to encode external knowledge), <strong>predicted soft label embedding</strong> from the previous CCM (which will be used to help ​ classification at current hierarchy), and a <strong>CSM</strong> module — where the concept is shared via Dynamic Routing (whose pseudo code is ​ very similar to Capsule Network). And at each CCM module, predictions for current hierarchy is outputted.</p> <p>​ <strong>Highlights</strong></p> <p>​ I think the good part of this work is its idea of <strong>using stacked modules</strong>, which is a common architecture in popular models like ​ Transformer or ResNet, and the interactions from the previous module to the next one may enable the model <strong>to extract different</strong> ​ <strong>features at different depth</strong> (which is some kind of sense naturally suits the characteristics of text classification with a hierarchical ​ structure)</p> <ul> <li><strong>Architecture</strong></li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_1/pic3.png?token=AOW4OJTZFW2JHH5G5BZ2GTTBQ7ECA" style="zoom:90%;"/></div> <ul> <li><strong>Paper</strong></li> </ul> <h3 id="explicit-interaction-model-towards-text-classification---paper-"><strong>Explicit Interaction Model towards Text Classification</strong> <a href="https://arxiv.org/abs/1811.09386#">[paper]</a> <a name="2"></a></h3> <ul> <li><strong>Background and motivation</strong></li> </ul> <p>​ The interaction between words and classes is ignored, and the motivation is to explicitly model this kind of interaction</p> <ul> <li><strong>Solution</strong></li> </ul> <p>​ A transformation is performed on word-level embeddings where a matrix is acted as the role of label information</p> <p>​ <strong>Highlights</strong></p> <p>​ The only shining point might only be <strong>its motivation of introducing interactions between classes and texts at word level</strong>, ​ however the way of constructing this kind of interaction is rather casual — simply doing a transformation with the encoded words ​ with a randomly initialized matrix, without introducing some statistical information or some similarity measurements (e.g. cos ​ similarity) while maintaining its dynamic (e.g. in <a href="#1">Label-Specific Dual Graph Neural Network for Multi-Label Text Classification</a>)</p> <ul> <li><strong>Architecture</strong></li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_1/pic4.png?token=AOW4OJTAFQ2OAVGACNQ4IE3BQ7ECU"/></div> <ul> <li><strong>Paper</strong></li> </ul> <h3 id="joint-embedding-of--words-and-labels--for-text--classification--paper-"><strong>Joint Embedding of Words and Labels for Text Classification</strong> <a href="https://aclanthology.org/P18-1216.pdf">[paper]</a> <a name="3"></a></h3> <ul> <li><strong>Background and motivation</strong></li> </ul> <p>​ Embeds both the text and label in the same word embedding space and carries out an attention operation over and text and label to ​ attend to relevant words.</p> <ul> <li><strong>Solution</strong></li> </ul> <p>​ By embedding texts and labels in the same space, a cos similarity matrix is calculated between the text and label, the matrix is then ​ converted to attention score to aggregate the text words to a universal representation used for classification.</p> <p>​ <strong>Highlights</strong></p> <p>​ The idea and model are simple, however it shows good structure as well as more convincing way of leveraging label information ​ (<strong>used cos similarity</strong> unlike in <a href="#2">Explicit Interaction Model towards Text Classification</a> where label representation is randomly ​ initialized)</p> <ul> <li><strong>Architecture</strong></li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_1/pic5.png?token=AOW4OJSMJ2C5LHQPTMXHP5LBQ7EDG"/></div> <ul> <li><strong>Paper</strong></li> </ul> <h3 id="multi-task-label-embedding-for-text-classification---paper"><strong>Multi-Task Label Embedding for Text Classification</strong> <a href="https://arxiv.org/abs/1710.07210#">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong></li> </ul> <p>​ Previous works ignored label information thus result in semantic label information loss, this work leverages semantic label ​ information and shows promising performance on task transferring.</p> <ul> <li><strong>Solution</strong></li> </ul> <p>​ Texts and respective labels are grouped at task level and then embedded, for the <strong>Input Encoder</strong>, words in the text are first ​ converted to word embeddings (<strong>embedding layer</strong>) then feed into a BiLSTM, the concatenation of the outputs from the BiLSTM at ​ last time step is treated as the final representation for the texts (<strong>learning layer</strong>). For the <strong>Label Encoder</strong>, words in the label are first ​ converted to word embeddings (<strong>embedding layer</strong>) then their average is treated as the final representation for labels (<strong>learning ​ layer</strong>). Matching probability is calculated using a MLP for each task, the training loss is Cross Entropy with weights for each task.</p> <p>​ For task transfer, there are three ways of updating the tasks — Hot Update, Cold Update, Zero Update</p> <p>​ <strong>Hot Update</strong>: when a task C <strong>with label annotations</strong> is added, using <strong>only newly added task C</strong> to keep training the trained model</p> <p>​ <strong>Cold Update</strong>: when a task C <strong>with label annotations</strong> is added, using <strong>all the available tasks</strong> (previous+task C) to train the model ​ from start.</p> <p>​ <strong>Zero Update</strong>: when a task D <strong>without label annotation</strong> is added, applying the newly added task D on the already trained model to ​ see its transfer ability.</p> <p>​ Note: task <strong>with</strong> label annotation means there are {text, label} pairs you know corresponding label for a text (<strong>able to train</strong>), ​ <strong>without</strong> means there isn’t such pair (<strong>unable to train, only able to test</strong>)</p> <p>​ <strong>Highlights</strong></p> <p>​ The actually structure of the model is not very fancy (e.g.: the Matcher is simply a MLP) however the core idea of this work is ​ amazing, which re-thinks training at task level, with label information and weighted loss (w.r.t tasks), the way how it does task ​ transfer (Hot Update, Cold Update, Zero Update) is inspiring as well</p> <ul> <li><strong>Architecture</strong></li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_1/pic6.png?token=AOW4OJXM5MY6H573N2FV7WTBQ7EDU"/></div> <ul> <li><strong>Paper</strong></li> </ul> <h3 id="distinct-label-representations-for-few-shot-text-classification----paper"><strong>Distinct Label Representations for Few-Shot Text Classification</strong> <a href="https://aclanthology.org/2021.acl-short.105.pdf">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong></li> </ul> <p>​ Instances from semantically close classes makes it hard to do few-shot classification under meta learning settings, thus label ​ information is introduced to help classification</p> <ul> <li><strong>Solution</strong></li> </ul> <p>​ A multi-head attention is performed over the embedded texts and labels to attend to different features in texts (Difference Extractor), ​ the outputs is later used for classification and mutual information between labels is calculated and minimized as an extra loss.</p> <p>​ <strong>Highlights</strong></p> <p>​ The introduction of multi-head is good, and I reckon the mutual information loss is quite interesting (though I did not understand it ​ at all:disappointed:, however I hope to dig into it when I have some time)</p> <ul> <li><strong>Architecture</strong></li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_1/pic7.png?token=AOW4OJSZXFWIVKX7PU42UKLBQ7EEC" style="zoom:75%;"/></div> <p>​</p> <ul> <li><strong>Paper</strong></li> </ul> <h3 id="dont-miss-the-labels-label-semantic-augmented-meta-learner-for-few-shot-text-classification---paper"><strong>Don’t Miss the Labels: Label-semantic Augmented Meta-Learner for Few-Shot Text Classification</strong> <a href="https://aclanthology.org/2021.findings-acl.245.pdf">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong></li> </ul> <p>​ Previous work mainly focus on building a meta-learner using only information from texts, thus the information from label is wasted, ​ to solve this drawback, this work boosts meta-learner with label semantics</p> <ul> <li><strong>Solution</strong></li> </ul> <p>​ When training, the corresponding label is concatenated after the sentence and the next sentence prediction task of BERT is used to ​ try to mix label information into the text representation, and the support representation can be computed in the following ways: <strong>1.</strong> ​ [CLS]; <strong>2.</strong> average words’ embeddings in the sentence; <strong>3.</strong> average labels’ embeddings in the sentence; ​ For query, there are two inputting options: <strong>1.</strong> origin text; <strong>2.</strong> origin text concatenates all the labels; and for output representations ​ for queries used for classification, there are three options: <strong>1.</strong> [CLS] from the origin text; <strong>2.</strong> [CLS] from the origin text concatenated ​ with all the labels; <strong>3.</strong> all label embeddings from the origin text concatenated with all labels;</p> <p>​ <strong>Highlights</strong></p> <p>​ The gains may all comes from BERT instead of its proposed modifications, if encoded changed, its performance may not be ​ guaranteed.</p> <ul> <li><strong>Architecture</strong></li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_1/pic8.png?token=AOW4OJTBG357IBCCVK45V7DBQ7EEW" style="zoom:65%;"/></div> <ul> <li><strong>Paper</strong></li> </ul> <h3 id="learning-to-bridge-metric-spaces-few-shot-joint-learning-of-intent-detection-and-slot-filling----paper"><strong>Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent Detection and Slot Filling</strong> <a href="https://arxiv.org/abs/2106.07343">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong></li> </ul> <p>​ The authors consider learning intent detection and slot filling together can benefit both tasks, thus propose to learn both tasks ​ simultaneously using prototype network under a few shot learning setting, contrastive learning is used as well to help learn better ​ representations of the prototypes.</p> <ul> <li><strong>Solution</strong></li> </ul> <p>​ Since there are two tasks training together, the prototype is merged for later classification, in detail: <strong>1.</strong> a cross-attention score is ​ estimated using additive attention with raw prototypes (the average of the encoded sentences) from intent detection and slot filling, ​ <strong>2.</strong> and fused prototypes for two tasks are calculated using the cross-attention score over the raw prototypes, <strong>3.</strong> the fused and raw ​ prototypes of each task are then merged with weight of { $\alpha$ , 1 - $\alpha$ }, which will later be the final prototypes for classification of intent ​ detection/slot filling. ​ For Contrastive Learning, the loss is composed of two parts, one is <strong>Intra Contrastive Loss</strong> (where the loss is modified with margin ​ and calculated task-wise, the losses of two tasks are averaged as the final Intra Contrastive Loss), the other is <strong>Inter Contrastive ​ Loss</strong> (which deals with interactions of prototypes between intent detection and slot filling, and loss $\mathcal{L}^\mathcal{R}_\mathcal{i}$ of the set of related slots to the i-th intent is calculated, and loss $\mathcal{L}^\mathcal{U}_\mathcal{i}$ of that except with unrelated slots, the two terms is summed as the final Inter Contrastive ​ loss), <strong>Final Contrastive Loss</strong> is the sum of Intra and Inter Contrastive Loss</p> <p>​ <strong>Highlights</strong></p> <p>​ The Intra and Inter Contrastive Loss is quite fresh and suits the initiative of bridging the two metric spaces, however, neither ​ the modified Contrastive Loss nor the Merged prototypes actually make it fit really well to the motivation of ‘Bridging’, and of course I ​ was hoping to see more dissimilar tasks can be tackled using the idea of ‘Bridging the Metric Space’, this work definitely could have ​ done much better by digging deeper.</p> <ul> <li><strong>Architecture</strong></li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_1/pic9.png?token=AOW4OJQKXG2LOS6JAEAO2CTBQ7EFC"/></div> <ul> <li><strong>Paper</strong></li> </ul> <h3 id="can-constrained-attention-networks-for-multi-aspect-sentiment-analysis---paper"><strong>CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis</strong> <a href="https://aclanthology.org/D19-1467.pdf">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong></li> </ul> <p>​ For multi-aspect sentiment analysis, the traditional attention on sentiment words for different aspects tends to be mixed up thus ​ introducing noise for classification. In this work, the author divide sentences into two categories: <strong>overlapping</strong> and <strong>non-</strong> ​ <strong>overlapping</strong>. ​ <strong>Overlapping</strong> means sentiment words for different aspects may overlap, e.g.: ‘<em>the food and service are good there.</em>’ where the ​ sentiment word for ‘<em>food</em>’ and ‘<em>service</em>’ is both ‘<em>good</em>’, <strong>non-overlapping</strong> means sentiment words for different aspects aren’t ​ overlapped, e.g.: ‘<em>I like the food and the service is OK as well.</em>’ where the sentiment word for ‘<em>food</em>’ is ‘<em>like</em>’ and for ‘<em>service</em>’ is ‘<em>OK</em>’. ​ And this work only tackles the mixed attention problem for the non-overlapping sentences, with the assumption of attentions for ​ different sentiment words should be sparse, there are two interesting regularizers are proposed: 1. <strong>Sparse Regularizer</strong>; 2. ​ <strong>Orthogonal Regularizer</strong> to help attend to different words for different aspects in non-overlapping sentences.</p> <ul> <li><strong>Solution</strong></li> </ul> <p>​ For Sparse Regularizer, the regularization term is: $\mathcal{R}_\mathcal{s}$= $\vert \sum_{l=1}^L\alpha_{kl}^2-1\vert$, where $\sum_{l=1}^L\alpha_{kl}=1$ whose elements are all positive ​ (attention score), by minimizing this regularizer, the elements will tend to a distribution like: {0, 0, …, 1, 0, …, 0} with the sentiment ​ word having the largest attention score 1 and other words as 0. For Orthogonal Regularizer, $\mathcal{R}_\mathcal{o}=|\mathcal{M}^T\mathcal{M}-\mathcal{I}|_2$, where ​ $\mathcal{M}\in\mathbb{R}^{\mathcal{k}\times\mathcal{L}}$, whose first dimension stands for K aspects in one sentence, second dimension stands for the length L for that ​ sentence, in that sense, each row of the matrix is actually attention scores for one aspect for the sentence, by minimizing this ​ regularizer, the attention score for different aspects tend to be close to 0 (since they are not on the diagonal of $\mathcal{M}^T\mathcal{M}$), thus ​ producing orthogonality for different aspect attention scores.</p> <p>​ <strong>Highlights</strong></p> <p>​ I personally like very much <strong>the idea of making attention score sparse to attend to different sentiment words</strong> by the ​ proposed two regularizers, however if observe carefully, the Orthogonal Regularizer actually did the job for the Sparse Regularizer ​ on its diagonal elements, so the Sparse Regularizer is some kind of redundant, which is also verified by the ablation experiments ​ where the Orthogonal Regularizer performs better than the Sparse Regularizer.</p> <ul> <li><strong>Architecture</strong></li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_1/pic10.png?token=AOW4OJW5M3ED4T4TWZWYSULBQ7EFU"/></div> <ul> <li><strong>Paper</strong></li> </ul> <h3 id="contrast-learning-visual-attention-for-multi-label-classification---paper"><strong>Contrast Learning Visual Attention for Multi Label Classification</strong> <a href="https://arxiv.org/pdf/2107.11626.pdf">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong></li> </ul> <p>​ Previous image classification work simply apply contrastive learning to multi-label image classification which may only achieve OK ​ results. By using label information to help the model to focus on different semantic components of the image, the proposed ​ framework achieved better performance on multi-label image classification.</p> <ul> <li><strong>Solution</strong></li> </ul> <p>​ The image is firstly encoded with CNN-based model (e.g.: ResNet) and then later the encoded features of the image are later ​ together with a randomly initialized class-embedding matrix $\mathcal{U}\in\mathbb{R}^{\mathcal{L}\times\mathcal{C}}$ whose raw is the embedding for one class. Later the matrix ​ is used as Query, and the encoded features are treated as Key and Value for a Multi-Attention block, the outputs from the Multi- ​ Attention Blocks are then later used for classification. The framework adopts a two stage training, in stage one, the model is trained ​ with only binary cross entropy loss to make the model to learn a task specified $\mathcal{U}$, in stage two, a contrastive loss is added to help ​ fine-tune the class-embedding matrix $\mathcal{U}$ to achieve promising results.</p> <p>​ <strong>Highlights</strong></p> <p>​ The randomly initialized class-embedding matrix is OK however <strong>the performance of one task may be closely related to how the weights are initialized</strong> (e.g.: ‘RandomUniform’ or ‘RandomNormal’) , thus this framework may not be very robust for different ​ image classification datasets, like how I mentioned in the <strong>Highlights</strong> of <a href="#3"><strong>Joint Embedding of Words and Labels for Text ​ Classification</strong></a>. ​ The usage of simply a cos similarity of something distance/similarity alike calculation for the initialization of the class-embedding ​ matrix $\mathcal{U}$ will be definitely more convincing. (<strong>Though this work used train + fine-tune to overcome the possible instability ​ caused by the initialization of the class-embedding matrix $\mathcal{U}$, still, not a very optimal way in my opinion.</strong>)</p> <ul> <li><strong>Architecture</strong></li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_1/pic11.png?token=AOW4OJUGLPNULTYUMZP3HD3BQ7EGG" style="zoom:75%;"/></div> <ul> <li><strong>Paper</strong></li> </ul> <h3 id="inconsistent-few-shot-relation-classification-via-cross-attentional-prototype-networks-with-contrastive-learning----paper"><strong>Inconsistent Few-Shot Relation Classification via Cross-Attentional Prototype Networks with Contrastive Learning</strong> <a href="https://arxiv.org/abs/2110.08254">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong></li> </ul> <p>​ For standard Few-Shot learning, when training/testing, it always sticks to the setting of N-way K-shot, however in real life scenario, ​ it’s hard to keep the N and K invariable, it’s more likely that the N and K for two different meta-tasks ( during training/testing) will be ​ different as well. Under the inconsistent scenario, the author proposed a prototype based network, together with cross-attention as ​ well as contrastive learning to tackle the inconsistent N/K problem for Few-Shot Learning.</p> <ul> <li><strong>Solution</strong></li> </ul> <p>​ To tackle the inconsistent N/K problem, a cross-attention is performed to learning better prototype representations (since N/K is ​ inconsistent, for some meta-tasks it will be harder for the meta-learner to learn good prototypes with more classes and less ​ examples). For attention <em>Support</em> $\rightarrow$ <em>Query</em>, a distribution $\mathcal{d}^\mathcal{i}_\mathcal{r}$ is calculated for every support instance $\mathcal{s}^\mathcal{i}_\mathcal{r}$ with all the query ​ instances, and a distance loss $\mathcal{L}_\mathcal{dist}$ is minimized when the closer the intra-class instances and the further the inter-class instances ​ are, the smaller the loss is. For attention <em>Query</em> $\rightarrow$ <em>Support</em>, an attention mechanism similar to xxx(links here) is calculated to ​ weight the support instance $\mathcal{s}^\mathcal{i}_\mathcal{r}$. With the main Cross-Entropy loss and the distance loss, a contrastive loss is calculated over $\mathcal{s}^\mathcal{i}_\mathcal{r}$ where distance between two instances from the same class will be small while that of different classes will be large.</p> <p>​ <strong>Highlights</strong></p> <p>​ <strong>The setting of inconsistent N/K</strong> is definitely the core idea of the paper, however, in my eyes, <strong>how it solved this problem is ​ controversial</strong>, since it used the information from the query to do attention on support set to get the prototype, which can be viewed ​ as the examples for the support set are implicitly increased (query information is involved, in that sense, I think it should be carefully ​ judged the way how <a href="#4">Hybrid Attention-Based Prototypical Networks for Noisy Few-Shot Relation Classification</a> did support set ​ attention as well), and when comparing, the model is directly compared to prototype-based networks <strong>without</strong> doing cross attention ​ and only achieved a few performance improvements (thus hard to define whether this architecture is really good at solving the ​ inconsistent problem or the prototype-based model with cross-attention can still perform well), also, <strong>the experiments are ​ incomplete</strong>, where for inconsistent N/K settings, the K/N did not cover a reasonable range. (e.g.: when K for training are 5, 10, 20, ​ K for testing are 1, 5, 10, 20, didn’t experiment with the setting of K for testing greater than 20)</p> <ul> <li><strong>Architecture</strong></li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_1/pic12.png?token=AOW4OJSKLNQWQ73I2QDI3A3BQ7EGU" style="zoom:75%;"/></div> <ul> <li><strong>Paper</strong></li> </ul> <h3 id="supervised-contrastive-learning----paper----favorite-paper-in-this-summary"><strong>Supervised Contrastive Learning</strong>* <a href="https://arxiv.org/abs/2004.11362">[paper]</a> <em><u>(favorite paper in this summary)</u></em></h3> <ul> <li><strong>Background and motivation</strong></li> </ul> <p>​ The supervised contrastive learning actually makes use of the class information to modify the previous self-supervised contrastive ​ learning, with the intuition that given instance labels, those of the same label should be closer to each other.</p> <p>​ <strong>Highlights</strong></p> <p>​ The usage of label information, <strong>simple yet efficient</strong> ! And the proof in the Supplementary is excellent as well.</p> <ul> <li><strong>Solution</strong></li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_1/pic13.png?token=AOW4OJQOJPOZ46RBQNRKFC3BQ7ER2" style="zoom:75%;"/></div> <ul> <li><strong>Paper</strong></li> </ul> <h3 id="multi-label-few-shot-learning-for-aspect-category-detection---paper"><strong>Multi-Label Few-Shot Learning for Aspect Category Detection</strong> <a href="https://arxiv.org/abs/2105.14174">[paper]</a></h3> <ul> <li><strong>Background and motivation</strong></li> </ul> <p>​ A new dataset in Multi-Aspect Category Detection for Few-Shot learning is released in this work, and <strong>to alleviate noise meanwhile learn expressive and distinct representation to do classification</strong>, an attention based model is proposed.</p> <ul> <li><strong>Solution</strong></li> </ul> <p>​ Attention is done on support set texts as well as on query sentences, which is with the function of reducing noise thus help to learn ​ a better prototype representation/ a clearer query representation, Euclidean distance is computed between prototypes and query ​ representations to output the final classification probability.</p> <p>​ <strong>Highlights</strong></p> <p>​ The idea of reducing noise on support and query set is quite cool, and the release of a new dataset is a good supplementary ​ as well, however the way the author does attention on support and query set is inconsistent and a little bit weird.</p> <ul> <li><strong>Architecture</strong></li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_1/pic14.png?token=AOW4OJUJ77XBSGUEI7P2B2LBQ7EIO" style="zoom:85%;"/></div> <ul> <li><strong>Paper</strong></li> </ul> <h3 id="hybrid-attention-based-prototypical-networks-for-noisy-few-shot-relation-classification---paper"><strong>Hybrid Attention-Based Prototypical Networks for Noisy Few-Shot Relation Classification</strong> <a href="https://ojs.aaai.org//index.php/AAAI/article/view/4604">[paper]</a><a name="4"></a></h3> <ul> <li><strong>Background and motivation</strong></li> </ul> <p>​ Previous work of Relation Classification relies on distant supervision (e.g.: the usage of database), this work treat the task as Few- ​ Shot Learning problem and uses attention to produce <strong>noise-proof</strong> hybrid prototypes to do classification.</p> <ul> <li><strong>Solution</strong></li> </ul> <p>​ Two different attention mechanisms (Instance-level attention and Feature-level attention) are used to reduce noise, for Instance- ​ level attention, query texts are used to help calculate attention weights over the support sentences to get the prototypes, and for ​ Feature-level attention, a scoring vector $\mathcal{z}_i$ for each class is computed using K sentences of that class. Later the distance between ​ the encoded query texts and the prototypes is computed with the help of the scoring vector $\mathcal{z}_i$.</p> <p>​ <strong>Highlights</strong></p> <p>​ The idea of applying Few-Shot learning to do relation classification is stunning, however the way support attention ​ $\alpha=softmax(\mathcal{e})$ (where $\mathcal{e}_j=sum{tanh(g(x^j_i)\odot g(x))}$, $x^j_i$ is the encoded support sentence and $x$ stands for all the encoded ​ query texts) is calculated has introduced extra query information to help the support set to learn a better prototype, and I personally ​ do not recommend doing support set’s attention with query information.</p> <ul> <li><strong>Architecture</strong></li> </ul> <div align="center"><img src="https://raw.githubusercontent.com/A-Chicharito-S/img/paper_summary_1/pic15.png?token=AOW4OJQ5GJ27U47UB37WEVLBQ7EJE" style="zoom:75%;"/></div>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[paper summary]]></summary></entry></feed>